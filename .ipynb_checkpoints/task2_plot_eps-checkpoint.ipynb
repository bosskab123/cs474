{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c98ca99",
   "metadata": {},
   "source": [
    "### Libs & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3521df2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kwsst\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kwsst\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import string\n",
    "import random\n",
    "import unicodedata\n",
    "from string import punctuation\n",
    "from string import digits\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4845c0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5324, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/df_park.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80b2b9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>time</th>\n",
       "      <th>description</th>\n",
       "      <th>body</th>\n",
       "      <th>section</th>\n",
       "      <th>summarized_body</th>\n",
       "      <th>agg_title_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[Weekender] Korea’s dynamic 2017</td>\n",
       "      <td>Choi He-suk</td>\n",
       "      <td>2018-01-01 13:22:00</td>\n",
       "      <td>From North Korea’s nuclear weapons program nea...</td>\n",
       "      <td>From North Korea’s nuclear weapons program nea...</td>\n",
       "      <td>Social affairs</td>\n",
       "      <td>The corruption scandal that broke out in 2016 ...</td>\n",
       "      <td>[Weekender] Korea’s dynamic 2017. From North K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>Headline makers of 2017</td>\n",
       "      <td>Korea Herald</td>\n",
       "      <td>2017-12-29 16:42:00</td>\n",
       "      <td>Moon Jae-inPresident Moon Jae-in (Yonhap)Moon ...</td>\n",
       "      <td>Moon Jae-inPresident Moon Jae-in (Yonhap)Moon ...</td>\n",
       "      <td>Social affairs</td>\n",
       "      <td>South Korean President Moon Jae-in rose to bec...</td>\n",
       "      <td>Headline makers of 2017. Moon Jae-inPresident ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>Moon pardons 6,444 people, excluding business ...</td>\n",
       "      <td>Ock Hyun-ju</td>\n",
       "      <td>2017-12-29 16:01:00</td>\n",
       "      <td>President Moon Jae-in granted pardons to 6,444...</td>\n",
       "      <td>President Moon Jae-in granted pardons to 6,444...</td>\n",
       "      <td>Social affairs</td>\n",
       "      <td>President Moon Jae-in granted pardons to 6,444...</td>\n",
       "      <td>Moon pardons 6,444 people, excluding business ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>Businessmen call for probe into shutdown of fa...</td>\n",
       "      <td>Yonhap</td>\n",
       "      <td>2017-12-29 15:04:00</td>\n",
       "      <td>A private task force on Friday pressed the gov...</td>\n",
       "      <td>A private task force on Friday pressed the gov...</td>\n",
       "      <td>North Korea</td>\n",
       "      <td>South Korea pulled the plug on the factory par...</td>\n",
       "      <td>Businessmen call for probe into shutdown of fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>Special pardons aimed at helping ordinary peop...</td>\n",
       "      <td>Yonhap</td>\n",
       "      <td>2017-12-29 11:39:00</td>\n",
       "      <td>The latest pardon extended to more than 6,000 ...</td>\n",
       "      <td>The latest pardon extended to more than 6,000 ...</td>\n",
       "      <td>Politics</td>\n",
       "      <td>The latest pardon extended to more than 6,000 ...</td>\n",
       "      <td>Special pardons aimed at helping ordinary peop...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0           1                   [Weekender] Korea’s dynamic 2017   \n",
       "1          13                            Headline makers of 2017   \n",
       "2          16  Moon pardons 6,444 people, excluding business ...   \n",
       "3          19  Businessmen call for probe into shutdown of fa...   \n",
       "4          21  Special pardons aimed at helping ordinary peop...   \n",
       "\n",
       "         author                 time  \\\n",
       "0   Choi He-suk  2018-01-01 13:22:00   \n",
       "1  Korea Herald  2017-12-29 16:42:00   \n",
       "2   Ock Hyun-ju  2017-12-29 16:01:00   \n",
       "3        Yonhap  2017-12-29 15:04:00   \n",
       "4        Yonhap  2017-12-29 11:39:00   \n",
       "\n",
       "                                         description  \\\n",
       "0  From North Korea’s nuclear weapons program nea...   \n",
       "1  Moon Jae-inPresident Moon Jae-in (Yonhap)Moon ...   \n",
       "2  President Moon Jae-in granted pardons to 6,444...   \n",
       "3  A private task force on Friday pressed the gov...   \n",
       "4  The latest pardon extended to more than 6,000 ...   \n",
       "\n",
       "                                                body         section  \\\n",
       "0  From North Korea’s nuclear weapons program nea...  Social affairs   \n",
       "1  Moon Jae-inPresident Moon Jae-in (Yonhap)Moon ...  Social affairs   \n",
       "2  President Moon Jae-in granted pardons to 6,444...  Social affairs   \n",
       "3  A private task force on Friday pressed the gov...     North Korea   \n",
       "4  The latest pardon extended to more than 6,000 ...        Politics   \n",
       "\n",
       "                                     summarized_body  \\\n",
       "0  The corruption scandal that broke out in 2016 ...   \n",
       "1  South Korean President Moon Jae-in rose to bec...   \n",
       "2  President Moon Jae-in granted pardons to 6,444...   \n",
       "3  South Korea pulled the plug on the factory par...   \n",
       "4  The latest pardon extended to more than 6,000 ...   \n",
       "\n",
       "                                      agg_title_body  \n",
       "0  [Weekender] Korea’s dynamic 2017. From North K...  \n",
       "1  Headline makers of 2017. Moon Jae-inPresident ...  \n",
       "2  Moon pardons 6,444 people, excluding business ...  \n",
       "3  Businessmen call for probe into shutdown of fa...  \n",
       "4  Special pardons aimed at helping ordinary peop...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f032c39",
   "metadata": {},
   "source": [
    "### Import the embedded documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f1438f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5324, 300)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vects = joblib.load('data/tfidf_titlebody_park.csv')\n",
    "vects.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9999d8d",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1030df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clustering \n",
    "def document_clustering(doc_vectors, clustering_method='kmeans', evaluate=False):\n",
    "    if clustering_method=='kmeans':\n",
    "        # Hyperparameters\n",
    "        k_event = 10000\n",
    "        k_issue = 6000\n",
    "        \n",
    "        # Clustering event\n",
    "        kmeans_event = KMeans(n_clusters=k_event, random_state=69).fit(doc_vectors)\n",
    "        # Represent each event by average sum of related news\n",
    "        event_vectors = np.zeros((k_event, doc_vectors.shape[1]))\n",
    "        for i in range(k_event):\n",
    "            event_vectors[i] = sum(doc_vectors[kmeans_event.labels_ == i])\n",
    "        \n",
    "        # Clustering issue\n",
    "        kmeans_issue = KMeans(n_clusters=k_issue, random_state=69).fit(event_vectors)\n",
    "        # Represent each issue by average sum of related news\n",
    "        issue_vectors = np.zeros((k_issue, doc_vectors.shape[1]))\n",
    "        for i in range(k_issue):\n",
    "            issue_vectors[i] = sum(event_vectors[kmeans_issue.labels_ == i])\n",
    "\n",
    "        issue_labels = np.array([ kmeans_issue.labels_[kmeans_event.labels_[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        return k_issue, k_event, issue_labels, kmeans_event.labels_\n",
    "    \n",
    "    elif clustering_method=='DBSCAN':\n",
    "        \n",
    "        # Hyperparameters\n",
    "        doc_eps = 0.255\n",
    "        doc_neighbors = 1\n",
    "        event_eps = 0.40\n",
    "        event_neighbors = 1\n",
    "        \n",
    "        '''\n",
    "            Find best doc_eps and event_eps\n",
    "        '''\n",
    "        if evaluate:\n",
    "            # Find best eps to group same document\n",
    "            doc_eps_list = [ 0.2 + 0.001*i for i in range(1,201) ]\n",
    "            doc_score = []\n",
    "            doc_event = []\n",
    "            doc_best_score = 0\n",
    "            doc_best_eps = 0.0001\n",
    "            for doc_eps in doc_eps_list:\n",
    "                # Clustering event\n",
    "                db_event = DBSCAN(eps=doc_eps, min_samples=doc_neighbors).fit(doc_vectors)\n",
    "                # Number of clusters in labels, ignoring noise if present.\n",
    "                n_events_ = len(set(db_event.labels_)) - (1 if -1 in db_event.labels_ else 0)\n",
    "                if len(set(db_event.labels_)) >= 2 and len(set(db_event.labels_)) <= len(doc_vectors)-1:\n",
    "                    score_ = silhouette_score(doc_vectors, db_event.labels_)\n",
    "                else:\n",
    "                    score_ = -1\n",
    "                doc_event.append(n_events_)\n",
    "                doc_score.append(score_)\n",
    "                if score_ > doc_best_score:\n",
    "                    doc_best_score = score_\n",
    "                    doc_best_eps = doc_eps\n",
    "            print(\"Best Silhouete score is {} at eps: {} and number of events: {}\".format(doc_best_score, doc_eps, n_events_))\n",
    "            fig = plt.figure()\n",
    "            plt.plot(doc_eps_list, doc_score)\n",
    "            fig.suptitle('Doc eps and Silhouette score', fontsize=20)\n",
    "            plt.xlabel('eps', fontsize=18)\n",
    "            plt.ylabel('Silhouette score', fontsize=16)\n",
    "            plt.show()\n",
    "            \n",
    "            fig = plt.figure()\n",
    "            plt.plot(doc_eps_list, doc_event)\n",
    "            fig.suptitle('Doc eps and number of events', fontsize=20)\n",
    "            plt.xlabel('eps', fontsize=18)\n",
    "            plt.ylabel('number of events', fontsize=16)\n",
    "            plt.show()\n",
    "            \n",
    "            # Set doc_eps to the best value\n",
    "            doc_eps = doc_best_eps\n",
    "            # Find best eps to group same event\n",
    "            # Clustering event\n",
    "            db_event = DBSCAN(eps=doc_eps, min_samples=doc_neighbors).fit(doc_vectors)\n",
    "            n_events_ = len(set(db_event.labels_)) - (1 if -1 in db_event.labels_ else 0)\n",
    "            event_labels = np.array(list(map(lambda x: n_events_ if x==-1 else x, db_event.labels_)))\n",
    "            event_vectors = np.zeros((n_events_, doc_vectors.shape[1]))\n",
    "            for i in range(n_events_+1):\n",
    "                if np.sum(event_labels == i) != 0:\n",
    "                    event_vectors[i] = np.sum(doc_vectors[event_labels == i], axis=0)/np.sum(event_labels == i)\n",
    "            \n",
    "            # Clustering issue\n",
    "            event_eps_list = [ 0.2 + 0.001*i for i in range(1,401) ]\n",
    "            event_score = []\n",
    "            event_issue = []\n",
    "            event_best_score = 0\n",
    "            event_best_eps = 0.001\n",
    "            for event_eps in event_eps_list:\n",
    "                db_issue = DBSCAN(eps=event_eps, min_samples=event_neighbors).fit(event_vectors)\n",
    "                # Number of clusters in labels, ignoring noise if present.\n",
    "                n_issues_ = len(set(db_issue.labels_)) - (1 if -1 in db_issue.labels_ else 0)\n",
    "                if len(set(db_issue.labels_)) >= 2 and len(set(db_issue.labels_)) <= len(event_vectors)-1:\n",
    "                    score_ = silhouette_score(event_vectors, db_issue.labels_)\n",
    "                else:\n",
    "                    score_ = -1\n",
    "                event_issue.append(n_issues_)\n",
    "                event_score.append(score_)\n",
    "                if score_ > event_best_score:\n",
    "                    event_best_score = score_\n",
    "                    event_best_eps = event_eps\n",
    "            print(\"Best Silhouete score is {} at eps: {} and number of issues: {}\".format(event_best_score, event_eps, n_issues_))\n",
    "            fig = plt.figure()\n",
    "            plt.plot(event_eps_list, event_score)\n",
    "            fig.suptitle('Event eps and Silhouette score', fontsize=20)\n",
    "            plt.xlabel('eps', fontsize=18)\n",
    "            plt.ylabel('Silhouette score', fontsize=16)\n",
    "            plt.show()\n",
    "            \n",
    "            fig = plt.figure()\n",
    "            plt.plot(event_eps_list, event_issue)\n",
    "            fig.suptitle('Event eps and number of issues', fontsize=20)\n",
    "            plt.xlabel('eps', fontsize=18)\n",
    "            plt.ylabel('number of issues', fontsize=16)\n",
    "            plt.show()\n",
    "            \n",
    "            # Set event_eps to best value\n",
    "            event_eps = event_best_eps\n",
    "            # Clustering issue\n",
    "            db_issue = DBSCAN(eps=event_eps, min_samples=event_neighbors).fit(event_vectors)\n",
    "            # Number of clusters in labels, ignoring noise if present.\n",
    "            n_issues_ = len(set(db_issue.labels_)) - (1 if -1 in db_issue.labels_ else 0)\n",
    "            n_noise_ = list(db_issue.labels_).count(-1)\n",
    "            print(n_issues_, n_noise_)\n",
    "            # Represent each issue by average sum of related news\n",
    "            issue_labels = np.array(list(map(lambda x: n_issues_ if x==-1 else x, db_issue.labels_)))\n",
    "            issue_vectors = np.zeros((n_issues_, doc_vectors.shape[1]))\n",
    "            for i in range(n_issues_+1):\n",
    "                if np.sum(issue_labels == i) != 0:\n",
    "                    issue_vectors[i] = np.sum(event_vectors[issue_labels == i], axis=0)/np.sum(issue_labels == i)\n",
    "       \n",
    "            issue_labels = np.array([ issue_labels[event_labels[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        else:\n",
    "            '''\n",
    "            Clustering using specific value\n",
    "            '''\n",
    "            # Clustering event\n",
    "            db_event = DBSCAN(eps=doc_eps, min_samples=doc_neighbors).fit(doc_vectors)\n",
    "            # Number of clusters in labels, ignoring noise if present.\n",
    "            n_events_ = len(set(db_event.labels_)) - (1 if -1 in db_event.labels_ else 0)\n",
    "            n_noise_ = list(db_event.labels_).count(-1)\n",
    "            print(n_events_, n_noise_)\n",
    "            # Represent each event by average sum of related news\n",
    "            event_labels = np.array(list(map(lambda x: n_events_ if x==-1 else x, db_event.labels_)))\n",
    "            event_vectors = np.zeros((n_events_, doc_vectors.shape[1]))\n",
    "            for i in range(n_events_+1):\n",
    "                if np.sum(event_labels == i) != 0:\n",
    "                    event_vectors[i] = np.sum(doc_vectors[event_labels == i], axis=0)/np.sum(event_labels == i)\n",
    "\n",
    "            # Clustering issue\n",
    "            db_issue = DBSCAN(eps=event_eps, min_samples=event_neighbors).fit(event_vectors)\n",
    "            # Number of clusters in labels, ignoring noise if present.\n",
    "            n_issues_ = len(set(db_issue.labels_)) - (1 if -1 in db_issue.labels_ else 0)\n",
    "            n_noise_ = list(db_issue.labels_).count(-1)\n",
    "            print(n_issues_, n_noise_)\n",
    "            # Represent each issue by average sum of related news\n",
    "            issue_labels = np.array(list(map(lambda x: n_issues_ if x==-1 else x, db_issue.labels_)))\n",
    "            issue_vectors = np.zeros((n_issues_, doc_vectors.shape[1]))\n",
    "            for i in range(n_issues_+1):\n",
    "                if np.sum(issue_labels == i) != 0:\n",
    "                    issue_vectors[i] = np.sum(event_vectors[issue_labels == i], axis=0)/np.sum(issue_labels == i)\n",
    "        \n",
    "            issue_labels = np.array([ issue_labels[event_labels[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        return n_issues_, n_events_, issue_labels, event_labels\n",
    "    \n",
    "    elif clustering_method=='agglomerative':\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    elif clustering_method=='LDA':\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        assert(\"Doesn't support {}\".format(clustering_method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b29683",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_issue, num_event, issue_labels, event_labels = document_clustering(vects,\n",
    "                                                                       clustering_method='DBSCAN',\n",
    "                                                                       evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdca20f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmining",
   "language": "python",
   "name": "tmining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
