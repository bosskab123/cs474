{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c5bfb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import string\n",
    "import random\n",
    "import unicodedata\n",
    "from string import punctuation\n",
    "from string import digits\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5724fe5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23769, 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Reading data\n",
    "data_dir = 'data/'\n",
    "filename_prefix = 'koreaherald_1517_'\n",
    "df = []\n",
    "\n",
    "for i in range(8):\n",
    "    df.append(pd.read_json(os.path.join(data_dir, filename_prefix + str(i) + '.json')))\n",
    "df = pd.concat(df)\n",
    "df.reset_index(inplace=True)\n",
    "df = df.rename(columns=dict(zip(df.columns,[df.columns[i].strip() for i in range(len(df.columns))])))\n",
    "df.drop('index', inplace=True, axis=1)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5212d147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load large spacy model \n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Aggregate title and content\n",
    "title_weight = 1\n",
    "df['agg_title_body'] = title_weight*(df['title']+'. ') + df['body']\n",
    "\n",
    "### Embed document and clustering\n",
    "df2017 = df['2017' < df['time']]\n",
    "df2016 = df[('2016' < df['time']) & (df['time'] < '2017')]\n",
    "df2015 = df[('2015' < df['time']) & (df['time'] < '2016')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff64cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lemmatization tool\n",
    "stemmer = WordNetLemmatizer()\n",
    "### Change similar words to the same word\n",
    "UN_WORD = \"The United Nations\"\n",
    "US_WORD = \"The United States\"\n",
    "NK_WORD = \"North Korea\"\n",
    "SK_WORD = \"South Korea\"\n",
    "\n",
    "similar_words = {\n",
    "    # Change to \"The United States\"\n",
    "    \"U.S.\": US_WORD,\n",
    "    \"US\": US_WORD,\n",
    "    \"USA\": US_WORD,\n",
    "    \"United States\": US_WORD,\n",
    "    \"United States'\": US_WORD,\n",
    "    \"The United States'\": US_WORD,\n",
    "    \n",
    "    # Change to \"North Korea\"\n",
    "    \"NK\": NK_WORD,\n",
    "    \"NK's\": NK_WORD,\n",
    "    \"N. Korea\": NK_WORD,\n",
    "    \"N. Korea's\": NK_WORD,\n",
    "    \"North Korea's\": NK_WORD,\n",
    "    \n",
    "    # Change to \"South Korea\"\n",
    "    \"SK\": SK_WORD,\n",
    "    \"SK's\": SK_WORD,\n",
    "    \"S. Korea\": SK_WORD,\n",
    "    \"S. Korea's\": SK_WORD,\n",
    "    \"South Korea's\": SK_WORD,\n",
    "    \n",
    "    # Change to \"The United Nations\"\n",
    "    \"United Nations\": UN_WORD,\n",
    "    \"United Nations'\": UN_WORD,\n",
    "    \"The United Nations'\": UN_WORD,\n",
    "    \"UN\": UN_WORD,\n",
    "}\n",
    "\n",
    "### Transform function\n",
    "def text_cleaning(s: str):\n",
    "        \n",
    "    def replace_strange_char(s: str):\n",
    "        non_en_chars = {\n",
    "            \"’\": \"'\",\n",
    "            \"‘\": \"'\"\n",
    "        }\n",
    "\n",
    "        def remove_non_en_chars(txt):\n",
    "            # remove non english characters\n",
    "            txt = convert_latin_chars(txt)\n",
    "            for char in non_en_chars.keys():\n",
    "                txt = re.sub(char, non_en_chars[char], txt)\n",
    "            txt = re.sub(r'[^\\x00-\\x7F]+', ' ', txt)\n",
    "            return txt\n",
    "\n",
    "        def convert_latin_chars(txt):\n",
    "            # convert latin characters\n",
    "            return ''.join(char for char in unicodedata.normalize('NFKD', txt) if unicodedata.category(char) != 'Mn')\n",
    "\n",
    "        s = remove_non_en_chars(s)\n",
    "        s = convert_latin_chars(s)\n",
    "        return s\n",
    "    s = replace_strange_char(s)\n",
    "    for key,value in similar_words.items():\n",
    "        s = re.sub(key, value, s)\n",
    "    return s\n",
    "\n",
    "# Using spacy to preprocess\n",
    "def preprocess_spacy(s: str):\n",
    "    # Change similar terms to the same term\n",
    "    new_str = text_cleaning(s)\n",
    "    doc = nlp(s)\n",
    "    # Group tokens\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    token_groupup_pattern = [\n",
    "        [{\"LOWER\": \"the\"}, {\"LOWER\": \"united\"}, {\"LOWER\": \"nations\"}],\n",
    "        [{\"LOWER\": \"the\"}, {\"LOWER\": \"united\"}, {\"LOWER\": \"states\"}],\n",
    "        [{\"LOWER\": \"north\"}, {\"LOWER\": \"korea\"}],\n",
    "        [{\"LOWER\": \"south\"}, {\"LOWER\": \"korea\"}],\n",
    "    ]\n",
    "    matcher.add(\"TermGroup\",token_groupup_pattern)\n",
    "    matches = matcher(doc)\n",
    "    merge_doc = []\n",
    "    for nid, start, end in matches:\n",
    "        merge_doc.append((start,end))\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for i in range(len(merge_doc)-1,-1,-1):\n",
    "            retokenizer.merge(doc[merge_doc[i][0]:merge_doc[i][1]])\n",
    "        \n",
    "    # Remove all stopword, punctuation, number\n",
    "    tokens = [ token for token in doc if not token.is_stop and not token.is_punct and not token.like_num]\n",
    "    new_str = ' '.join([ token.lemma_.lower() for token in tokens ])\n",
    "    return new_str, tokens, doc\n",
    "\n",
    "def spacy_tokenizer(s: str):\n",
    "    # Change similar terms to the same term\n",
    "    new_str = text_cleaning(s)\n",
    "    doc = nlp(s)\n",
    "    # Group tokens\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    token_groupup_pattern = [\n",
    "        [{\"LOWER\": \"the\"}, {\"LOWER\": \"united\"}, {\"LOWER\": \"nations\"}],\n",
    "        [{\"LOWER\": \"the\"}, {\"LOWER\": \"united\"}, {\"LOWER\": \"states\"}],\n",
    "        [{\"LOWER\": \"north\"}, {\"LOWER\": \"korea\"}],\n",
    "        [{\"LOWER\": \"south\"}, {\"LOWER\": \"korea\"}],\n",
    "    ]\n",
    "    matcher.add(\"TermGroup\",token_groupup_pattern)\n",
    "    matches = matcher(doc)\n",
    "    merge_doc = []\n",
    "    for nid, start, end in matches:\n",
    "        merge_doc.append((start,end))\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for i in range(len(merge_doc)-1,-1,-1):\n",
    "            retokenizer.merge(doc[merge_doc[i][0]:merge_doc[i][1]])\n",
    "        \n",
    "    # Remove all stopword, punctuation, number\n",
    "    tokens = [ token.lemma_.lower() for token in doc \\\n",
    "              if not token.is_stop and not token.is_punct and not token.like_num and token.lemma_.strip()!= '']\n",
    "    return tokens\n",
    "\n",
    "### Preprocess function for grouping similar topic\n",
    "def preprocess_manual(s: str):\n",
    "    # Change similar words to the same word\n",
    "    new_str = transform_to_similar_sentence(s)\n",
    "    # Remove punctuation\n",
    "    new_str = ''.join(ch if ch not in set(punctuation) else \" \" for ch in new_str)\n",
    "    # Remove all single characters\n",
    "    new_str = re.sub(r'\\W', ' ', new_str)\n",
    "    new_str = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', new_str)\n",
    "    new_str = re.sub(r'\\^[a-zA-Z]\\s+', ' ', new_str) \n",
    "    # Substituting multiple spaces with single space\n",
    "    new_str = re.sub(r'\\s+', ' ', new_str, flags=re.I)\n",
    "    # Removing prefixed 'b' - when data is in bytes format\n",
    "    new_str = re.sub(r'^b\\s+', '', new_str)\n",
    "    # Removing all numbers\n",
    "    new_str = new_str.translate(str.maketrans('', '', digits))\n",
    "    # Converting to Lowercase\n",
    "    new_str = new_str.lower()\n",
    "    # Lemmatization and remove stopwords\n",
    "    new_str = new_str.split()\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    tokens = [stemmer.lemmatize(word) for word in new_str if word not in stopwords]\n",
    "    new_str = ' '.join(tokens)\n",
    "    \n",
    "    return new_str, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a2f7c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clustering \n",
    "def document_clustering(doc_vectors, clustering_method='kmeans', evaluate=False):\n",
    "    if clustering_method=='kmeans':\n",
    "        # Hyperparameters\n",
    "        k_event = 10000\n",
    "        k_issue = 6000\n",
    "        \n",
    "        # Clustering event\n",
    "        kmeans_event = KMeans(n_clusters=k_event, random_state=69).fit(doc_vectors)\n",
    "        # Represent each event by average sum of related news\n",
    "        event_vectors = np.zeros((k_event, doc_vectors.shape[1]))\n",
    "        for i in range(k_event):\n",
    "            event_vectors[i] = sum(doc_vectors[kmeans_event.labels_ == i])\n",
    "        \n",
    "        # Clustering issue\n",
    "        kmeans_issue = KMeans(n_clusters=k_issue, random_state=69).fit(event_vectors)\n",
    "        # Represent each issue by average sum of related news\n",
    "        issue_vectors = np.zeros((k_issue, doc_vectors.shape[1]))\n",
    "        for i in range(k_issue):\n",
    "            issue_vectors[i] = sum(event_vectors[kmeans_issue.labels_ == i])\n",
    "\n",
    "        issue_labels = np.array([ kmeans_issue.labels_[kmeans_event.labels_[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        return k_issue, k_event, issue_labels, kmeans_event.labels_\n",
    "    \n",
    "    elif clustering_method=='DBSCAN':\n",
    "        \n",
    "        # Hyperparameters\n",
    "        doc_eps = 0.19\n",
    "        doc_neighbors = 1\n",
    "        event_eps = 0.50\n",
    "        event_neighbors = 1\n",
    "        \n",
    "        '''\n",
    "            Find best doc_eps and event_eps\n",
    "        '''\n",
    "        if evaluate:\n",
    "            # Find best eps to group same document\n",
    "            doc_eps_list = [ 0.10 + 0.001*i for i in range(1,301) ]\n",
    "            doc_score = []\n",
    "            doc_event = []\n",
    "            doc_best_score = 0\n",
    "            doc_best_eps = 0.0001\n",
    "            for doc_eps in doc_eps_list:\n",
    "                # Clustering event\n",
    "                db_event = DBSCAN(eps=doc_eps, min_samples=doc_neighbors).fit(doc_vectors)\n",
    "                # Number of clusters in labels, ignoring noise if present.\n",
    "                n_events_ = len(set(db_event.labels_)) - (1 if -1 in db_event.labels_ else 0)\n",
    "                if len(set(db_event.labels_)) >= 2 and len(set(db_event.labels_)) <= len(doc_vectors)-1:\n",
    "                    score_ = silhouette_score(doc_vectors, db_event.labels_)\n",
    "                else:\n",
    "                    score_ = -1\n",
    "                doc_event.append(n_events_)\n",
    "                doc_score.append(score_)\n",
    "                if score_ > doc_best_score:\n",
    "                    doc_best_score = score_\n",
    "                    doc_best_eps = doc_eps\n",
    "            print(\"Best Silhouete score is {} at eps: {} and number of events: {}\".format(doc_best_score, doc_eps, n_events_))\n",
    "            fig = plt.figure()\n",
    "            plt.plot(doc_eps_list, doc_score)\n",
    "            fig.suptitle('Doc eps and Silhouette score', fontsize=20)\n",
    "            plt.xlabel('eps', fontsize=18)\n",
    "            plt.ylabel('Silhouette score', fontsize=16)\n",
    "            plt.show()\n",
    "            \n",
    "            fig = plt.figure()\n",
    "            plt.plot(doc_eps_list, doc_event)\n",
    "            fig.suptitle('Doc eps and number of events', fontsize=20)\n",
    "            plt.xlabel('eps', fontsize=18)\n",
    "            plt.ylabel('number of events', fontsize=16)\n",
    "            plt.show()\n",
    "            \n",
    "            # Set doc_eps to the best value\n",
    "            doc_eps = doc_best_eps\n",
    "            # Find best eps to group same event\n",
    "            # Clustering event\n",
    "            db_event = DBSCAN(eps=doc_eps, min_samples=doc_neighbors).fit(doc_vectors)\n",
    "            n_events_ = len(set(db_event.labels_)) - (1 if -1 in db_event.labels_ else 0)\n",
    "            event_labels = np.array(list(map(lambda x: n_events_ if x==-1 else x, db_event.labels_)))\n",
    "            event_vectors = np.zeros((n_events_, doc_vectors.shape[1]))\n",
    "            for i in range(n_events_+1):\n",
    "                if np.sum(event_labels == i) != 0:\n",
    "                    event_vectors[i] = np.sum(doc_vectors[event_labels == i], axis=0)/np.sum(event_labels == i)\n",
    "            \n",
    "            \n",
    "#             # Clustering issue\n",
    "#             event_eps_list = [ 0.2 + 0.001*i for i in range(1,401) ]\n",
    "#             event_score = []\n",
    "#             event_issue = []\n",
    "#             event_best_score = 0\n",
    "#             event_best_eps = 0.001\n",
    "#             for event_eps in event_eps_list:\n",
    "#                 db_issue = DBSCAN(eps=event_eps, min_samples=event_neighbors).fit(event_vectors)\n",
    "#                 # Number of clusters in labels, ignoring noise if present.\n",
    "#                 n_issues_ = len(set(db_issue.labels_)) - (1 if -1 in db_issue.labels_ else 0)\n",
    "#                 if len(set(db_issue.labels_)) >= 2 and len(set(db_issue.labels_)) <= len(event_vectors)-1:\n",
    "#                     score_ = silhouette_score(event_vectors, db_issue.labels_)\n",
    "#                 else:\n",
    "#                     score_ = -1\n",
    "#                 event_issue.append(n_issues_)\n",
    "#                 event_score.append(score_)\n",
    "#                 if score_ > event_best_score:\n",
    "#                     event_best_score = score_\n",
    "#                     event_best_eps = event_eps\n",
    "#             print(\"Best Silhouete score is {} at eps: {} and number of issues: {}\".format(event_best_score, event_eps, n_issues_))\n",
    "#             fig = plt.figure()\n",
    "#             plt.plot(event_eps_list, event_score)\n",
    "#             fig.suptitle('Event eps and Silhouette score', fontsize=20)\n",
    "#             plt.xlabel('eps', fontsize=18)\n",
    "#             plt.ylabel('Silhouette score', fontsize=16)\n",
    "#             plt.show()\n",
    "            \n",
    "#             fig = plt.figure()\n",
    "#             plt.plot(event_eps_list, event_issue)\n",
    "#             fig.suptitle('Event eps and number of issues', fontsize=20)\n",
    "#             plt.xlabel('eps', fontsize=18)\n",
    "#             plt.ylabel('number of issues', fontsize=16)\n",
    "#             plt.show()\n",
    "            \n",
    "            # Set event_eps to best value\n",
    "            event_eps = 0.5\n",
    "            # Clustering issue\n",
    "            db_issue = DBSCAN(eps=event_eps, min_samples=event_neighbors).fit(event_vectors)\n",
    "            # Number of clusters in labels, ignoring noise if present.\n",
    "            n_issues_ = len(set(db_issue.labels_)) - (1 if -1 in db_issue.labels_ else 0)\n",
    "            n_noise_ = list(db_issue.labels_).count(-1)\n",
    "            # Represent each issue by average sum of related news\n",
    "            issue_labels = np.array(list(map(lambda x: n_issues_ if x==-1 else x, db_issue.labels_)))\n",
    "            issue_vectors = np.zeros((n_issues_, doc_vectors.shape[1]))\n",
    "            for i in range(n_issues_+1):\n",
    "                if np.sum(issue_labels == i) != 0:\n",
    "                    issue_vectors[i] = np.sum(event_vectors[issue_labels == i], axis=0)/np.sum(issue_labels == i)\n",
    "       \n",
    "            issue_labels = np.array([ issue_labels[event_labels[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        else:\n",
    "            '''\n",
    "            Clustering using specific value\n",
    "            '''\n",
    "            # Clustering event\n",
    "            db_event = DBSCAN(eps=doc_eps, min_samples=doc_neighbors).fit(doc_vectors)\n",
    "            # Number of clusters in labels, ignoring noise if present.\n",
    "            n_events_ = len(set(db_event.labels_)) - (1 if -1 in db_event.labels_ else 0)\n",
    "            n_noise_ = list(db_event.labels_).count(-1)\n",
    "            print(\"1st cluster:\\n\\tThe number of cluster is {}\".format(n_events_))\n",
    "            # Represent each event by average sum of related news\n",
    "            event_labels = np.array(list(map(lambda x: n_events_ if x==-1 else x, db_event.labels_)))\n",
    "            event_vectors = np.zeros((n_events_, doc_vectors.shape[1]))\n",
    "            for i in range(n_events_+1):\n",
    "                if np.sum(event_labels == i) != 0:\n",
    "                    event_vectors[i] = np.sum(doc_vectors[event_labels == i], axis=0)/np.sum(event_labels == i)\n",
    "\n",
    "            # Clustering issue\n",
    "            db_issue = DBSCAN(eps=event_eps, min_samples=event_neighbors).fit(event_vectors)\n",
    "            # Number of clusters in labels, ignoring noise if present.\n",
    "            n_issues_ = len(set(db_issue.labels_)) - (1 if -1 in db_issue.labels_ else 0)\n",
    "            n_noise_ = list(db_issue.labels_).count(-1)\n",
    "            print(\"2nd cluster:\\n\\tThe number of cluster is {}\".format(n_issues_))\n",
    "            # Represent each issue by average sum of related news\n",
    "            issue_labels = np.array(list(map(lambda x: n_issues_ if x==-1 else x, db_issue.labels_)))\n",
    "            issue_vectors = np.zeros((n_issues_, doc_vectors.shape[1]))\n",
    "            for i in range(n_issues_+1):\n",
    "                if np.sum(issue_labels == i) != 0:\n",
    "                    issue_vectors[i] = np.sum(event_vectors[issue_labels == i], axis=0)/np.sum(issue_labels == i)\n",
    "        \n",
    "            issue_labels = np.array([ issue_labels[event_labels[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        return n_issues_, n_events_, issue_labels, event_labels\n",
    "    \n",
    "    elif clustering_method=='agglomerative':\n",
    "        # Hyperparameters\n",
    "        n_events = 10000\n",
    "        n_issues = 6000\n",
    "        \n",
    "        # Clustering event\n",
    "        agg_event = AgglomerativeClustering(distance_threshold=0, n_clusters=n_events).fit(doc_vectors)\n",
    "        # Represent each event by average sum of related news\n",
    "        event_vectors = np.zeros((n_events, doc_vectors.shape[1]))\n",
    "        for i in range(n_events):\n",
    "            event_vectors[i] = sum(doc_vectors[agg_event.labels_ == i])\n",
    "        \n",
    "        plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "        # plot the top three levels of the dendrogram\n",
    "        plot_dendrogram(agg_event, truncate_mode=\"level\", p=3)\n",
    "        plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Clustering issue\n",
    "        agg_issue = AgglomerativeClustering(distance_threshold=0, n_clusters=n_issues).fit(event_vectors)\n",
    "        # Represent each issue by average sum of related news\n",
    "        issue_vectors = np.zeros((n_issues, doc_vectors.shape[1]))\n",
    "        for i in range(n_issues):\n",
    "            issue_vectors[i] = sum(event_vectors[agg_issue.labels_ == i])\n",
    "\n",
    "        issue_labels = np.array([ agg_issue.labels_[agg_event.labels_[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        return agg_issue, agg_event, issue_labels, agg_event.labels_\n",
    "    \n",
    "    elif clustering_method=='LDA':\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        assert(\"Doesn't support {}\".format(clustering_method))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "239533c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "tfidf_doc2017_vectors = joblib.load('tfidf_titlebody_2017.csv')\n",
    "tfidf_doc2016_vectors = joblib.load('tfidf_titlebody_2016.csv')\n",
    "tfidf_doc2015_vectors = joblib.load('tfidf_titlebody_2015.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "393a0cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st cluster:\n",
      "\tThe number of cluster is 6914\n",
      "2nd cluster:\n",
      "\tThe number of cluster is 1159\n",
      "1st cluster:\n",
      "\tThe number of cluster is 7335\n",
      "2nd cluster:\n",
      "\tThe number of cluster is 1244\n",
      "1st cluster:\n",
      "\tThe number of cluster is 8997\n",
      "2nd cluster:\n",
      "\tThe number of cluster is 1517\n"
     ]
    }
   ],
   "source": [
    "tfidf_doc2015_num_issue, tfidf_doc2015_num_event, tfidf_doc2015_issue_labels, tfidf_doc2015_event_labels = document_clustering(tfidf_doc2015_vectors, clustering_method='DBSCAN', evaluate=False)\n",
    "tfidf_doc2016_num_issue, tfidf_doc2016_num_event, tfidf_doc2016_issue_labels, tfidf_doc2016_event_labels = document_clustering(tfidf_doc2016_vectors, clustering_method='DBSCAN', evaluate=False)\n",
    "tfidf_doc2017_num_issue, tfidf_doc2017_num_event, tfidf_doc2017_issue_labels, tfidf_doc2017_event_labels = document_clustering(tfidf_doc2017_vectors, clustering_method='DBSCAN', evaluate=False)\n",
    "\n",
    "from collections import Counter\n",
    "counter_2017_event = Counter(tfidf_doc2017_event_labels)\n",
    "counter_2016_event = Counter(tfidf_doc2016_event_labels)\n",
    "counter_2015_event = Counter(tfidf_doc2015_event_labels)\n",
    "\n",
    "top10_event_2017 = counter_2017_event.most_common(10)\n",
    "top10_event_2016 = counter_2016_event.most_common(10)\n",
    "top10_event_2015 = counter_2015_event.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c82e7e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster id 67\n",
      "\tGPE: [(Korea, 1), (Seoul, 1), (Incheon, 1), (Gangneung, 1), (Gwangju, 1), (Busan, 1), (Gangwon Province, 1), (Gyeonggi Province, 1), (North Chungcheong Province, 1), (North Gyeongsang Province, 1)]\n",
      "\tORG: [(Chuncheon, 1), (Daejeon, 1), (the Korea Meteorological Administration, 1), (the Korea Meteorological Administration, 1), (Chuncheon, 1), (Cheongju, 1), (Yonhap)The KMA, 1), (KMA, 1), (mercury, 1), (The Korea Meteorological Administration, 1)]\n",
      "\tNORP: [(Yonhap)The, 1)]\n",
      "\tPERSON: [(Gyeonggi, 1), (North Chungcheong, 1), (Park Ju-young, 1), (Park Ju-young, 1), (Park Ju-young, 1), (Park Ju-young, 1), (Gyeonggi, 1), (Gangwon, 1), (Gyeongsang, 1), (Park Ju-young, 1)]\n",
      "\tFAC: [(Gangneung 2, 1)]\n",
      "\tLOC: [(west coast, 1), (Jeju Island, 1), (west coast, 1), (Northern Gyeonggi Province, 1)]\n",
      "Cluster id 617\n",
      "\tNORP: [(South Korean, 1), (Gunbuk, 1), (Korean, 1), (Korean, 1), (Korean, 1), (Japanese, 1), (Korean, 1), (Russian, 1), (Korean, 1), (Japanese, 1)]\n",
      "\tGPE: [(South Korea, 1), (Haman, 1), (South Gyeongsang Province, 1), (Yonhap)The, 1), (Seoul, 1), (Yanggu, 1), (Gangwon Province, 1), (Korea, 1), (Mount Baekdu, 1), (London, 1)]\n",
      "\tPERSON: [(Paik In-sung, 1), (Paik, 1), (Yonhap, 1), (Gorals, 1), (Mount Jiri, 1), (Ahn Jae-yong, 1), (Park Hyun-koo, 1), (phko@heraldcorp.com)Written, 1), (Ock Hyun-ju, 1), (Jo Cook, 1)]\n",
      "\tORG: [(Pukyong National University, 1), (Nature, 1), (the Association of Korean Goral Conservation, 1), (Gorals, 1), (Seoul Zoo, 1), (Tiger Alliance, 1), (the World Association of Zoos and Aquariums, 1), (Yonhap)\"We, 1), (ALTA, 1), (Seoul Zoo, 1)]\n",
      "\tWORK_OF_ART: [(Scientific Reports, 1)]\n",
      "\tLOC: [(the Korean Peninsula, 1), (Amur leopards, 1), (Europe, 1), (Far East, 1), (The Korean Peninsula, 1), (Amur leopards, 1), (Korean Peninsula, 1)]\n",
      "\tFAC: [(the China-North Korea, 1)]\n",
      "\tPRODUCT: [(Amur, 1), (Amur Leopard, 1), (Amur, 1), (Amur, 1), (Amur, 1), (Amur, 1), (Amur, 1)]\n",
      "\tLAW: [(Amur, 1)]\n",
      "\tEVENT: [(the Joseon Dynasty, 1)]\n",
      "Cluster id 3706\n",
      "\tGPE: [(South Korea, 1), (Statistics Korea, 1), (South Korea, 1), (South Korea, 1), (Statistics Korea, 1), (South Korea, 1), (Seoul, 1), (South Korea, 1), (Statistics Korea, 1), (South Korea, 1)]\n",
      "\tORG: [(Yonhap)The, 1)]\n",
      "\tLOC: [(Asia, 1), (Asia, 1), (Asia, 1), (Asia, 1), (Asia, 1)]\n",
      "\tPERSON: [(Yonhap, 1), (Yonhap, 1), (Yonhap, 1), (Yonhap, 1), (Yonhap)The, 1), (Yonhap, 1)]\n",
      "\tNORP: [(South Korean, 1)]\n",
      "Cluster id 4733\n",
      "\tNORP: [(South Korean, 1), (South Korean, 1), (South Korean, 1), (Technology)The, 1), (South Korean, 1), (South Korean, 1)]\n",
      "\tORG: [(ICT, 1), (Sungkyunkwan University, 1), (the Ministry of Science, 1), (ICT, 1), (Future Planning, 1), (the Korea Electrotechnology Research Institute, 1), (the Korea Electrotechnology Research Institute)\"To, 1), (MWNT, 1), (KERI, 1), (ACS Applied Materials & Interfaces, 1)]\n",
      "\tPERSON: [(Bang Chang-hyun, 1), (Bang, 1), (Yonhap, 1), (Seol Seung-kwon, 1), (Seol, 1), (Yonhap, 1), (Son Jeong-gon, 1), (Yonhap, 1), (Han Chang-soo, 1), (Yonhap, 1)]\n",
      "\tWORK_OF_ART: [(Nature, 1), (ACSNano, 1), (Advanced Materials, 1), (Science Advances, 1)]\n",
      "\tPRODUCT: [(3D, 1)]\n",
      "Cluster id 5\n",
      "\tGPE: [(South Korea, 1), (Yeongam, 1), (Seoul, 1), (South Korea, 1), (South Korea, 1), (Jeongeup, 1), (Seoul, 1), (Gochang, 1), (Seoul, 1), (Yeongam, 1)]\n",
      "\tORG: [(the agriculture ministry, 1), (the Ministry of Agriculture, Food and Rural Affairs, 1), (the Ministry of Agriculture, Food and Rural Affairs, 1), (the World Health Organization, 1), (The Animal and Plant Quarantine Agency, 1), (APQA, 1), (the World Health Organization, 1), (the agriculture ministry, 1), (the agriculture ministry, 1), (the Ministry of Agriculture, Food and Rural Affairs, 1)]\n",
      "\tPERSON: [(Yonhap, 1), (Yonhap, 1), (Yeongam, 1), (Yonhap, 1), (Yonhap, 1)]\n",
      "\tNORP: [(South Korean, 1), (H6N6, 1), (H5N6, 1)]\n",
      "\tLOC: [(Europe, 1), (Jeju Island, 1)]\n",
      "Cluster id 6652\n",
      "\tORG: [(The Brazilian Air Force, 1), (the Brazilian Air Force, 1), (Polaris Shipping, 1), (the Brazilian Air Force, 1), (Navy, 1), (Polaris Shipping, 1), (Polaris Shipping's, 1), (the Brazilian Air Force, 1), (the Brazilian Air Force, 1)]\n",
      "\tNORP: [(South Korean, 1), (South Korean, 1), (South Korean, 1), (Brazilian, 1), (South Korean, 1), (South Korean, 1), (Filipino, 1), (Filipino, 1), (South Korean, 1), (South Korean, 1)]\n",
      "\tLOC: [(the South Atlantic, 1), (the South Atlantic, 1), (the South Atlantic, 1), (the South Atlantic, 1), (Yonhap)The Marshall Islands, 1), (the South Atlantic, 1), (the South Atlantic, 1)]\n",
      "\tFAC: [(C-130, 1), (C-130, 1), (C-130, 1), (C-130, 1)]\n",
      "\tPRODUCT: [(the Stellar Daisy, 1), (Polaris Shipping, 1)]\n",
      "\tPERSON: [(Stellar Daisy, 1), (Stella Daisy, 1), (Yonhap, 1), (Stella Daisy, 1), (Yonhap, 1), (Stella Daisy, 1), (Stella Daisy, 1), (Stella Daisy, 1), (Yonhap)The Marshall Islands, 1), (Stella Daisy, 1)]\n",
      "\tGPE: [(Uruguay, 1), (Brazil, 1), (Argentina, 1), (Uruguay, 1), (The Marshall Islands, 1), (Uruguay, 1), (South Korea's, 1), (Brazil, 1), (China, 1), (Uruguay, 1)]\n",
      "Cluster id 813\n",
      "\tNORP: [(South Korean, 1), (South Korean, 1), (American, 1), (South Korean, 1), (South Korean, 1), (South Korean, 1), (American, 1), (South Korean, 1), (South Korean, 1), (South Korean, 1)]\n",
      "\tPERSON: [(Bang Tae-hyun, 1), (Bang, 1), (Bang, 1), (Bang, 1), (Bang, 1), (Bang Tae-hyun, 1), (Leo Kuntz, 1), (Bang, 1), (Leo Kuntz, 1), (Kuntz, 1)]\n",
      "\tORG: [(The Seoul Central District Court, 1), (MMA, 1), (UFC Fight Night 79, 1), (UFC, 1), (UFC, 1), (UFC, 1), (MMA, 1), (UFC, 1), (UFC Fight Night, 1), (UFC, 1)]\n",
      "\tFAC: [(the Ultimate Fighting Championship event, 1), (the Ultimate Fighting Championship event, 1), (the UFC Fight Night 93, 1), (the UFC Fight Night 79, 1)]\n",
      "\tGPE: [(South Korea, 1), (the United States, 1), (Seoul, 1), (Seoul, 1), (Seoul, 1), (Germany, 1), (US, 1), (South Korea, 1), (Seoul, 1), (the United States, 1)]\n",
      "\tEVENT: [(Olympic Gymnastics Arena, 1), (the UFC Fight Night 79, 1), (Olympic Gymnastics Arena, 1), (the UFC Fight Night 79, 1), (Olympic Gymnastics Arena, 1)]\n",
      "\tLOC: [(Seoul Central District Prosecutors' Office, 1)]\n",
      "Cluster id 911\n",
      "\tPERSON: [(Moon Jae, 1), (Hong Jong-haak, 1), (Moon, 1), (Hong Jong-haak, 1), (Moon, 1), (Yonhap, 1), (Moon Jae, 1), (Moon, 1), (Hong Jong-haak, 1), (Hong Jong-haak, 1)]\n",
      "\tORG: [(the National Assembly, 1), (Yonhap)The main, 1), (Liberty Korea Party, 1), (Democratic Party, 1), (Cabinet, 1), (the National Assembly, 1), (Yonhap)The main, 1), (Liberty Korea Party, 1), (People's Party, 1), (Democratic Party, 1)]\n",
      "\tGPE: [(Seoul, 1), (Hong, 1), (Hong, 1), (Hong, 1), (Seoul, 1), (Hong, 1), (Hong, 1), (Seoul, 1), (Hong, 1), (Hong, 1)]\n",
      "Cluster id 4110\n",
      "\tGPE: [(Seoul, 1), (Seoul, 1), (Seoul, 1), (Seoul, 1), (Seoul, 1), (Seoul, 1), (Seoul, 1), (Jongno, 1), (Korea, 1), (Seoul, 1)]\n",
      "\tFAC: [(Seoul City Hall, 1), (Seoul City Hall, 1), (Seoul City Hall, 1), (Seoul City Hall, 1), (Dongdaemun Design Plaza, 1), (Dongdaemun Design Plaza, 1), (Dongdaemun Design Plaza, 1), (Dongdaemun Design Plaza, 1)]\n",
      "\tORG: [(Citizens, 1), (Citizens, 1), (The Seoul Metropolitan Government)During, 1), (Sampoong Department Store Collapse, 1), (the Seoul Metropolitan Government, 1), (Citizens, 1), (The Seoul Metropolitan Government)Under, 1), (Citizens, 1), (Sampoong Department Store Collapse, 1), (the Seoul Metropolitan Government, 1)]\n",
      "\tLANGUAGE: [(Yangjae, 1), (Yangjae, 1)]\n",
      "\tPERSON: [(Jeon Tae-il Bridge, 1), (Jung Yo-han, 1), (Gwak Na-yeon, 1), (Byun Seo, 1), (Kim Da-sol, 1), (Jeon Tae-il Bridge, 1), (Jung Yo-han, 1), (Gwak Na-yeon, 1), (Byun Seo, 1), (Kim Da-sol, 1)]\n",
      "\tWORK_OF_ART: [(Arts on Seoul’s street found by citizens, 1)]\n",
      "Cluster id 5033\n",
      "\tGPE: [(South Korea, 1), (Zika, 1), (Maldives, 1), (Zika, 1), (South Korea, 1), (Zika, 1), (Philippines, 1), (Daegu, 1), (Zika, 1), (Philippines, 1)]\n",
      "\tNORP: [(Korean, 1), (Korean, 1), (Southeast Asian, 1), (Southeast Asian, 1), (South Koreans, 1), (Southeast Asian, 1), (South American, 1), (Korean, 1), (Southeast Asian, 1), (South Koreans, 1)]\n",
      "\tORG: [(the Korea Centers for Disease Control and Prevention, 1), (KCDC, 1), (KCDC, 1), (the Korea Centers for Disease Control and Prevention, 1), (KCDC, 1), (Yonhap)The health, 1), (KCDC, 1), (KCDC, 1), (the Korea Centers for Disease Control and Prevention, 1), (KCDC, 1)]\n",
      "\tFAC: [(Yonhap)She, 1)]\n",
      "\tLOC: [(Africa, 1), (Central, 1), (Africa, 1), (Central, 1), (Africa, 1)]\n",
      "\tPERSON: [(Yonhap, 1), (Hwang Kyo-ahn, 1), (Yonhap, 1), (Yonhap, 1)]\n"
     ]
    }
   ],
   "source": [
    "for (cluster_id, number_event) in top10_event_2017:\n",
    "    candidates = df2017[tfidf_doc2017_event_labels == cluster_id]['body']\n",
    "    docs = nlp.pipe(candidates)\n",
    "    ent_list = dict()\n",
    "    for doc in docs:\n",
    "        for ent in doc.ents:\n",
    "            if not ent.label_ in ['MONEY','TIME','CARDINAL','QUANTITY','DATE','ORDINAL','PERCENT']:\n",
    "                if ent.label_ in ent_list:\n",
    "                    ent_list[ent.label_].append(ent)\n",
    "                else:\n",
    "                    ent_list[ent.label_] = [ent]\n",
    "    print(\"Cluster id {}\".format(cluster_id))\n",
    "    for k in ent_list:\n",
    "        v = ent_list[k]\n",
    "        ent_counter = Counter(v)\n",
    "        top10item = ent_counter.most_common(10)\n",
    "        print(\"\\t{}: {}\".format(k, top10item))\n",
    "#     print(ent_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b37b28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster id 16\n",
      "\tGPE: [(North Korea's, 1), (South Korea, 1), (North, 1), (Seoul, 1), (North Korea, 1), (South Korea, 1), (South Korea, 1), (Pyongyang, 1), (Koreas, 1), (North Korea's, 1)]\n",
      "\tORG: [(Radio Pyongyang, 1), (Radio Pyongyang, 1), (Radio Pyongyang, 1), (Radio Pyongyang, 1), (Radio Pyongyang, 1), (Workers' Party of Korea, 1)]\n",
      "\tLAW: [(Page 894, 1)]\n",
      "\tEVENT: [(the Cold War, 1), (the Cold War, 1), (the Cold War, 1), (the Cold War, 1), (the Cold War, 1)]\n",
      "\tPERSON: [(Yonhap, 1), (Yonhap, 1), (Yonhap, 1), (Yonhap, 1)]\n",
      "\tNORP: [(North Korean, 1)]\n",
      "\tLOC: [(North, 1)]\n",
      "Cluster id 5095\n",
      "\tNORP: [(South Korean, 1), (Koreans, 1)]\n",
      "\tORG: [(National Assembly, 1), (the National Election Commission, 1), (NEC, 1), (NEC, 1), (NEC, 1), (Saenuri Party, 1), (the National Election Commission, 1), (the National Election Commission, 1), (the National Election Commission, 1), (Daejeon, 1)]\n",
      "\tGPE: [(South Jeolla, 1), (Sejong, 1), (Daegu, 1), (Busan, 1), (Seoul, 1), (South Korea’s, 1), (South Jeolla Province, 1), (Daegu, 1), (Seoul, 1), (South Korea’s, 1)]\n",
      "\tPERSON: [(Yonhap, 1)]\n",
      "\tEVENT: [(Daegu, 1)]\n",
      "Cluster id 431\n",
      "\tPERSON: [(Park Geun-hye‘s, 1), (Park Geun-hye, 1), (Yonhap)The, 1), (Park, 1), (Gwanghwamun, 1), (Kim Seong-ju, 1), (Park, 1), (Park, 1), (Park, 1), (Choi Soon-sil, 1)]\n",
      "\tGPE: [(Seoul, 1), (Seoul, 1), (Busan, 1), (seoul, 1), (The Korea Herald, 1), (Seoul, 1), (Korea, 1), (Cheongwoon, 1), (Yeouido, 1), (Seoul, 1)]\n",
      "\tFAC: [(City Hall, 1), (Gwanghamun Square, 1), (Gyeongbok Palace, 1), (Gwanghwamun Square, 1), (Park, 1), (the Gwanghwamun Gage, 1), (Park, 1), (Dongdaemun Design Plaza, 1), (City Hall, 1), (Gwanghwamun Square, 1)]\n",
      "\tNORP: [(South Koreans, 1), (Sewol, 1), (Koreans, 1), (Koreans, 1), (South Koreans, 1), (South Koreans, 1), (Koreans, 1), (South Koreans, 1), (Koreans, 1)]\n",
      "\tORG: [(Gwangju, 1), (the National Assembly, 1), (the National Assembly, 1), (the National Assembly, 1), (the Saenuri Party, 1), (Saenuri Party, 1), (Saenuri Party, 1), (the Saenuri Party, 1), (The Saenuri Party, 1), (Saenuri Party, 1)]\n",
      "\tWORK_OF_ART: [(Park Geun-hye, 1), (Arrest Park Geun-hye, 1), (Seo Young-soo, 23, 1), (Park Geun-hye, 1), (Arrest Park Geun-hye, 1), (Seo Young-soo, 23, 1), (Seo Young-soo, 23, told the Korea Herald at the Gwanghwamun Square, 1)]\n",
      "Cluster id 518\n",
      "\tGPE: [(Yangju, 1), (Gyeonggi Province, 1), (Sejong, 1), (Eumseong, 1), (Haenam, 1), (South Jeolla, 1), (Gyeonggi Province, 1), (H5N6, 1), (South Korea, 1), (H5N6, 1)]\n",
      "\tORG: [(The Korea Centers for Disease Control and Prevention, 1), (The Ministry of Agriculture, Food and Rural Affairs, 1), (Paju, 1), (The Korea Centers for Disease Control and Prevention, 1), (The Ministry of Food, Agriculture and Rural Affairs, 1)]\n",
      "\tNORP: [(H5N6, 1), (H5N6, 1)]\n",
      "\tPERSON: [(Gimpo, 1), (Hwang Kyo-ahn, 1), (virogene, 1), (Kim Da-sol, 1), (Kim Da-sol, 1)]\n",
      "\tLOC: [(the H5N6-strain bird, 1)]\n",
      "Cluster id 706\n",
      "\tGPE: [(Deoksugung, 1), (Seoul, 1), (Seoul City, 1), (Seoul City, 1), (Korea, 1), (Seoul, 1), (Seoul, 1), (Seoul, 1), (Korea, 1), (Seoul, 1)]\n",
      "\tORG: [(The Seoul Metropolitan Government, 1), (the British Embassy, 1), (the British Embassy, 1), (The Korea Herald)“(Opening, 1), (the British Embassy, 1), (the British Embassy, 1), (the Seoul Metropolitan Government, 1), (the Ministry of Land, Infrastructure and Transport, 1), (Palace of Virtuous Longevity, 1), (SMG, 1)]\n",
      "\tFAC: [(the Deoksugung Stone Wall Walkway, 1), (Deoksu Palace, 1), (Stone Wall Walkway, 1), (the Deoksugung Stonewall Walkway, 1), (the Deoksugung Stonewall Walkway, 1), (Deoksu Palace, 1), (Deoksu Palace, 1), (the Deoksu Palace, 1), (Deoksugung Palace, 1), (the British Embassy, 1)]\n",
      "\tLOC: [(Central Seoul, 1), (Joseon, 1)]\n",
      "\tPERSON: [(Deoksugung Stone Wall Walkway, 1), (Deoksugung, 1), (Deoksugung, 1), (King Seonjo, 1), (Kim Da-sol, 1), (Choe Pan-sul, 1), (Gyeonghui, 1), (Changdeok, 1), (Yonhap, 1), (Choi Pan-sul, 1)]\n",
      "\tNORP: [(Japanese, 1), (million).The, 1), (Korean, 1), (Korean, 1), (British, 1), (Japanese, 1), (Western, 1)]\n",
      "\tEVENT: [(the Joseon Dynasty, 1)]\n",
      "\tWORK_OF_ART: [(doldam-gil, 1)]\n",
      "Cluster id 5782\n",
      "\tGPE: [(South Korea, 1), (Nonsan, 1), (South Chungcheong Province, 1), (Gongju, 1), (Cheonan, 1), (Paris, 1), (South Korea, 1), (Nonsan, 1), (Nonsan, 1), (South Chungcheong Province, 1)]\n",
      "\tLOC: [(South Chungcheong Province, 1), (South Chungcheong Province, 1), (South Chungcheong Province, 1)]\n",
      "\tORG: [(the Ministry of Agriculture, Food and Rural Affairs, 1), (Nonsan, 1), (FMD, 1), (FMD, 1), (World Organization for Animal Health, 1), (the Ministry of Agriculture, Food and Rural Affairs, 1), (FMD, 1), (FMD, 1), (World Organization for Animal Health, 1), (the Ministry of Agriculture, Food and Rural Affairs, 1)]\n",
      "\tPERSON: [(Yonhap, 1), (Yonhap, 1), (Yonhap, 1)]\n",
      "\tEVENT: [(the Lunar New Year's holiday, 1)]\n",
      "Cluster id 6234\n",
      "\tGPE: [(South Korea's, 1), (North Korea's, 1), (North, 1), (Seoul, 1), (the United States, 1), (North Korea, 1), (Pyongyang, 1), (Korea, 1), (U.S., 1), (North Korea's, 1)]\n",
      "\tORG: [(foreign ministry, 1), (the Foreign Ministry, 1), (Yonhap News Agency, 1), (Yonhap News Agency, 1), (the Wall Street Journal, 1), (State Department, 1), (the U.N. General Assembly, 1), (the Unification Ministry, 1), (the issue).\"Daniel Kritenbrink, 1), (the National Security Council, 1)]\n",
      "\tEVENT: [(the Korean War, 1), (the Korean War, 1), (the Korean War, 1), (the Korean War, 1), (the Korean War, 1), (the Korean War, 1)]\n",
      "\tLOC: [(North, 1), (North, 1), (North, 1), (North, 1), (North, 1), (North, 1), (North, 1)]\n",
      "\tPERSON: [(Barack Obama, 1), (John Kirby, 1), (Ri Su-yong, 1), (Jeong Joon-hee, 1), (Wang Yi, 1), (Yonhap, 1), (Barack Obama, 1), (John Kirby, 1), (Ri Su-yong, 1), (Jeong Joon-hee, 1)]\n",
      "\tNORP: [(the North Koreans, 1), (Asian, 1), (Chinese, 1), (the North Koreans, 1), (South Korean, 1), (Asian, 1), (Chinese, 1), (the North Koreans, 1), (Asian, 1), (Chinese, 1)]\n",
      "Cluster id 7166\n",
      "\tGPE: [(Korea, 1), (Korea, 1), (Korea, 1), (Korea, 1), (Korea, 1), (Korea, 1), (Korea, 1), (Korea, 1), (Korea, 1), (Korea, 1)]\n",
      "\tPERSON: [(Chung Hee-cho, 1), (Suh Ye-won, 1), (Suh, 1), (Chung Hee-cho, 1), (Suh, 1), (Suh, 1), (Suh, 1), (Yeo Jun-suk(jasonyeo@heraldcorp.com, 1), (Song Yoo-geun, 1), (Hong Kyeong-hee, 1)]\n",
      "\tORG: [(National Research Center for Gifted and Talented Education, 1), (The Korea Herald, 1), (Seoul Metropolitan Office of Education, 1), (the Education Ministry, 1), (The Minjoo Party, 1), (the World Without Worries About Private Education, 1), (the Korea Society for the Gifted, 1), (Korea Education Development Institute, 1), (the Education Ministry, 1), (the Education Ministry, 1)]\n",
      "\tLAW: [(the Constitution Article 31 Clause 1, 1), (the Promotion Act, 1), (the Act for Promoting Education for the Gifted, 1), (the Constitution Article 31 Clause 1, 1), (the Promotion Act, 1)]\n",
      "\tWORK_OF_ART: [(Axisymmetric, Nonstationary Black Hole Magnetospheres: Revisited, 1), (The System and Policy for the Education of the Gifted in Korea, 1), (Gifted Education Center, 1), (A World Without Worries About Private Education, 1)]\n",
      "\tNORP: [(Korean, 1), (Korean, 1), (English, 1)]\n",
      "Cluster id 11\n",
      "\tPERSON: [(Park Geun-hye, 1), (people‘s, 1), (Chung Sye-kyun, 1), (Choi Soon-sil, 1), (Chung Se-kyun, 1), (Joo Seung-yong, 1), (Sang-ho, 1), (Chung, 1), (Chung Woo-taik, 1), (Joo Ho-young, 1)]\n",
      "\tORG: [(Saenuri Party’s, 1), (Assembly, 1), (Assembly, 1), (National Assembly, 1), (the People‘s Party, 1), (the Democratic Party of Korea, 1), (the Saenuri Party, 1), (the New Conservative Party for Reform, 1), (National Assembly, 1), (Saenuri, 1)]\n",
      "\tGPE: [(Yeouido, 1), (Seoul, 1), (US, 1), (South Korea, 1), (North Korea’s, 1), (US, 1), (South Korea, 1), (North Korea’s, 1)]\n",
      "Cluster id 90\n",
      "\tNORP: [(South Korean, 1), (European, 1), (South Korean, 1), (European, 1)]\n",
      "\tPERSON: [(Park Geun-hye, 1), (Park, 1), (Choi Soon-sil, 1), (Ahn Jong-beom, 1), (Jeong Ho-seong, 1), (Choi, 1), (Choi, 1), (Choi, 1), (Choi, 1), (Choi, 1)]\n",
      "\tGPE: [(Seoul, 1), (Germany, 1), (North Korea, 1), (Seoul, 1), (Germany, 1), (North Korea, 1)]\n",
      "\tORG: [(the National Assembly, 1), (the National Assembly, 1), (Park, 1), (the National Assembly, 1), (Park, 1)]\n",
      "\tFAC: [(Mir, 1), (Mir, 1)]\n"
     ]
    }
   ],
   "source": [
    "for (cluster_id, number_event) in top10_event_2016:\n",
    "    candidates = df2016[tfidf_doc2016_event_labels == cluster_id]['body']\n",
    "    docs = nlp.pipe(candidates)\n",
    "    ent_list = dict()\n",
    "    for doc in docs:\n",
    "        for ent in doc.ents:\n",
    "            if not ent.label_ in ['MONEY','TIME','CARDINAL','QUANTITY','DATE','ORDINAL','PERCENT']:\n",
    "                if ent.label_ in ent_list:\n",
    "                    ent_list[ent.label_].append(ent)\n",
    "                else:\n",
    "                    ent_list[ent.label_] = [ent]\n",
    "    print(\"Cluster id {}\".format(cluster_id))\n",
    "    for k in ent_list:\n",
    "        v = ent_list[k]\n",
    "        ent_counter = Counter(v)\n",
    "        top10item = ent_counter.most_common(10)\n",
    "        print(\"\\t{}: {}\".format(k, top10item))\n",
    "#     print(ent_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "054aea85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster id 2954\n",
      "\tGPE: [(South Korea, 1), (South Korea, 1), (Saudi Arabia, 1), (South Korea, 1), (South Korea, 1), (Saudi Arabia, 1), (South Korea, 1), (South Korea, 1), (Saudi Arabia, 1), (South Korea, 1)]\n",
      "\tLOC: [(Middle East Respiratory Syndrome, 1), (Middle East Respiratory Syndrome, 1), (Middle East Respiratory Syndrome, 1), (Middle East Respiratory Syndrome, 1), (Middle East Respiratory Syndrome, 1), (Middle East Respiratory Syndrome, 1)]\n",
      "\tORG: [(the health ministry, 1), (the Ministry of Health and Welfare, 1), (MERS, 1), (MERS, 1), (the health ministry, 1), (the Ministry of Health and Welfare, 1), (MERS, 1), (MERS, 1), (MERS, 1), (the health ministry, 1)]\n",
      "\tPERSON: [(MERS, 1), (Yonhap, 1), (MERS, 1), (Yonhap, 1), (MERS, 1), (MERS, 1), (Yonhap, 1), (MERS, 1), (MERS, 1), (Yonhap, 1)]\n",
      "Cluster id 132\n",
      "\tGPE: [(South Korea, 1), (Statistics Korea, 1), (South Korea, 1), (South Korea, 1), (Statistics Korea, 1), (South Korea, 1), (South Korea, 1), (Statistics Korea, 1), (South Korea, 1), (South Korea, 1)]\n",
      "\tORG: [(the Organization for Economic Cooperation and Development, 1), (the Organization for Economic Cooperation and Development, 1), (the statistics office, 1), (the Organization for Economic Cooperation and Development, 1), (the statistics office, 1)]\n",
      "\tLOC: [(Asia, 1), (Asia, 1), (Asia, 1), (Asia, 1), (Asia, 1)]\n",
      "\tPERSON: [(Yonhap, 1), (Yonhap, 1), (Yonhap, 1), (Yonhap, 1), (Yonhap, 1)]\n",
      "Cluster id 5034\n",
      "\tGPE: [(South Korea's, 1), (South Korea's, 1), (South Korea's, 1), (South Korea's, 1), (South Korea's, 1), (South Korea's, 1)]\n",
      "\tORG: [(FSC, 1), (Financial Services Commission, 1), (central bank data, 1), (The Financial Services Commission, 1), (central bank data, 1), (The Financial Services Commission, 1), (The Korea Housing Finance Corp., 1), (the Bank of Korea, 1), (Chung-ang University, 1), (Chungnam National University, 1)]\n",
      "\tPERSON: [(Yim Jong-yong, 1), (Yim, 1), (Yonhap, 1), (Yonhap, 1), (Yeom Myung-bae, 1), (Shin Je-yoon, 1), (Yonhap, 1), (Shin Je-yoon, 1), (Yonhap, 1)]\n",
      "\tNORP: [(South Koreans, 1), (South Korean, 1)]\n",
      "\tWORK_OF_ART: [(Park Chang-kyun, 1)]\n",
      "Cluster id 5452\n",
      "\tGPE: [(South Korea, 1), (Japan, 1), (Tokyo, 1), (Seoul, 1), (South Korea, 1), (Japan, 1), (Seoul, 1), (Tokyo, 1), (Seoul, 1), (Seoul, 1)]\n",
      "\tNORP: [(Korean, 1), (Japanese, 1), (South Korean, 1), (Japanese, 1), (South Korean, 1), (Korean, 1), (Japanese, 1), (South Korean, 1), (Japanese, 1), (South Korean, 1)]\n",
      "\tEVENT: [(World War II, 1), (World War II, 1), (World War II, 1), (World War II, 1), (World War II.But, 1), (World War II, 1), (New Year, 1)]\n",
      "\tPERSON: [(Lee Sang-deok, 1), (Junichi Ihara, 1), (Shinzo Abe, 1), (Abe, 1), (Murayama, 1), (Park Geun-hye, 1), (Abe, 1), (Yonhap, 1), (Lee Sang-deok, 1), (Junichi Ihara, 1)]\n",
      "\tORG: [(the foreign ministry, 1), (the foreign ministry, 1), (World War, 1), (the foreign ministry, 1), (the foreign ministry, 1), (World War, 1)]\n",
      "\tLAW: [(the Kono Statement, 1), (the Kono Statement, 1), (the Kono Statement, 1), (the Kono Statement, 1)]\n",
      "Cluster id 6013\n",
      "\tGPE: [(Seoul, 1), (New York, 1), (Seoul, 1), (New York, 1), (Seoul, 1), (Park, 1), (Gimpo, 1), (Busan, 1), (New York, 1), (Seoul, 1)]\n",
      "\tPERSON: [(Cho Hyun-ah, 1), (Cho, 1), (Cho, 1), (Cho, 1), (Park Chang-jin, 1), (Cho, 1), (Cho, 1), (Cho, 1), (Cho Yang-ho, 1), (Cho Hyun-ah, 1)]\n",
      "\tORG: [(Korean Air, 1), (Korean Air, 1), (Hanjin Group, 1), (Korean Air, 1), (Transportation Ministry, 1), (Korean Air, 1), (Transport Ministry, 1), (Korean Air, 1), (Korean Air, 1), (Korean Air, 1)]\n",
      "\tFAC: [(The Seoul Seobu District Court, 1), (the Seoul Seobu District Court, 1), (the Seoul Seobu District Court, 1), (the Seoul Seobu District Court, 1)]\n",
      "Cluster id 1223\n",
      "\tNORP: [(South Koreans, 1), (German, 1), (German, 1), (South Korean, 1), (German, 1), (German, 1), (South Koreans, 1), (German, 1), (German, 1), (South Korean, 1)]\n",
      "\tORG: [(Volkswagen, 1), (Audi, 1), (Audi, 1), (Volkswagen Group, 1), (Audi Volkswagen Korea, 1), (Volkswagen Group, 1), (A4, 1), (A6, 1), (TDI, 1), (Q3, 1)]\n",
      "\tGPE: [(U.S., 1), (Seoul, 1), (the United States, 1), (South Korea, 1), (U.S., 1), (the United States, 1), (New Jersey, 1), (U.S., 1), (South Korea, 1), (South Korea, 1)]\n",
      "\tPRODUCT: [(A5, 1)]\n",
      "\tPERSON: [(Yonhap, 1), (Jason Ha, 1), (Quinn Emanuel, 1), (Yonhap, 1), (Jason Ha, 1), (Yonhap, 1)]\n",
      "Cluster id 1837\n",
      "\tGPE: [(South Korea's, 1), (Jeju, 1), (Jeju Island, 1), (Jeju, 1), (South Korea's, 1), (Jeju, 1), (Jeju, 1), (Jeju, 1), (Jeju Island, 1), (Jeju, 1)]\n",
      "\tORG: [(Coast Guard, 1), (the Korea Coast Guard, 1), (Coast Guard, 1), (the Korea Coast Guard, 1), (the Coast Guard, 1), (Coast Guard, 1), (the Korea Coast Guard, 1)]\n",
      "\tLOC: [(Chuja Island, 1), (Chuja Island, 1), (Jeju Island, 1), (Chuja Island, 1)]\n",
      "\tPERSON: [(Lee, 1), (Yonhap, 1), (Lee, 1), (Yonhap, 1), (Chang, 1), (Yonhap, 1)]\n",
      "\tPRODUCT: [(Dolphin, 1), (Dolphin, 1), (Dolphin, 1)]\n",
      "Cluster id 2166\n",
      "\tGPE: [(Green Korea United, 1), (Gangwon Province, 1), (Yangyang County, 1), (Yangyang County’s, 1), (Gyeonggi Province, 1), (Gangwon Province, 1), (Yangyang County, 1), (Seoul, 1), (Yangyang, 1), (Green Korea United, 1)]\n",
      "\tLOC: [(Mount Seoraksan, 1), (Mount Seoraksan, 1), (Mount Seoraksan, 1), (Mount Seoraksan, 1), (Mount Seoraksan, 1), (Mount Seoraksan, 1), (Mount Seoraksan, 1), (Mount Seoraksan, 1), (Mount Seoraksan, 1), (Chuncheon City, 1)]\n",
      "\tORG: [(national park committee, 1), (the Board of Audit and Inspection, 1), (the National Assembly, 1), (The Environment Ministry’s, 1), (Yangyang County Office of Gangwon Province, 1), (Gwacheon Government Complex, 1), (Yonhap)The, 1), (the national park committee, 1), (the county office, 1), (Seoraksan, 1)]\n",
      "\tPERSON: [(Yangyang, 1), (Lee Hyun-jeong, 1), (Yangyang, 1), (Park Geun-hye, 1), (Park, 1), (Lee Hyun-jeong, 1), (Yangyang, 1), (Yangyang, 1), (Lee Jang-kyo, 1), (Daecheongbong, 1)]\n",
      "\tEVENT: [(the PyeongChang Winter Olympics, 1), (the PyeongChang Winter Olympics, 1), (PyeongChang Winter Olympics, 1), (the PyeongChang Winter Olympics, 1)]\n",
      "Cluster id 2864\n",
      "\tNORP: [(South Korean, 1), (South Koreans, 1), (inter-Korean, 1), (South Korean, 1), (inter-Korean, 1), (South Korean, 1), (South Korean, 1), (North Korean, 1), (South Koreans, 1), (inter-Korean, 1)]\n",
      "\tGPE: [(North Korea, 1), (Korea, 1), (Japan, 1), (Kaesong, 1), (Koreas, 1), (Pyongyang, 1), (Seoul, 1), (Pyongyang, 1), (Panmunjom, 1), (Japan, 1)]\n",
      "\tLOC: [(North, 1), (South, 1), (North, 1), (North, 1), (North, 1), (South, 1), (North, 1), (North, 1), (North, 1), (The Korean Peninsula, 1)]\n",
      "\tORG: [(The Unification Ministry, 1), (Korean Central News Agency, 1), (KCNA, 1), (The Unification Ministry, 1), (Korean Central News Agency, 1), (The Unification Ministry, 1), (Korean Central News Agency, 1)]\n",
      "\tEVENT: [(Liberation Day, 1)]\n",
      "\tPERSON: [(Yonhap, 1), (Yonhap, 1), (Yonhap, 1)]\n",
      "Cluster id 3613\n",
      "\tPERSON: [(Choi Jin-kyu, 1), (bosintang, 1), (Gangnam-gu, 1), (Bosintang, 1), (Yeo Jun-suk, 1), (Choi Young, 1), (Choi, 1), (Park So-youn, 1), (Park, 1), (Yeo Jun-suk, 1)]\n",
      "\tORG: [(MERS, 1), (Dog Meat Farmers Association, 1), (Dongui Bogam, 1), (Coexistence of Animal Rights on Earth, 1), (the Ministry of Agriculture, Food and Rural Affairs, 1), (the Humane Society International, 1), (the Change for Animals Foundation, 1), (the Livestock Products Sanitary Control Act, 1), (the Humane Society International, 1), (the Change for Animals Foundation, 1)]\n",
      "\tLOC: [(Middle East, 1)]\n",
      "\tGPE: [(Seoul, 1), (Korea, 1), (Korea, 1), (South Korea’s, 1), (Seoul, 1), (Korea, 1), (U.S., 1), (South Korea, 1), (South Korea, 1), (South Korea, 1)]\n",
      "\tNORP: [(Koreans, 1), (Korean, 1), (Korean, 1), (Koreans, 1), (Koreans, 1), (South Korean, 1), (Koreans, 1), (South Korean, 1)]\n",
      "\tEVENT: [(the 1988 Seoul Olympics, 1), (Seoul Asian Games, 1), (Seoul Summer Olympics, 1), (Seoul Asian Games, 1), (Seoul Summer Olympics, 1)]\n"
     ]
    }
   ],
   "source": [
    "for (cluster_id, number_event) in top10_event_2015:\n",
    "    candidates = df2015[tfidf_doc2015_event_labels == cluster_id]['body']\n",
    "    docs = nlp.pipe(candidates)\n",
    "    ent_list = dict()\n",
    "    for doc in docs:\n",
    "        for ent in doc.ents:\n",
    "            if not ent.label_ in ['MONEY','TIME','CARDINAL','QUANTITY','DATE','ORDINAL','PERCENT']:\n",
    "                if ent.label_ in ent_list:\n",
    "                    ent_list[ent.label_].append(ent)\n",
    "                else:\n",
    "                    ent_list[ent.label_] = [ent]\n",
    "    print(\"Cluster id {}\".format(cluster_id))\n",
    "    for k in ent_list:\n",
    "        v = ent_list[k]\n",
    "        ent_counter = Counter(v)\n",
    "        top10item = ent_counter.most_common(10)\n",
    "        print(\"\\t{}: {}\".format(k, top10item))\n",
    "#     print(ent_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ed8058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9378ca0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
