{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c5bfb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import string\n",
    "import random\n",
    "import unicodedata\n",
    "from string import punctuation\n",
    "from string import digits\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "020a3d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23769, 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Reading data\n",
    "data_dir = 'data/'\n",
    "filename_prefix = 'koreaherald_1517_'\n",
    "df = []\n",
    "\n",
    "for i in range(8):\n",
    "    df.append(pd.read_json(os.path.join(data_dir, filename_prefix + str(i) + '.json')))\n",
    "df = pd.concat(df)\n",
    "df.reset_index(inplace=True)\n",
    "df = df.rename(columns=dict(zip(df.columns,[df.columns[i].strip() for i in range(len(df.columns))])))\n",
    "df.drop('index', inplace=True, axis=1)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07ae4d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load large spacy model \n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Aggregate title and content\n",
    "title_weight = 1\n",
    "df['agg_title_body'] = title_weight*(df['title']+'. ') + df['body']\n",
    "\n",
    "### Embed document and clustering\n",
    "df2017 = df['2017' < df['time']]\n",
    "df2016 = df[('2016' < df['time']) & (df['time'] < '2017')]\n",
    "df2015 = df[('2015' < df['time']) & (df['time'] < '2016')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b422933",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lemmatization tool\n",
    "stemmer = WordNetLemmatizer()\n",
    "### Change similar words to the same word\n",
    "UN_WORD = \"The United Nations\"\n",
    "US_WORD = \"The United States\"\n",
    "NK_WORD = \"North Korea\"\n",
    "SK_WORD = \"South Korea\"\n",
    "\n",
    "similar_words = {\n",
    "    # Change to \"The United States\"\n",
    "    \"U.S.\": US_WORD,\n",
    "    \"US\": US_WORD,\n",
    "    \"USA\": US_WORD,\n",
    "    \"United States\": US_WORD,\n",
    "    \"United States'\": US_WORD,\n",
    "    \"The United States'\": US_WORD,\n",
    "    \n",
    "    # Change to \"North Korea\"\n",
    "    \"NK\": NK_WORD,\n",
    "    \"NK's\": NK_WORD,\n",
    "    \"N. Korea\": NK_WORD,\n",
    "    \"N. Korea's\": NK_WORD,\n",
    "    \"North Korea's\": NK_WORD,\n",
    "    \n",
    "    # Change to \"South Korea\"\n",
    "    \"SK\": SK_WORD,\n",
    "    \"SK's\": SK_WORD,\n",
    "    \"S. Korea\": SK_WORD,\n",
    "    \"S. Korea's\": SK_WORD,\n",
    "    \"South Korea's\": SK_WORD,\n",
    "    \n",
    "    # Change to \"The United Nations\"\n",
    "    \"United Nations\": UN_WORD,\n",
    "    \"United Nations'\": UN_WORD,\n",
    "    \"The United Nations'\": UN_WORD,\n",
    "    \"UN\": UN_WORD,\n",
    "}\n",
    "\n",
    "### Transform function\n",
    "def text_cleaning(s: str):\n",
    "        \n",
    "    def replace_strange_char(s: str):\n",
    "        non_en_chars = {\n",
    "            \"’\": \"'\",\n",
    "            \"‘\": \"'\"\n",
    "        }\n",
    "\n",
    "        def remove_non_en_chars(txt):\n",
    "            # remove non english characters\n",
    "            txt = convert_latin_chars(txt)\n",
    "            for char in non_en_chars.keys():\n",
    "                txt = re.sub(char, non_en_chars[char], txt)\n",
    "            txt = re.sub(r'[^\\x00-\\x7F]+', ' ', txt)\n",
    "            return txt\n",
    "\n",
    "        def convert_latin_chars(txt):\n",
    "            # convert latin characters\n",
    "            return ''.join(char for char in unicodedata.normalize('NFKD', txt) if unicodedata.category(char) != 'Mn')\n",
    "\n",
    "        s = remove_non_en_chars(s)\n",
    "        s = convert_latin_chars(s)\n",
    "        return s\n",
    "    s = replace_strange_char(s)\n",
    "    for key,value in similar_words.items():\n",
    "        s = re.sub(key, value, s)\n",
    "    return s\n",
    "\n",
    "# Using spacy to preprocess\n",
    "def preprocess_spacy(s: str):\n",
    "    # Change similar terms to the same term\n",
    "    new_str = text_cleaning(s)\n",
    "    doc = nlp(s)\n",
    "    # Group tokens\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    token_groupup_pattern = [\n",
    "        [{\"LOWER\": \"the\"}, {\"LOWER\": \"united\"}, {\"LOWER\": \"nations\"}],\n",
    "        [{\"LOWER\": \"the\"}, {\"LOWER\": \"united\"}, {\"LOWER\": \"states\"}],\n",
    "        [{\"LOWER\": \"north\"}, {\"LOWER\": \"korea\"}],\n",
    "        [{\"LOWER\": \"south\"}, {\"LOWER\": \"korea\"}],\n",
    "    ]\n",
    "    matcher.add(\"TermGroup\",token_groupup_pattern)\n",
    "    matches = matcher(doc)\n",
    "    merge_doc = []\n",
    "    for nid, start, end in matches:\n",
    "        merge_doc.append((start,end))\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for i in range(len(merge_doc)-1,-1,-1):\n",
    "            retokenizer.merge(doc[merge_doc[i][0]:merge_doc[i][1]])\n",
    "        \n",
    "    # Remove all stopword, punctuation, number\n",
    "    tokens = [ token for token in doc if not token.is_stop and not token.is_punct and not token.like_num]\n",
    "    new_str = ' '.join([ token.lemma_.lower() for token in tokens ])\n",
    "    return new_str, tokens, doc\n",
    "\n",
    "def spacy_tokenizer(s: str):\n",
    "    # Change similar terms to the same term\n",
    "    new_str = text_cleaning(s)\n",
    "    doc = nlp(s)\n",
    "    # Group tokens\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    token_groupup_pattern = [\n",
    "        [{\"LOWER\": \"the\"}, {\"LOWER\": \"united\"}, {\"LOWER\": \"nations\"}],\n",
    "        [{\"LOWER\": \"the\"}, {\"LOWER\": \"united\"}, {\"LOWER\": \"states\"}],\n",
    "        [{\"LOWER\": \"north\"}, {\"LOWER\": \"korea\"}],\n",
    "        [{\"LOWER\": \"south\"}, {\"LOWER\": \"korea\"}],\n",
    "    ]\n",
    "    matcher.add(\"TermGroup\",token_groupup_pattern)\n",
    "    matches = matcher(doc)\n",
    "    merge_doc = []\n",
    "    for nid, start, end in matches:\n",
    "        merge_doc.append((start,end))\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for i in range(len(merge_doc)-1,-1,-1):\n",
    "            retokenizer.merge(doc[merge_doc[i][0]:merge_doc[i][1]])\n",
    "        \n",
    "    # Remove all stopword, punctuation, number\n",
    "    tokens = [ token.lemma_.lower() for token in doc \\\n",
    "              if not token.is_stop and not token.is_punct and not token.like_num and token.lemma_.strip()!= '']\n",
    "    return tokens\n",
    "\n",
    "### Preprocess function for grouping similar topic\n",
    "def preprocess_manual(s: str):\n",
    "    # Change similar words to the same word\n",
    "    new_str = transform_to_similar_sentence(s)\n",
    "    # Remove punctuation\n",
    "    new_str = ''.join(ch if ch not in set(punctuation) else \" \" for ch in new_str)\n",
    "    # Remove all single characters\n",
    "    new_str = re.sub(r'\\W', ' ', new_str)\n",
    "    new_str = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', new_str)\n",
    "    new_str = re.sub(r'\\^[a-zA-Z]\\s+', ' ', new_str) \n",
    "    # Substituting multiple spaces with single space\n",
    "    new_str = re.sub(r'\\s+', ' ', new_str, flags=re.I)\n",
    "    # Removing prefixed 'b' - when data is in bytes format\n",
    "    new_str = re.sub(r'^b\\s+', '', new_str)\n",
    "    # Removing all numbers\n",
    "    new_str = new_str.translate(str.maketrans('', '', digits))\n",
    "    # Converting to Lowercase\n",
    "    new_str = new_str.lower()\n",
    "    # Lemmatization and remove stopwords\n",
    "    new_str = new_str.split()\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    tokens = [stemmer.lemmatize(word) for word in new_str if word not in stopwords]\n",
    "    new_str = ' '.join(tokens)\n",
    "    \n",
    "    return new_str, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c30c4610",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clustering \n",
    "def document_clustering(doc_vectors, clustering_method='kmeans', evaluate=False):\n",
    "    if clustering_method=='kmeans':\n",
    "        # Hyperparameters\n",
    "        k_event = 10000\n",
    "        k_issue = 6000\n",
    "        \n",
    "        # Clustering event\n",
    "        kmeans_event = KMeans(n_clusters=k_event, random_state=69).fit(doc_vectors)\n",
    "        # Represent each event by average sum of related news\n",
    "        event_vectors = np.zeros((k_event, doc_vectors.shape[1]))\n",
    "        for i in range(k_event):\n",
    "            event_vectors[i] = sum(doc_vectors[kmeans_event.labels_ == i])\n",
    "        \n",
    "        # Clustering issue\n",
    "        kmeans_issue = KMeans(n_clusters=k_issue, random_state=69).fit(event_vectors)\n",
    "        # Represent each issue by average sum of related news\n",
    "        issue_vectors = np.zeros((k_issue, doc_vectors.shape[1]))\n",
    "        for i in range(k_issue):\n",
    "            issue_vectors[i] = sum(event_vectors[kmeans_issue.labels_ == i])\n",
    "\n",
    "        issue_labels = np.array([ kmeans_issue.labels_[kmeans_event.labels_[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        return k_issue, k_event, issue_labels, kmeans_event.labels_\n",
    "    \n",
    "    elif clustering_method=='DBSCAN':\n",
    "        \n",
    "        # Hyperparameters\n",
    "        doc_eps = 0.190\n",
    "        doc_neighbors = 1\n",
    "        event_eps = 0.50\n",
    "        event_neighbors = 1\n",
    "        \n",
    "        '''\n",
    "            Find best doc_eps and event_eps\n",
    "        '''\n",
    "        if evaluate:\n",
    "            # Find best eps to group same document\n",
    "            doc_eps_list = [ 0.10 + 0.001*i for i in range(1,301) ]\n",
    "            doc_score = []\n",
    "            doc_event = []\n",
    "            doc_best_score = 0\n",
    "            doc_best_eps = 0.0001\n",
    "            for doc_eps in doc_eps_list:\n",
    "                # Clustering event\n",
    "                db_event = DBSCAN(eps=doc_eps, min_samples=doc_neighbors).fit(doc_vectors)\n",
    "                # Number of clusters in labels, ignoring noise if present.\n",
    "                n_events_ = len(set(db_event.labels_)) - (1 if -1 in db_event.labels_ else 0)\n",
    "                if len(set(db_event.labels_)) >= 2 and len(set(db_event.labels_)) <= len(doc_vectors)-1:\n",
    "                    score_ = silhouette_score(doc_vectors, db_event.labels_)\n",
    "                else:\n",
    "                    score_ = -1\n",
    "                doc_event.append(n_events_)\n",
    "                doc_score.append(score_)\n",
    "                if score_ > doc_best_score:\n",
    "                    doc_best_score = score_\n",
    "                    doc_best_eps = doc_eps\n",
    "            print(\"Best Silhouete score is {} at eps: {} and number of events: {}\".format(doc_best_score, doc_eps, n_events_))\n",
    "            fig = plt.figure()\n",
    "            plt.plot(doc_eps_list, doc_score)\n",
    "            fig.suptitle('Doc eps and Silhouette score', fontsize=20)\n",
    "            plt.xlabel('eps', fontsize=18)\n",
    "            plt.ylabel('Silhouette score', fontsize=16)\n",
    "            plt.show()\n",
    "            \n",
    "            fig = plt.figure()\n",
    "            plt.plot(doc_eps_list, doc_event)\n",
    "            fig.suptitle('Doc eps and number of events', fontsize=20)\n",
    "            plt.xlabel('eps', fontsize=18)\n",
    "            plt.ylabel('number of events', fontsize=16)\n",
    "            plt.show()\n",
    "            \n",
    "            # Set doc_eps to the best value\n",
    "            doc_eps = doc_best_eps\n",
    "            # Find best eps to group same event\n",
    "            # Clustering event\n",
    "            db_event = DBSCAN(eps=doc_eps, min_samples=doc_neighbors).fit(doc_vectors)\n",
    "            n_events_ = len(set(db_event.labels_)) - (1 if -1 in db_event.labels_ else 0)\n",
    "            event_labels = np.array(list(map(lambda x: n_events_ if x==-1 else x, db_event.labels_)))\n",
    "            event_vectors = np.zeros((n_events_, doc_vectors.shape[1]))\n",
    "            for i in range(n_events_+1):\n",
    "                if np.sum(event_labels == i) != 0:\n",
    "                    event_vectors[i] = np.sum(doc_vectors[event_labels == i], axis=0)/np.sum(event_labels == i)\n",
    "            \n",
    "            \n",
    "#             # Clustering issue\n",
    "#             event_eps_list = [ 0.2 + 0.001*i for i in range(1,401) ]\n",
    "#             event_score = []\n",
    "#             event_issue = []\n",
    "#             event_best_score = 0\n",
    "#             event_best_eps = 0.001\n",
    "#             for event_eps in event_eps_list:\n",
    "#                 db_issue = DBSCAN(eps=event_eps, min_samples=event_neighbors).fit(event_vectors)\n",
    "#                 # Number of clusters in labels, ignoring noise if present.\n",
    "#                 n_issues_ = len(set(db_issue.labels_)) - (1 if -1 in db_issue.labels_ else 0)\n",
    "#                 if len(set(db_issue.labels_)) >= 2 and len(set(db_issue.labels_)) <= len(event_vectors)-1:\n",
    "#                     score_ = silhouette_score(event_vectors, db_issue.labels_)\n",
    "#                 else:\n",
    "#                     score_ = -1\n",
    "#                 event_issue.append(n_issues_)\n",
    "#                 event_score.append(score_)\n",
    "#                 if score_ > event_best_score:\n",
    "#                     event_best_score = score_\n",
    "#                     event_best_eps = event_eps\n",
    "#             print(\"Best Silhouete score is {} at eps: {} and number of issues: {}\".format(event_best_score, event_eps, n_issues_))\n",
    "#             fig = plt.figure()\n",
    "#             plt.plot(event_eps_list, event_score)\n",
    "#             fig.suptitle('Event eps and Silhouette score', fontsize=20)\n",
    "#             plt.xlabel('eps', fontsize=18)\n",
    "#             plt.ylabel('Silhouette score', fontsize=16)\n",
    "#             plt.show()\n",
    "            \n",
    "#             fig = plt.figure()\n",
    "#             plt.plot(event_eps_list, event_issue)\n",
    "#             fig.suptitle('Event eps and number of issues', fontsize=20)\n",
    "#             plt.xlabel('eps', fontsize=18)\n",
    "#             plt.ylabel('number of issues', fontsize=16)\n",
    "#             plt.show()\n",
    "            \n",
    "            # Set event_eps to best value\n",
    "            event_eps = 0.5\n",
    "            # Clustering issue\n",
    "            db_issue = DBSCAN(eps=event_eps, min_samples=event_neighbors).fit(event_vectors)\n",
    "            # Number of clusters in labels, ignoring noise if present.\n",
    "            n_issues_ = len(set(db_issue.labels_)) - (1 if -1 in db_issue.labels_ else 0)\n",
    "            n_noise_ = list(db_issue.labels_).count(-1)\n",
    "            # Represent each issue by average sum of related news\n",
    "            issue_labels = np.array(list(map(lambda x: n_issues_ if x==-1 else x, db_issue.labels_)))\n",
    "            issue_vectors = np.zeros((n_issues_, doc_vectors.shape[1]))\n",
    "            for i in range(n_issues_+1):\n",
    "                if np.sum(issue_labels == i) != 0:\n",
    "                    issue_vectors[i] = np.sum(event_vectors[issue_labels == i], axis=0)/np.sum(issue_labels == i)\n",
    "       \n",
    "            issue_labels = np.array([ issue_labels[event_labels[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        else:\n",
    "            '''\n",
    "            Clustering using specific value\n",
    "            '''\n",
    "            # Clustering event\n",
    "            db_event = DBSCAN(eps=doc_eps, min_samples=doc_neighbors).fit(doc_vectors)\n",
    "            # Number of clusters in labels, ignoring noise if present.\n",
    "            n_events_ = len(set(db_event.labels_)) - (1 if -1 in db_event.labels_ else 0)\n",
    "            n_noise_ = list(db_event.labels_).count(-1)\n",
    "            print(\"1st cluster:\\n\\tThe number of cluster is {}\".format(n_events_))\n",
    "            # Represent each event by average sum of related news\n",
    "            event_labels = np.array(list(map(lambda x: n_events_ if x==-1 else x, db_event.labels_)))\n",
    "            event_vectors = np.zeros((n_events_, doc_vectors.shape[1]))\n",
    "            for i in range(n_events_+1):\n",
    "                if np.sum(event_labels == i) != 0:\n",
    "                    event_vectors[i] = np.sum(doc_vectors[event_labels == i], axis=0)/np.sum(event_labels == i)\n",
    "\n",
    "            # Clustering issue\n",
    "            db_issue = DBSCAN(eps=event_eps, min_samples=event_neighbors).fit(event_vectors)\n",
    "            # Number of clusters in labels, ignoring noise if present.\n",
    "            n_issues_ = len(set(db_issue.labels_)) - (1 if -1 in db_issue.labels_ else 0)\n",
    "            n_noise_ = list(db_issue.labels_).count(-1)\n",
    "            print(\"2nd cluster:\\n\\tThe number of cluster is {}\".format(n_issues_))\n",
    "            # Represent each issue by average sum of related news\n",
    "            issue_labels = np.array(list(map(lambda x: n_issues_ if x==-1 else x, db_issue.labels_)))\n",
    "            issue_vectors = np.zeros((n_issues_, doc_vectors.shape[1]))\n",
    "            for i in range(n_issues_+1):\n",
    "                if np.sum(issue_labels == i) != 0:\n",
    "                    issue_vectors[i] = np.sum(event_vectors[issue_labels == i], axis=0)/np.sum(issue_labels == i)\n",
    "        \n",
    "            issue_labels = np.array([ issue_labels[event_labels[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        return n_issues_, n_events_, issue_labels, event_labels\n",
    "    \n",
    "    elif clustering_method=='agglomerative':\n",
    "        # Hyperparameters\n",
    "        n_events = 10000\n",
    "        n_issues = 6000\n",
    "        \n",
    "        # Clustering event\n",
    "        agg_event = AgglomerativeClustering(distance_threshold=0, n_clusters=n_events).fit(doc_vectors)\n",
    "        # Represent each event by average sum of related news\n",
    "        event_vectors = np.zeros((n_events, doc_vectors.shape[1]))\n",
    "        for i in range(n_events):\n",
    "            event_vectors[i] = sum(doc_vectors[agg_event.labels_ == i])\n",
    "        \n",
    "        plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "        # plot the top three levels of the dendrogram\n",
    "        plot_dendrogram(agg_event, truncate_mode=\"level\", p=3)\n",
    "        plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Clustering issue\n",
    "        agg_issue = AgglomerativeClustering(distance_threshold=0, n_clusters=n_issues).fit(event_vectors)\n",
    "        # Represent each issue by average sum of related news\n",
    "        issue_vectors = np.zeros((n_issues, doc_vectors.shape[1]))\n",
    "        for i in range(n_issues):\n",
    "            issue_vectors[i] = sum(event_vectors[agg_issue.labels_ == i])\n",
    "\n",
    "        issue_labels = np.array([ agg_issue.labels_[agg_event.labels_[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        return agg_issue, agg_event, issue_labels, agg_event.labels_\n",
    "    \n",
    "    elif clustering_method=='LDA':\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        assert(\"Doesn't support {}\".format(clustering_method))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "876542cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "tfidf_doc2017_vectors = joblib.load('tfidf_titlebody_2017.csv')\n",
    "tfidf_doc2016_vectors = joblib.load('tfidf_titlebody_2016.csv')\n",
    "tfidf_doc2015_vectors = joblib.load('tfidf_titlebody_2015.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "040b7bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st cluster:\n",
      "\tThe number of cluster is 6914\n",
      "2nd cluster:\n",
      "\tThe number of cluster is 1159\n",
      "1st cluster:\n",
      "\tThe number of cluster is 7335\n",
      "2nd cluster:\n",
      "\tThe number of cluster is 1244\n",
      "1st cluster:\n",
      "\tThe number of cluster is 8997\n",
      "2nd cluster:\n",
      "\tThe number of cluster is 1517\n"
     ]
    }
   ],
   "source": [
    "tfidf_doc2015_num_issue, tfidf_doc2015_num_event, tfidf_doc2015_issue_labels, tfidf_doc2015_event_labels = document_clustering(tfidf_doc2015_vectors, clustering_method='DBSCAN', evaluate=False)\n",
    "tfidf_doc2016_num_issue, tfidf_doc2016_num_event, tfidf_doc2016_issue_labels, tfidf_doc2016_event_labels = document_clustering(tfidf_doc2016_vectors, clustering_method='DBSCAN', evaluate=False)\n",
    "tfidf_doc2017_num_issue, tfidf_doc2017_num_event, tfidf_doc2017_issue_labels, tfidf_doc2017_event_labels = document_clustering(tfidf_doc2017_vectors, clustering_method='DBSCAN', evaluate=False)\n",
    "\n",
    "from collections import Counter\n",
    "counter_2017_event = Counter(tfidf_doc2017_event_labels)\n",
    "counter_2016_event = Counter(tfidf_doc2016_event_labels)\n",
    "counter_2015_event = Counter(tfidf_doc2015_event_labels)\n",
    "\n",
    "top10_event_2017 = counter_2017_event.most_common(10)\n",
    "top10_event_2016 = counter_2016_event.most_common(10)\n",
    "top10_event_2015 = counter_2015_event.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "891e0ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster id 67\n",
      "\tGPE: [('Seoul', 19), ('Gangwon Province', 13), ('Busan', 9), ('Gyeonggi Province', 8), ('Incheon', 5), ('Chungcheong', 5), ('Korea', 4), ('Daegu', 3), ('Gangneung', 2), ('Gwangju', 2)]\n",
      "\tORG: [('Chuncheon', 9), ('Daejeon', 7), ('the Korea Meteorological Administration', 3), ('KMA', 3), ('Yonhap)The Korea Meteorological Administration', 3), ('Cheongju', 2), ('The Korea Meteorological Administration', 2), ('Yonhap)The KMA', 1), ('mercury', 1), ('C.The KMA', 1)]\n",
      "\tNORP: [('Yonhap)The', 1)]\n",
      "\tPERSON: [('Park Ju-young', 11), ('Gyeonggi', 7), ('North Chungcheong', 2), ('Gangwon', 1), ('Gyeongsang', 1)]\n",
      "\tFAC: [('Gangneung 2', 1)]\n",
      "\tLOC: [('west coast', 2), ('Jeju Island', 1), ('Northern Gyeonggi Province', 1)]\n",
      "Cluster id 617\n",
      "\tNORP: [('Korean', 6), ('Japanese', 2), ('South Korean', 1), ('Gunbuk', 1), ('Russian', 1)]\n",
      "\tGPE: [('South Korea', 3), ('Gangwon Province', 3), ('Seoul', 2), ('Haman', 1), ('South Gyeongsang Province', 1), ('Yonhap)The', 1), ('Yanggu', 1), ('Korea', 1), ('Mount Baekdu', 1), ('London', 1)]\n",
      "\tPERSON: [('Yonhap', 3), ('Lee', 2), ('Gregory PenceHomepage', 2), ('Paik In-sung', 1), ('Paik', 1), ('Gorals', 1), ('Mount Jiri', 1), ('Ahn Jae-yong', 1), ('Park Hyun-koo', 1), ('phko@heraldcorp.com)Written', 1)]\n",
      "\tORG: [('Seoul Zoo', 2), ('Pukyong National University', 1), ('Nature', 1), ('the Association of Korean Goral Conservation', 1), ('Gorals', 1), ('Tiger Alliance', 1), ('the World Association of Zoos and Aquariums', 1), ('Yonhap)\"We', 1), ('ALTA', 1), ('Gayasan National Park)The Ministry of Environment', 1)]\n",
      "\tWORK_OF_ART: [('Scientific Reports', 1)]\n",
      "\tLOC: [('Amur leopards', 2), ('the Korean Peninsula', 1), ('Europe', 1), ('Far East', 1), ('The Korean Peninsula', 1), ('Korean Peninsula', 1)]\n",
      "\tFAC: [('the China-North Korea', 1)]\n",
      "\tPRODUCT: [('Amur', 6), ('Amur Leopard', 1)]\n",
      "\tLAW: [('Amur', 1)]\n",
      "\tEVENT: [('the Joseon Dynasty', 1)]\n",
      "Cluster id 3706\n",
      "\tGPE: [('South Korea', 9), ('Statistics Korea', 5), ('Seoul', 1)]\n",
      "\tORG: [('Yonhap)The', 1)]\n",
      "\tLOC: [('Asia', 5)]\n",
      "\tPERSON: [('Yonhap', 5), ('Yonhap)The', 1)]\n",
      "\tNORP: [('South Korean', 1)]\n",
      "Cluster id 4733\n",
      "\tNORP: [('South Korean', 5), ('Technology)The', 1)]\n",
      "\tORG: [('ICT', 4), ('the Ministry of Science', 2), ('the science ministry', 2), ('the Institute for Basic Science', 2), ('IBS', 2), ('Sungkyunkwan University', 1), ('Future Planning', 1), ('the Korea Electrotechnology Research Institute', 1), ('the Korea Electrotechnology Research Institute)\"To', 1), ('MWNT', 1)]\n",
      "\tPERSON: [('Yonhap', 5), ('Bang Chang-hyun', 1), ('Bang', 1), ('Seol Seung-kwon', 1), ('Seol', 1), ('Son Jeong-gon', 1), ('Han Chang-soo', 1), ('Lee Hyun-jae', 1), ('Lee', 1)]\n",
      "\tWORK_OF_ART: [('Nature', 1), ('ACSNano', 1), ('Advanced Materials', 1), ('Science Advances', 1)]\n",
      "\tPRODUCT: [('3D', 1)]\n",
      "Cluster id 5\n",
      "\tGPE: [('South Korea', 9), ('Seoul', 6), ('Yeongam', 5), ('Gochang', 4), ('Jeolla Province', 4), ('H5N6', 2), ('Jeju', 2), ('Jeongeup', 1), ('Cheonan', 1), (\"South Korea's\", 1)]\n",
      "\tORG: [('the agriculture ministry', 3), ('the Ministry of Agriculture, Food and Rural Affairs', 3), ('the World Health Organization', 3), ('The Animal and Plant Quarantine Agency', 1), ('APQA', 1)]\n",
      "\tPERSON: [('Yonhap', 4), ('Yeongam', 1)]\n",
      "\tNORP: [('South Korean', 1), ('H6N6', 1), ('H5N6', 1)]\n",
      "\tLOC: [('Europe', 1), ('Jeju Island', 1)]\n",
      "Cluster id 6652\n",
      "\tORG: [('the Brazilian Air Force', 4), ('Polaris Shipping', 2), ('The Brazilian Air Force', 1), ('Navy', 1), (\"Polaris Shipping's\", 1)]\n",
      "\tNORP: [('South Korean', 18), ('Filipino', 8), ('Brazilian', 4), ('South Koreans', 2), ('Filipinos', 1)]\n",
      "\tLOC: [('the South Atlantic', 6), ('Yonhap)The Marshall Islands', 1)]\n",
      "\tFAC: [('C-130', 4)]\n",
      "\tPRODUCT: [('the Stellar Daisy', 1), ('Polaris Shipping', 1)]\n",
      "\tPERSON: [('Stella Daisy', 6), ('Yonhap', 3), ('Stellar Daisy', 1), ('Yonhap)The Marshall Islands', 1)]\n",
      "\tGPE: [('Uruguay', 6), ('Brazil', 5), ('Seoul', 4), ('The Marshall Islands', 2), (\"South Korea's\", 2), ('China', 2), ('Rio de Janeiro', 2), ('Argentina', 1), ('Busan', 1)]\n",
      "Cluster id 813\n",
      "\tNORP: [('South Korean', 9), ('American', 3), ('Chinese', 1)]\n",
      "\tPERSON: [('Bang', 19), ('Bang Tae-hyun', 6), ('Leo Kuntz', 6), ('Kuntz', 6), ('Kim', 5), ('Yonhap', 3), ('Yang', 3), ('Nick Hein', 2), ('Kim Dae-won', 1)]\n",
      "\tORG: [('UFC', 17), ('MMA', 7), ('UFC Fight Night 79', 2), ('The Seoul Central District Court', 1), ('UFC Fight Night', 1), (\"the Seoul Central District Prosecutors' Office\", 1), ('Pride', 1), ('Bang 100 million', 1)]\n",
      "\tFAC: [('the Ultimate Fighting Championship event', 2), ('the UFC Fight Night 93', 1), ('the UFC Fight Night 79', 1)]\n",
      "\tGPE: [('Seoul', 7), ('South Korea', 3), ('the United States', 3), ('Germany', 2), ('US', 1), ('Japan', 1), ('Las Vegas', 1)]\n",
      "\tEVENT: [('Olympic Gymnastics Arena', 3), ('the UFC Fight Night 79', 2)]\n",
      "\tLOC: [(\"Seoul Central District Prosecutors' Office\", 1)]\n",
      "Cluster id 911\n",
      "\tPERSON: [('Hong Jong-haak', 6), ('Moon', 6), ('Moon Jae', 3), ('Yonhap', 3)]\n",
      "\tORG: [('the National Assembly', 3), ('Liberty Korea Party', 3), ('Democratic Party', 3), ('Cabinet', 3), ('Yonhap)The main', 2), (\"People's Party\", 2), ('Ministry of SMEs and Startups', 1)]\n",
      "\tGPE: [('Hong', 8), ('Seoul', 3)]\n",
      "Cluster id 4110\n",
      "\tGPE: [('Seoul', 26), ('Seoul City', 4), ('Jongno', 2), ('Korea', 2), ('Seoul City’s', 1)]\n",
      "\tFAC: [('Seoul City Hall', 4), ('Dongdaemun Design Plaza', 4)]\n",
      "\tORG: [('Citizens', 4), ('Sampoong Department Store Collapse', 2), ('the Seoul Metropolitan Government', 2), ('The Seoul Metropolitan', 2), ('The Seoul Metropolitan Government)During', 1), ('The Seoul Metropolitan Government)Under', 1), ('the Seoul Metropolitan Government’s', 1), ('Hongik University', 1), ('Jeon', 1), ('The Seoul Metropolitan Government)Once', 1)]\n",
      "\tLANGUAGE: [('Yangjae', 2)]\n",
      "\tPERSON: [('Kim Da-sol', 3), ('Ahn Kyu-chul', 3), ('Jeon Tae-il Bridge', 2), ('Jung Yo-han', 2), ('Gwak Na-yeon', 2), ('Byun Seo', 2), ('Byun Seo-yeon', 1), ('Kim Min-yeob', 1), ('Kim', 1), ('Yeonnam-dong', 1)]\n",
      "\tWORK_OF_ART: [('Arts on Seoul’s street found by citizens', 1)]\n",
      "Cluster id 5033\n",
      "\tGPE: [('Zika', 5), ('Philippines', 4), ('South Korea', 3), ('Vietnam', 2), ('Brazil', 2), ('Maldives', 1), ('Daegu', 1), ('Seoul', 1)]\n",
      "\tNORP: [('Southeast Asian', 5), ('Korean', 3), ('South Koreans', 2), ('South American', 2)]\n",
      "\tORG: [('KCDC', 7), ('the Korea Centers for Disease Control and Prevention', 3), ('Yonhap)The health', 1), ('Yonhap)The KCDC', 1)]\n",
      "\tFAC: [('Yonhap)She', 1)]\n",
      "\tLOC: [('Africa', 3), ('Central', 2)]\n",
      "\tPERSON: [('Yonhap', 3), ('Hwang Kyo-ahn', 1)]\n"
     ]
    }
   ],
   "source": [
    "for (cluster_id, number_event) in top10_event_2017:\n",
    "    candidates = df2017[tfidf_doc2017_event_labels == cluster_id]['body']\n",
    "    docs = nlp.pipe(candidates)\n",
    "    ent_list = dict()\n",
    "    for doc in docs:\n",
    "        for ent in doc.ents:\n",
    "            if not ent.label_ in ['MONEY','TIME','CARDINAL','QUANTITY','DATE','ORDINAL','PERCENT']:\n",
    "                if ent.label_ in ent_list:\n",
    "                    ent_list[ent.label_].append(ent.text)\n",
    "                else:\n",
    "                    ent_list[ent.label_] = [ent.text]\n",
    "    print(\"Cluster id {}\".format(cluster_id))\n",
    "    for k in ent_list:\n",
    "        v = ent_list[k]\n",
    "        ent_counter = Counter(v)\n",
    "        top10item = ent_counter.most_common(10)\n",
    "        print(\"\\t{}: {}\".format(k, top10item))\n",
    "#     print(ent_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c99ada82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster id 16\n",
      "\tGPE: [('South Korea', 14), ('North Korea', 12), ('Seoul', 6), (\"North Korea's\", 5), ('Pyongyang', 5), ('Koreas', 5), ('North', 1), ('Korea', 1), ('Washington', 1)]\n",
      "\tORG: [('Radio Pyongyang', 5), (\"Workers' Party of Korea\", 1)]\n",
      "\tLAW: [('Page 894', 1)]\n",
      "\tEVENT: [('the Cold War', 5)]\n",
      "\tPERSON: [('Yonhap', 4)]\n",
      "\tNORP: [('North Korean', 1)]\n",
      "\tLOC: [('North', 1)]\n",
      "Cluster id 5095\n",
      "\tNORP: [('South Korean', 1), ('Koreans', 1)]\n",
      "\tORG: [('the National Election Commission', 4), ('NEC', 3), ('National Assembly', 1), ('Saenuri Party', 1), ('Daejeon', 1)]\n",
      "\tGPE: [('Seoul', 4), ('South Korea’s', 3), ('Daegu', 2), ('South Jeolla Province', 2), ('South Jeolla', 1), ('Sejong', 1), ('Busan', 1), ('Gangwon Province', 1)]\n",
      "\tPERSON: [('Yonhap', 1)]\n",
      "\tEVENT: [('Daegu', 1)]\n",
      "Cluster id 431\n",
      "\tPERSON: [('Park', 28), ('Park Geun-hye', 14), ('Choi Soon-sil', 6), ('Kim Seong-ju', 3), ('Choi', 3), ('Cheong Wa Dae', 3), ('Kim Hye-yeon', 2), ('Choi Jeong-seon', 2), ('Park Jae-young', 2), ('Ock Hyun-ju & Bak Se-hwan', 2)]\n",
      "\tGPE: [('Seoul', 21), ('Yeouido', 4), ('Gallup Korea', 3), ('Busan', 2), ('The Korea Herald', 2), ('Cheongwoon', 2), ('seoul', 1), ('Korea', 1)]\n",
      "\tFAC: [('Gwanghwamun Square', 4), ('Dongdaemun Design Plaza', 4), ('City Hall', 2), ('Gyeongbok Palace', 2), ('Park', 2), ('Gwanghamun Square', 1), ('the Gwanghwamun Gage', 1)]\n",
      "\tNORP: [('South Koreans', 4), ('Koreans', 4), ('Sewol', 1)]\n",
      "\tORG: [('the Saenuri Party', 11), ('the National Assembly', 8), ('Saenuri Party', 5), ('Gwangju', 2), ('Assembly', 2), ('The Saenuri Party', 1), ('Sanuri Party', 1), ('the Saenuri Party’s', 1), ('YonhapBlasting', 1)]\n",
      "\tWORK_OF_ART: [('Park Geun-hye', 2), ('Arrest Park Geun-hye', 2), ('Seo Young-soo, 23', 2), ('Seo Young-soo, 23, told the Korea Herald at the Gwanghwamun Square', 1)]\n",
      "Cluster id 518\n",
      "\tGPE: [('Gyeonggi Province', 7), ('Yangju', 5), ('H5N6', 5), ('China', 4), ('Haenam', 3), ('South Jeolla', 3), ('South Korea', 3), ('Sejong', 2), ('Eumseong', 2), ('North Chungcheong Province', 2)]\n",
      "\tORG: [('The Korea Centers for Disease Control and Prevention', 2), ('The Ministry of Agriculture, Food and Rural Affairs', 1), ('Paju', 1), ('The Ministry of Food, Agriculture and Rural Affairs', 1)]\n",
      "\tNORP: [('H5N6', 2)]\n",
      "\tPERSON: [('Kim Da-sol', 2), ('Gimpo', 1), ('Hwang Kyo-ahn', 1), ('virogene', 1)]\n",
      "\tLOC: [('the H5N6-strain bird', 1)]\n",
      "Cluster id 706\n",
      "\tGPE: [('Seoul', 6), ('Seoul City', 3), ('Korea', 3), ('Deoksugung', 1), ('Gyeongbok', 1), ('Changgyeong', 1), ('Jeongdong', 1), ('U.K.', 1)]\n",
      "\tORG: [('the British Embassy', 10), ('the Seoul Metropolitan Government', 3), ('The Seoul Metropolitan Government', 1), ('The Korea Herald)“(Opening', 1), ('the Ministry of Land, Infrastructure and Transport', 1), ('Palace of Virtuous Longevity', 1), ('SMG', 1), ('the Seoul Metropolitan Council', 1), ('The Cultural Heritage Administration', 1), ('Seoul City Council', 1)]\n",
      "\tFAC: [('Deoksu Palace', 3), ('Deoksugung Palace', 3), ('the Deoksugung Stonewall Walkway', 2), ('the Deoksugung Stone Wall Walkway', 1), ('Stone Wall Walkway', 1), ('the Deoksu Palace', 1), ('the British Embassy', 1)]\n",
      "\tLOC: [('Central Seoul', 1), ('Joseon', 1)]\n",
      "\tPERSON: [('Deoksugung', 2), ('Deoksugung Stone Wall Walkway', 1), ('King Seonjo', 1), ('Kim Da-sol', 1), ('Choe Pan-sul', 1), ('Gyeonghui', 1), ('Changdeok', 1), ('Yonhap', 1), ('Choi Pan-sul', 1), ('Lee Hyun-jeong', 1)]\n",
      "\tNORP: [('Japanese', 2), ('Korean', 2), ('million).The', 1), ('British', 1), ('Western', 1)]\n",
      "\tEVENT: [('the Joseon Dynasty', 1)]\n",
      "\tWORK_OF_ART: [('doldam-gil', 1)]\n",
      "Cluster id 5782\n",
      "\tGPE: [('South Korea', 3), ('Nonsan', 3), ('Gongju', 3), ('Cheonan', 3), ('Paris', 3), ('South Chungcheong Province', 2), ('North Jeolla', 1)]\n",
      "\tLOC: [('South Chungcheong Province', 3)]\n",
      "\tORG: [('FMD', 6), ('the Ministry of Agriculture, Food and Rural Affairs', 3), ('World Organization for Animal Health', 3), ('Nonsan', 1)]\n",
      "\tPERSON: [('Yonhap', 3)]\n",
      "\tEVENT: [(\"the Lunar New Year's holiday\", 1)]\n",
      "Cluster id 6234\n",
      "\tGPE: [('U.S.', 24), ('Pyongyang', 19), ('North Korea', 15), ('Washington', 10), (\"North Korea's\", 5), ('Iran', 3), ('Seoul', 2), ('South Korea', 2), ('The United States', 2), ('New York', 2)]\n",
      "\tORG: [('Yonhap News Agency', 7), ('the Wall Street Journal', 3), ('State Department', 3), ('the U.N. General Assembly', 3), ('the National Security Council', 3), ('the Unification Ministry', 2), ('the issue).\"Daniel Kritenbrink', 2), ('the United Nations', 2), ('foreign ministry', 1), ('the Foreign Ministry', 1)]\n",
      "\tEVENT: [('the Korean War', 6)]\n",
      "\tLOC: [('North', 7)]\n",
      "\tPERSON: [('Barack Obama', 3), ('John Kirby', 3), ('Ri Su-yong', 3), ('Wang Yi', 3), ('Yonhap', 3), ('Jeong Joon-hee', 2), ('Daniel Kritenbrink', 1)]\n",
      "\tNORP: [('the North Koreans', 3), ('Asian', 3), ('Chinese', 3), ('South Korean', 1)]\n",
      "Cluster id 7166\n",
      "\tGPE: [('Korea', 12)]\n",
      "\tPERSON: [('Suh', 8), ('Suh Ye-won', 4), ('Chung Hee-cho', 2), ('Kim', 2), ('Chung Hee-cho/', 2), ('Yeo Jun-suk(jasonyeo@heraldcorp.com', 1), ('Song Yoo-geun', 1), ('Hong Kyeong-hee', 1), ('Yoo Ki-hong', 1), ('Kim Jeong-yeon', 1)]\n",
      "\tORG: [('National Research Center for Gifted and Talented Education', 4), ('the Education Ministry', 3), ('The Korea Herald', 2), ('Seoul Metropolitan Office of Education', 1), ('The Minjoo Party', 1), ('the World Without Worries About Private Education', 1), ('the Korea Society for the Gifted', 1), ('Korea Education Development Institute', 1), ('Seoul National University of Education', 1), ('HeraldThe', 1)]\n",
      "\tLAW: [('the Constitution Article 31 Clause 1', 2), ('the Promotion Act', 2), ('the Act for Promoting Education for the Gifted', 1)]\n",
      "\tWORK_OF_ART: [('Axisymmetric, Nonstationary Black Hole Magnetospheres: Revisited', 1), ('The System and Policy for the Education of the Gifted in Korea', 1), ('Gifted Education Center', 1), ('A World Without Worries About Private Education', 1)]\n",
      "\tNORP: [('Korean', 2), ('English', 1)]\n",
      "Cluster id 11\n",
      "\tPERSON: [('Park Geun-hye', 2), ('people‘s', 2), ('Chung Sye-kyun', 2), ('Choi Soon-sil', 2), ('Woo Sang-ho', 2), ('Woo', 2), ('Pyeongyang', 2), ('Chung Se-kyun', 1), ('Joo Seung-yong', 1), ('Sang-ho', 1)]\n",
      "\tORG: [('Saenuri', 10), ('Assembly', 6), ('People’s Party', 6), ('National Assembly', 5), ('the Democratic Party of Korea', 3), ('Saenuri Party’s', 2), ('New Conservative Party for Reform', 2), ('the Democratic Party', 2), ('THAAD', 2), ('The Democratic Party', 2)]\n",
      "\tGPE: [('US', 2), ('South Korea', 2), ('North Korea’s', 2), ('Yeouido', 1), ('Seoul', 1)]\n",
      "Cluster id 90\n",
      "\tNORP: [('South Korean', 2), ('European', 2)]\n",
      "\tPERSON: [('Choi', 16), ('Park', 4), ('Park Geun-hye', 2), ('Choi Soon-sil', 2), ('Ahn Jong-beom', 2), ('Jeong Ho-seong', 2), ('Ahn', 2), ('Choi Tae-min', 2), ('Yook Young-soo', 2), ('Yonhap', 2)]\n",
      "\tGPE: [('Seoul', 2), ('Germany', 2), ('North Korea', 2)]\n",
      "\tORG: [('the National Assembly', 3), ('Park', 2)]\n",
      "\tFAC: [('Mir', 2)]\n"
     ]
    }
   ],
   "source": [
    "for (cluster_id, number_event) in top10_event_2016:\n",
    "    candidates = df2016[tfidf_doc2016_event_labels == cluster_id]['body']\n",
    "    docs = nlp.pipe(candidates)\n",
    "    ent_list = dict()\n",
    "    for doc in docs:\n",
    "        for ent in doc.ents:\n",
    "            if not ent.label_ in ['MONEY','TIME','CARDINAL','QUANTITY','DATE','ORDINAL','PERCENT']:\n",
    "                if ent.label_ in ent_list:\n",
    "                    ent_list[ent.label_].append(ent.text)\n",
    "                else:\n",
    "                    ent_list[ent.label_] = [ent.text]\n",
    "    print(\"Cluster id {}\".format(cluster_id))\n",
    "    for k in ent_list:\n",
    "        v = ent_list[k]\n",
    "        ent_counter = Counter(v)\n",
    "        top10item = ent_counter.most_common(10)\n",
    "        print(\"\\t{}: {}\".format(k, top10item))\n",
    "#     print(ent_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6b3972c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster id 2954\n",
      "\tGPE: [('South Korea', 12), ('Saudi Arabia', 6)]\n",
      "\tLOC: [('Middle East Respiratory Syndrome', 6)]\n",
      "\tORG: [('MERS', 18), ('the Ministry of Health and Welfare', 6), ('the health ministry', 4)]\n",
      "\tPERSON: [('MERS', 6), ('Yonhap', 6)]\n",
      "Cluster id 132\n",
      "\tGPE: [('South Korea', 12), ('Statistics Korea', 5)]\n",
      "\tORG: [('the Organization for Economic Cooperation and Development', 3), ('the statistics office', 2)]\n",
      "\tLOC: [('Asia', 5)]\n",
      "\tPERSON: [('Yonhap', 5)]\n",
      "Cluster id 5034\n",
      "\tGPE: [(\"South Korea's\", 6)]\n",
      "\tORG: [('FSC', 7), ('The Financial Services Commission', 3), ('central bank data', 2), ('The Korea Housing Finance Corp.', 2), ('Financial Services Commission', 1), ('the Bank of Korea', 1), ('Chung-ang University', 1), ('Chungnam National University', 1), ('MBS', 1)]\n",
      "\tPERSON: [('Yonhap', 4), ('Shin Je-yoon', 2), ('Yim Jong-yong', 1), ('Yim', 1), ('Yeom Myung-bae', 1)]\n",
      "\tNORP: [('South Koreans', 1), ('South Korean', 1)]\n",
      "\tWORK_OF_ART: [('Park Chang-kyun', 1)]\n",
      "Cluster id 5452\n",
      "\tGPE: [('Japan', 36), ('Seoul', 29), ('Tokyo', 26), ('South Korea', 5), ('Beijing', 4), ('Park', 1)]\n",
      "\tNORP: [('South Korean', 9), ('Japanese', 8), ('Korean', 4)]\n",
      "\tEVENT: [('World War II', 5), ('World War II.But', 1), ('New Year', 1)]\n",
      "\tPERSON: [('Abe', 5), ('Lee Sang-deok', 4), ('Junichi Ihara', 4), ('Shinzo Abe', 4), ('Murayama', 4), ('Park Geun-hye', 4), ('Yonhap', 4), ('Park', 1), ('Yun Byung-se', 1), ('Yun', 1)]\n",
      "\tORG: [('the foreign ministry', 4), ('World War', 2)]\n",
      "\tLAW: [('the Kono Statement', 4)]\n",
      "Cluster id 6013\n",
      "\tGPE: [('Seoul', 5), ('New York', 4), ('Park', 3), ('Gimpo', 3), ('Busan', 3)]\n",
      "\tPERSON: [('Cho', 35), ('Yeo', 11), ('Park', 9), ('Kim', 8), ('Cho Hyun-ah', 5), ('Park Chang-jin', 4), ('Cho Yang-ho', 4), ('Yeo Woon-jin', 2), ('Kim Yon-se', 2), ('purser.(kys@heraldcorp.com', 1)]\n",
      "\tORG: [('Korean Air', 20), ('Hanjin Group', 7), ('the Transport Ministry', 3), ('the Transportation Ministry', 3), ('Transportation Ministry', 1), ('Transport Ministry', 1)]\n",
      "\tFAC: [('the Seoul Seobu District Court', 3), ('The Seoul Seobu District Court', 1)]\n",
      "Cluster id 1223\n",
      "\tNORP: [('German', 9), ('South Korean', 3), ('South Koreans', 2)]\n",
      "\tORG: [('Volkswagen', 11), ('Audi', 9), ('Volkswagen Group', 6), ('Audi Volkswagen Korea', 4), ('the U.S. Environmental Protection Agency', 3), ('the Seoul Central District Court', 3), ('TDI', 2), ('the Environment Ministry', 2), ('A4', 1), ('A6', 1)]\n",
      "\tGPE: [('U.S.', 7), ('South Korea', 5), ('the United States', 3), ('Seoul', 1), ('New Jersey', 1), ('Tennessee', 1), ('Audi Korea', 1)]\n",
      "\tPRODUCT: [('A5', 1)]\n",
      "\tPERSON: [('Yonhap', 3), ('Jason Ha', 2), ('Quinn Emanuel', 1)]\n",
      "Cluster id 1837\n",
      "\tGPE: [('Jeju', 6), (\"South Korea's\", 2), ('Jeju Island', 2)]\n",
      "\tORG: [('Coast Guard', 3), ('the Korea Coast Guard', 3), ('the Coast Guard', 1)]\n",
      "\tLOC: [('Chuja Island', 3), ('Jeju Island', 1)]\n",
      "\tPERSON: [('Yonhap', 3), ('Lee', 2), ('Chang', 1)]\n",
      "\tPRODUCT: [('Dolphin', 3)]\n",
      "Cluster id 2166\n",
      "\tGPE: [('Yangyang County', 5), ('Green Korea United', 3), ('Gangwon Province', 3), ('The Korea Herald', 2), ('Yangyang County’s', 1), ('Gyeonggi Province', 1), ('Seoul', 1), ('Yangyang', 1), ('Yangyang City', 1)]\n",
      "\tLOC: [('Mount Seoraksan', 11), ('Chuncheon City', 1), ('the mountain ridge', 1)]\n",
      "\tORG: [('the Environment Ministry', 3), ('Yangyang County Office', 2), ('national park committee', 1), ('the Board of Audit and Inspection', 1), ('the National Assembly', 1), ('The Environment Ministry’s', 1), ('Yangyang County Office of Gangwon Province', 1), ('Gwacheon Government Complex', 1), ('Yonhap)The', 1), ('the national park committee', 1)]\n",
      "\tPERSON: [('Yangyang', 5), ('Lee Hyun-jeong', 3), ('Park Geun-hye', 2), ('Park', 2), ('Daecheongbong', 2), ('Lee Jang-kyo', 1), ('Lee', 1), ('Kim Cheol-lae', 1), ('Youn Yeo-chang', 1), ('Choung Heung-lak', 1)]\n",
      "\tEVENT: [('the PyeongChang Winter Olympics', 3), ('PyeongChang Winter Olympics', 1)]\n",
      "Cluster id 2864\n",
      "\tNORP: [('South Korean', 6), ('inter-Korean', 6), ('South Koreans', 3), ('North Korean', 2)]\n",
      "\tGPE: [('North Korea', 8), ('Pyongyang', 7), ('Japan', 6), ('Koreas', 4), ('Seoul', 4), ('Korea', 3), ('Kaesong', 3), ('Panmunjom', 3), (\"South Korea's\", 2), ('South Korea', 2)]\n",
      "\tLOC: [('North', 18), ('South', 5), ('The Korean Peninsula', 3)]\n",
      "\tORG: [('The Unification Ministry', 3), ('Korean Central News Agency', 3), ('KCNA', 1)]\n",
      "\tEVENT: [('Liberation Day', 1)]\n",
      "\tPERSON: [('Yonhap', 3)]\n",
      "Cluster id 3613\n",
      "\tPERSON: [('Jeong Hunny', 3), ('Yeo Jun-suk', 2), ('Kim Hong-shin', 2), ('Choi Jin-kyu', 1), ('bosintang', 1), ('Gangnam-gu', 1), ('Bosintang', 1), ('Choi Young', 1), ('Choi', 1), ('Park So-youn', 1)]\n",
      "\tORG: [('the Humane Society International', 2), ('the Change for Animals Foundation', 2), ('the Livestock Products Sanitary Control Act', 2), ('MERS', 1), ('Dog Meat Farmers Association', 1), ('Dongui Bogam', 1), ('Coexistence of Animal Rights on Earth', 1), ('the Ministry of Agriculture, Food and Rural Affairs', 1)]\n",
      "\tLOC: [('Middle East', 1)]\n",
      "\tGPE: [('South Korea', 14), ('Seoul', 5), ('U.S.', 4), ('Korea', 3), ('South Korea’s', 1), ('The Korea', 1)]\n",
      "\tNORP: [('Koreans', 4), ('Korean', 2), ('South Korean', 2)]\n",
      "\tEVENT: [('Seoul Asian Games', 2), ('Seoul Summer Olympics', 2), ('the 1988 Seoul Olympics', 1)]\n"
     ]
    }
   ],
   "source": [
    "for (cluster_id, number_event) in top10_event_2015:\n",
    "    candidates = df2015[tfidf_doc2015_event_labels == cluster_id]['body']\n",
    "    docs = nlp.pipe(candidates)\n",
    "    ent_list = dict()\n",
    "    for doc in docs:\n",
    "        for ent in doc.ents:\n",
    "            if not ent.label_ in ['MONEY','TIME','CARDINAL','QUANTITY','DATE','ORDINAL','PERCENT']:\n",
    "                if ent.label_ in ent_list:\n",
    "                    ent_list[ent.label_].append(ent.text)\n",
    "                else:\n",
    "                    ent_list[ent.label_] = [ent.text]\n",
    "    print(\"Cluster id {}\".format(cluster_id))\n",
    "    for k in ent_list:\n",
    "        v = ent_list[k]\n",
    "        ent_counter = Counter(v)\n",
    "        top10item = ent_counter.most_common(10)\n",
    "        print(\"\\t{}: {}\".format(k, top10item))\n",
    "#     print(ent_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3b0954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
