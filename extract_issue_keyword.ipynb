{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74e29a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import string\n",
    "import random\n",
    "import unicodedata\n",
    "from string import punctuation\n",
    "from string import digits\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "from rake_nltk import Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3f1769e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23769, 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Reading data\n",
    "data_dir = 'data/'\n",
    "filename_prefix = 'koreaherald_1517_'\n",
    "df = []\n",
    "\n",
    "for i in range(8):\n",
    "    df.append(pd.read_json(os.path.join(data_dir, filename_prefix + str(i) + '.json')))\n",
    "df = pd.concat(df)\n",
    "df.reset_index(inplace=True)\n",
    "df = df.rename(columns=dict(zip(df.columns,[df.columns[i].strip() for i in range(len(df.columns))])))\n",
    "df.drop('index', inplace=True, axis=1)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77c006b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load large spacy model \n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Aggregate title and content\n",
    "title_weight = 1\n",
    "df['agg_title_body'] = title_weight*(df['title']+'. ') + df['body']\n",
    "\n",
    "### Embed document and clustering\n",
    "df2017 = df['2017' < df['time']]\n",
    "df2016 = df[('2016' < df['time']) & (df['time'] < '2017')]\n",
    "df2015 = df[('2015' < df['time']) & (df['time'] < '2016')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd0e4f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lemmatization tool\n",
    "stemmer = WordNetLemmatizer()\n",
    "### Change similar words to the same word\n",
    "UN_WORD = \"The United Nations\"\n",
    "US_WORD = \"The United States\"\n",
    "NK_WORD = \"North Korea\"\n",
    "SK_WORD = \"South Korea\"\n",
    "\n",
    "similar_words = {\n",
    "    # Change to \"The United States\"\n",
    "    \"U.S.\": US_WORD,\n",
    "    \"US\": US_WORD,\n",
    "    \"USA\": US_WORD,\n",
    "    \"United States\": US_WORD,\n",
    "    \"United States'\": US_WORD,\n",
    "    \"The United States'\": US_WORD,\n",
    "    \n",
    "    # Change to \"North Korea\"\n",
    "    \"NK\": NK_WORD,\n",
    "    \"NK's\": NK_WORD,\n",
    "    \"N. Korea\": NK_WORD,\n",
    "    \"N. Korea's\": NK_WORD,\n",
    "    \"North Korea's\": NK_WORD,\n",
    "    \n",
    "    # Change to \"South Korea\"\n",
    "    \"SK\": SK_WORD,\n",
    "    \"SK's\": SK_WORD,\n",
    "    \"S. Korea\": SK_WORD,\n",
    "    \"S. Korea's\": SK_WORD,\n",
    "    \"South Korea's\": SK_WORD,\n",
    "    \n",
    "    # Change to \"The United Nations\"\n",
    "    \"United Nations\": UN_WORD,\n",
    "    \"United Nations'\": UN_WORD,\n",
    "    \"The United Nations'\": UN_WORD,\n",
    "    \"UN\": UN_WORD,\n",
    "}\n",
    "\n",
    "### Transform function\n",
    "def text_cleaning(s: str):\n",
    "        \n",
    "    def replace_strange_char(s: str):\n",
    "        non_en_chars = {\n",
    "            \"’\": \"'\",\n",
    "            \"‘\": \"'\"\n",
    "        }\n",
    "\n",
    "        def remove_non_en_chars(txt):\n",
    "            # remove non english characters\n",
    "            txt = convert_latin_chars(txt)\n",
    "            for char in non_en_chars.keys():\n",
    "                txt = re.sub(char, non_en_chars[char], txt)\n",
    "            txt = re.sub(r'[^\\x00-\\x7F]+', ' ', txt)\n",
    "            return txt\n",
    "\n",
    "        def convert_latin_chars(txt):\n",
    "            # convert latin characters\n",
    "            return ''.join(char for char in unicodedata.normalize('NFKD', txt) if unicodedata.category(char) != 'Mn')\n",
    "\n",
    "        s = remove_non_en_chars(s)\n",
    "        s = convert_latin_chars(s)\n",
    "        return s\n",
    "    s = replace_strange_char(s)\n",
    "    for key,value in similar_words.items():\n",
    "        s = re.sub(key, value, s)\n",
    "    return s\n",
    "\n",
    "# Using spacy to preprocess\n",
    "def preprocess_spacy(s: str):\n",
    "    # Change similar terms to the same term\n",
    "    new_str = text_cleaning(s)\n",
    "    doc = nlp(s)\n",
    "    # Group tokens\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    token_groupup_pattern = [\n",
    "        [{\"LOWER\": \"the\"}, {\"LOWER\": \"united\"}, {\"LOWER\": \"nations\"}],\n",
    "        [{\"LOWER\": \"the\"}, {\"LOWER\": \"united\"}, {\"LOWER\": \"states\"}],\n",
    "        [{\"LOWER\": \"north\"}, {\"LOWER\": \"korea\"}],\n",
    "        [{\"LOWER\": \"south\"}, {\"LOWER\": \"korea\"}],\n",
    "    ]\n",
    "    matcher.add(\"TermGroup\",token_groupup_pattern)\n",
    "    matches = matcher(doc)\n",
    "    merge_doc = []\n",
    "    for nid, start, end in matches:\n",
    "        merge_doc.append((start,end))\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for i in range(len(merge_doc)-1,-1,-1):\n",
    "            retokenizer.merge(doc[merge_doc[i][0]:merge_doc[i][1]])\n",
    "        \n",
    "    # Remove all stopword, punctuation, number\n",
    "    tokens = [ token for token in doc if not token.is_stop and not token.is_punct and not token.like_num]\n",
    "    new_str = ' '.join([ token.lemma_.lower() for token in tokens ])\n",
    "    return new_str, tokens, doc\n",
    "\n",
    "def spacy_tokenizer(s: str):\n",
    "    # Change similar terms to the same term\n",
    "    new_str = text_cleaning(s)\n",
    "    doc = nlp(s)\n",
    "    # Group tokens\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    token_groupup_pattern = [\n",
    "        [{\"LOWER\": \"the\"}, {\"LOWER\": \"united\"}, {\"LOWER\": \"nations\"}],\n",
    "        [{\"LOWER\": \"the\"}, {\"LOWER\": \"united\"}, {\"LOWER\": \"states\"}],\n",
    "        [{\"LOWER\": \"north\"}, {\"LOWER\": \"korea\"}],\n",
    "        [{\"LOWER\": \"south\"}, {\"LOWER\": \"korea\"}],\n",
    "    ]\n",
    "    matcher.add(\"TermGroup\",token_groupup_pattern)\n",
    "    matches = matcher(doc)\n",
    "    merge_doc = []\n",
    "    for nid, start, end in matches:\n",
    "        merge_doc.append((start,end))\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for i in range(len(merge_doc)-1,-1,-1):\n",
    "            retokenizer.merge(doc[merge_doc[i][0]:merge_doc[i][1]])\n",
    "        \n",
    "    # Remove all stopword, punctuation, number\n",
    "    tokens = [ token.lemma_.lower() for token in doc \\\n",
    "              if not token.is_stop and not token.is_punct and not token.like_num and token.lemma_.strip()!= '']\n",
    "    return tokens\n",
    "\n",
    "### Preprocess function for grouping similar topic\n",
    "def preprocess_manual(s: str):\n",
    "    # Change similar words to the same word\n",
    "    new_str = transform_to_similar_sentence(s)\n",
    "    # Remove punctuation\n",
    "    new_str = ''.join(ch if ch not in set(punctuation) else \" \" for ch in new_str)\n",
    "    # Remove all single characters\n",
    "    new_str = re.sub(r'\\W', ' ', new_str)\n",
    "    new_str = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', new_str)\n",
    "    new_str = re.sub(r'\\^[a-zA-Z]\\s+', ' ', new_str) \n",
    "    # Substituting multiple spaces with single space\n",
    "    new_str = re.sub(r'\\s+', ' ', new_str, flags=re.I)\n",
    "    # Removing prefixed 'b' - when data is in bytes format\n",
    "    new_str = re.sub(r'^b\\s+', '', new_str)\n",
    "    # Removing all numbers\n",
    "    new_str = new_str.translate(str.maketrans('', '', digits))\n",
    "    # Converting to Lowercase\n",
    "    new_str = new_str.lower()\n",
    "    # Lemmatization and remove stopwords\n",
    "    new_str = new_str.split()\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    tokens = [stemmer.lemmatize(word) for word in new_str if word not in stopwords]\n",
    "    new_str = ' '.join(tokens)\n",
    "    \n",
    "    return new_str, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57633ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make TF-IDF matrix\n",
    "def tfidf_embed(documents, dimension=None):\n",
    "    # documents: list of str\n",
    "    # dim: integer\n",
    "    embeddings_dict = {}\n",
    "    tfidf_vectorizer = TfidfVectorizer(input='content', tokenizer=spacy_tokenizer)\n",
    "    tfidf_vector = tfidf_vectorizer.fit_transform(documents)\n",
    "    \n",
    "    # Dimensionality Reduction\n",
    "    if dimension is not None:\n",
    "        svd_doc = TruncatedSVD(n_components=dimension, n_iter=5, random_state=42)\n",
    "        tfidf_vector = svd_doc.fit_transform(tfidf_vector)\n",
    "    return tfidf_vector\n",
    "\n",
    "### Make GloVe matrix\n",
    "glove_file = \"../glove.42B.300d.txt\"\n",
    "def glove_word_vector():\n",
    "    embeddings_dict = {}\n",
    "    with open(glove_file, 'r') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "    return embeddings_dict\n",
    "\n",
    "# Average sum of word vectors\n",
    "def sentence_embed(sentence, word_vectors, dimension):\n",
    "    sum_vector = np.zeros(dimension)\n",
    "    for w in sentence.split():\n",
    "        if w in word_vectors:\n",
    "            sum_vector += word_vectors[w]\n",
    "    return sum_vector/len(sentence)\n",
    "\n",
    "# Make document vector\n",
    "def document_embed(documents, embedding_technique='tfidf', dimension=None):\n",
    "    if embedding_technique=='tfidf':\n",
    "        doc_vector = tfidf_embed(documents, dimension)\n",
    "    elif embedding_technique=='glove':\n",
    "        word_vector = glove_word_vector()\n",
    "        if dimension is None:\n",
    "            dimension = 300\n",
    "        doc_vector = [ sentence_embed(s, word_vector, dimension).tolist() for s in documents ]\n",
    "    elif embedding_technique=='spacy':\n",
    "        doc_vector = [doc.vector for doc in documents]\n",
    "    \n",
    "    return doc_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06d4dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clustering \n",
    "def document_clustering(doc_vectors, clustering_method='kmeans', evaluate=False):\n",
    "    if clustering_method=='kmeans':\n",
    "        # Hyperparameters\n",
    "        k_event = 10000\n",
    "        k_issue = 6000\n",
    "        \n",
    "        # Clustering event\n",
    "        kmeans_event = KMeans(n_clusters=k_event, random_state=69).fit(doc_vectors)\n",
    "        # Represent each event by average sum of related news\n",
    "        event_vectors = np.zeros((k_event, doc_vectors.shape[1]))\n",
    "        for i in range(k_event):\n",
    "            event_vectors[i] = sum(doc_vectors[kmeans_event.labels_ == i])\n",
    "        \n",
    "        # Clustering issue\n",
    "        kmeans_issue = KMeans(n_clusters=k_issue, random_state=69).fit(event_vectors)\n",
    "        # Represent each issue by average sum of related news\n",
    "        issue_vectors = np.zeros((k_issue, doc_vectors.shape[1]))\n",
    "        for i in range(k_issue):\n",
    "            issue_vectors[i] = sum(event_vectors[kmeans_issue.labels_ == i])\n",
    "\n",
    "        issue_labels = np.array([ kmeans_issue.labels_[kmeans_event.labels_[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        return k_issue, k_event, issue_labels, kmeans_event.labels_\n",
    "    \n",
    "    elif clustering_method=='DBSCAN':\n",
    "        \n",
    "        # Hyperparameters\n",
    "        doc_eps = 0.195\n",
    "        doc_neighbors = 1\n",
    "        event_eps = 0.50\n",
    "        event_neighbors = 1\n",
    "        \n",
    "        '''\n",
    "            Find best doc_eps and event_eps\n",
    "        '''\n",
    "        if evaluate:\n",
    "            # Find best eps to group same document\n",
    "            doc_eps_list = [ 0.10 + 0.001*i for i in range(1,301) ]\n",
    "            doc_score = []\n",
    "            doc_event = []\n",
    "            doc_best_score = 0\n",
    "            doc_best_eps = 0.0001\n",
    "            for doc_eps in doc_eps_list:\n",
    "                # Clustering event\n",
    "                db_event = DBSCAN(eps=doc_eps, min_samples=doc_neighbors).fit(doc_vectors)\n",
    "                # Number of clusters in labels, ignoring noise if present.\n",
    "                n_events_ = len(set(db_event.labels_)) - (1 if -1 in db_event.labels_ else 0)\n",
    "                if len(set(db_event.labels_)) >= 2 and len(set(db_event.labels_)) <= len(doc_vectors)-1:\n",
    "                    score_ = silhouette_score(doc_vectors, db_event.labels_)\n",
    "                else:\n",
    "                    score_ = -1\n",
    "                doc_event.append(n_events_)\n",
    "                doc_score.append(score_)\n",
    "                if score_ > doc_best_score:\n",
    "                    doc_best_score = score_\n",
    "                    doc_best_eps = doc_eps\n",
    "            print(\"Best Silhouete score is {} at eps: {} and number of events: {}\".format(doc_best_score, doc_eps, n_events_))\n",
    "            fig = plt.figure()\n",
    "            plt.plot(doc_eps_list, doc_score)\n",
    "            fig.suptitle('Doc eps and Silhouette score', fontsize=20)\n",
    "            plt.xlabel('eps', fontsize=18)\n",
    "            plt.ylabel('Silhouette score', fontsize=16)\n",
    "            plt.show()\n",
    "            \n",
    "            fig = plt.figure()\n",
    "            plt.plot(doc_eps_list, doc_event)\n",
    "            fig.suptitle('Doc eps and number of events', fontsize=20)\n",
    "            plt.xlabel('eps', fontsize=18)\n",
    "            plt.ylabel('number of events', fontsize=16)\n",
    "            plt.show()\n",
    "            \n",
    "            # Set doc_eps to the best value\n",
    "            doc_eps = doc_best_eps\n",
    "            # Find best eps to group same event\n",
    "            # Clustering event\n",
    "            db_event = DBSCAN(eps=doc_eps, min_samples=doc_neighbors).fit(doc_vectors)\n",
    "            n_events_ = len(set(db_event.labels_)) - (1 if -1 in db_event.labels_ else 0)\n",
    "            event_labels = np.array(list(map(lambda x: n_events_ if x==-1 else x, db_event.labels_)))\n",
    "            event_vectors = np.zeros((n_events_, doc_vectors.shape[1]))\n",
    "            for i in range(n_events_+1):\n",
    "                if np.sum(event_labels == i) != 0:\n",
    "                    event_vectors[i] = np.sum(doc_vectors[event_labels == i], axis=0)/np.sum(event_labels == i)\n",
    "            \n",
    "            \n",
    "#             # Clustering issue\n",
    "#             event_eps_list = [ 0.2 + 0.001*i for i in range(1,401) ]\n",
    "#             event_score = []\n",
    "#             event_issue = []\n",
    "#             event_best_score = 0\n",
    "#             event_best_eps = 0.001\n",
    "#             for event_eps in event_eps_list:\n",
    "#                 db_issue = DBSCAN(eps=event_eps, min_samples=event_neighbors).fit(event_vectors)\n",
    "#                 # Number of clusters in labels, ignoring noise if present.\n",
    "#                 n_issues_ = len(set(db_issue.labels_)) - (1 if -1 in db_issue.labels_ else 0)\n",
    "#                 if len(set(db_issue.labels_)) >= 2 and len(set(db_issue.labels_)) <= len(event_vectors)-1:\n",
    "#                     score_ = silhouette_score(event_vectors, db_issue.labels_)\n",
    "#                 else:\n",
    "#                     score_ = -1\n",
    "#                 event_issue.append(n_issues_)\n",
    "#                 event_score.append(score_)\n",
    "#                 if score_ > event_best_score:\n",
    "#                     event_best_score = score_\n",
    "#                     event_best_eps = event_eps\n",
    "#             print(\"Best Silhouete score is {} at eps: {} and number of issues: {}\".format(event_best_score, event_eps, n_issues_))\n",
    "#             fig = plt.figure()\n",
    "#             plt.plot(event_eps_list, event_score)\n",
    "#             fig.suptitle('Event eps and Silhouette score', fontsize=20)\n",
    "#             plt.xlabel('eps', fontsize=18)\n",
    "#             plt.ylabel('Silhouette score', fontsize=16)\n",
    "#             plt.show()\n",
    "            \n",
    "#             fig = plt.figure()\n",
    "#             plt.plot(event_eps_list, event_issue)\n",
    "#             fig.suptitle('Event eps and number of issues', fontsize=20)\n",
    "#             plt.xlabel('eps', fontsize=18)\n",
    "#             plt.ylabel('number of issues', fontsize=16)\n",
    "#             plt.show()\n",
    "            \n",
    "            # Set event_eps to best value\n",
    "            event_eps = 0.5\n",
    "            # Clustering issue\n",
    "            db_issue = DBSCAN(eps=event_eps, min_samples=event_neighbors).fit(event_vectors)\n",
    "            # Number of clusters in labels, ignoring noise if present.\n",
    "            n_issues_ = len(set(db_issue.labels_)) - (1 if -1 in db_issue.labels_ else 0)\n",
    "            n_noise_ = list(db_issue.labels_).count(-1)\n",
    "            # Represent each issue by average sum of related news\n",
    "            issue_labels = np.array(list(map(lambda x: n_issues_ if x==-1 else x, db_issue.labels_)))\n",
    "            issue_vectors = np.zeros((n_issues_, doc_vectors.shape[1]))\n",
    "            for i in range(n_issues_+1):\n",
    "                if np.sum(issue_labels == i) != 0:\n",
    "                    issue_vectors[i] = np.sum(event_vectors[issue_labels == i], axis=0)/np.sum(issue_labels == i)\n",
    "       \n",
    "            issue_labels = np.array([ issue_labels[event_labels[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        else:\n",
    "            '''\n",
    "            Clustering using specific value\n",
    "            '''\n",
    "            # Clustering event\n",
    "            db_event = DBSCAN(eps=doc_eps, min_samples=doc_neighbors).fit(doc_vectors)\n",
    "            # Number of clusters in labels, ignoring noise if present.\n",
    "            n_events_ = len(set(db_event.labels_)) - (1 if -1 in db_event.labels_ else 0)\n",
    "            n_noise_ = list(db_event.labels_).count(-1)\n",
    "            print(\"1st cluster:\\n\\tThe number of cluster is {}\".format(n_events_))\n",
    "            # Represent each event by average sum of related news\n",
    "            event_labels = np.array(list(map(lambda x: n_events_ if x==-1 else x, db_event.labels_)))\n",
    "            event_vectors = np.zeros((n_events_, doc_vectors.shape[1]))\n",
    "            for i in range(n_events_+1):\n",
    "                if np.sum(event_labels == i) != 0:\n",
    "                    event_vectors[i] = np.sum(doc_vectors[event_labels == i], axis=0)/np.sum(event_labels == i)\n",
    "\n",
    "            # Clustering issue\n",
    "            db_issue = DBSCAN(eps=event_eps, min_samples=event_neighbors).fit(event_vectors)\n",
    "            # Number of clusters in labels, ignoring noise if present.\n",
    "            n_issues_ = len(set(db_issue.labels_)) - (1 if -1 in db_issue.labels_ else 0)\n",
    "            n_noise_ = list(db_issue.labels_).count(-1)\n",
    "            print(\"2nd cluster:\\n\\tThe number of cluster is {}\".format(n_issues_))\n",
    "            # Represent each issue by average sum of related news\n",
    "            issue_labels = np.array(list(map(lambda x: n_issues_ if x==-1 else x, db_issue.labels_)))\n",
    "            issue_vectors = np.zeros((n_issues_, doc_vectors.shape[1]))\n",
    "            for i in range(n_issues_+1):\n",
    "                if np.sum(issue_labels == i) != 0:\n",
    "                    issue_vectors[i] = np.sum(event_vectors[issue_labels == i], axis=0)/np.sum(issue_labels == i)\n",
    "        \n",
    "            issue_labels = np.array([ issue_labels[event_labels[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        return n_issues_, n_events_, issue_labels, event_labels\n",
    "    \n",
    "    elif clustering_method=='agglomerative':\n",
    "        # Hyperparameters\n",
    "        n_events = 10000\n",
    "        n_issues = 6000\n",
    "        \n",
    "        # Clustering event\n",
    "        agg_event = AgglomerativeClustering(distance_threshold=0, n_clusters=n_events).fit(doc_vectors)\n",
    "        # Represent each event by average sum of related news\n",
    "        event_vectors = np.zeros((n_events, doc_vectors.shape[1]))\n",
    "        for i in range(n_events):\n",
    "            event_vectors[i] = sum(doc_vectors[agg_event.labels_ == i])\n",
    "        \n",
    "        plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "        # plot the top three levels of the dendrogram\n",
    "        plot_dendrogram(agg_event, truncate_mode=\"level\", p=3)\n",
    "        plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Clustering issue\n",
    "        agg_issue = AgglomerativeClustering(distance_threshold=0, n_clusters=n_issues).fit(event_vectors)\n",
    "        # Represent each issue by average sum of related news\n",
    "        issue_vectors = np.zeros((n_issues, doc_vectors.shape[1]))\n",
    "        for i in range(n_issues):\n",
    "            issue_vectors[i] = sum(event_vectors[agg_issue.labels_ == i])\n",
    "\n",
    "        issue_labels = np.array([ agg_issue.labels_[agg_event.labels_[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        return agg_issue, agg_event, issue_labels, agg_event.labels_\n",
    "    \n",
    "    elif clustering_method=='LDA':\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        assert(\"Doesn't support {}\".format(clustering_method))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "638b44c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "tfidf_doc2017_vectors = joblib.load('tfidf_titlebody_2017.csv')\n",
    "tfidf_doc2016_vectors = joblib.load('tfidf_titlebody_2016.csv')\n",
    "tfidf_doc2015_vectors = joblib.load('tfidf_titlebody_2015.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31e387b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st cluster:\n",
      "\tThe number of cluster is 6914\n",
      "2nd cluster:\n",
      "\tThe number of cluster is 1159\n"
     ]
    }
   ],
   "source": [
    "tfidf_doc2015_num_issue, tfidf_doc2015_num_event, tfidf_doc2015_issue_labels, tfidf_doc2015_event_labels = document_clustering(tfidf_doc2015_vectors, clustering_method='DBSCAN', evaluate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "220c7349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st cluster:\n",
      "\tThe number of cluster is 7335\n",
      "2nd cluster:\n",
      "\tThe number of cluster is 1244\n"
     ]
    }
   ],
   "source": [
    "tfidf_doc2016_num_issue, tfidf_doc2016_num_event, tfidf_doc2016_issue_labels, tfidf_doc2016_event_labels = document_clustering(tfidf_doc2016_vectors, clustering_method='DBSCAN', evaluate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "777567a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st cluster:\n",
      "\tThe number of cluster is 8997\n",
      "2nd cluster:\n",
      "\tThe number of cluster is 1517\n"
     ]
    }
   ],
   "source": [
    "tfidf_doc2017_num_issue, tfidf_doc2017_num_event, tfidf_doc2017_issue_labels, tfidf_doc2017_event_labels = document_clustering(tfidf_doc2017_vectors, clustering_method='DBSCAN', evaluate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "511b6fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of document in 2017: 9128 / 8997 clusters\n",
      "Number of document in 2016: 7485 / 7335 clusters\n",
      "Number of document in 2015: 7156 / 6914 clusters\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of document in 2017: {} / {} clusters\".format(len(tfidf_doc2017_vectors), tfidf_doc2017_num_event))\n",
    "print(\"Number of document in 2016: {} / {} clusters\".format(len(tfidf_doc2016_vectors), tfidf_doc2016_num_event))\n",
    "print(\"Number of document in 2015: {} / {} clusters\".format(len(tfidf_doc2015_vectors), tfidf_doc2015_num_event))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5c276e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counter_2017_event = Counter(tfidf_doc2017_event_labels)\n",
    "counter_2016_event = Counter(tfidf_doc2016_event_labels)\n",
    "counter_2015_event = Counter(tfidf_doc2015_event_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "323f5523",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_event_2017 = counter_2017_event.most_common(10)\n",
    "top10_event_2016 = counter_2016_event.most_common(10)\n",
    "top10_event_2015 = counter_2015_event.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "843acf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "language = \"en\"\n",
    "max_ngram_size = 4\n",
    "deduplication_threshold = 0.9\n",
    "numOfKeywords = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32a66daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017: event 67 with total 11 events\n",
      "('range c c return', -0.03593283821625691)\n",
      "('range c c. yonhap', -0.02362183056301725)\n",
      "('minus c c. freezing', -0.012917611249861896)\n",
      "('minus c c low', -0.009707319799395639)\n",
      "('minus c c cold', -0.00694511646407574)\n",
      "2017: event 617 with total 7 events\n",
      "('brown long eared bat', 1.3236941614358072e-06)\n",
      "('skin impression sauropod dinosaur', 4.174526125532569e-06)\n",
      "('impression sauropod dinosaur footprint', 4.780558191818081e-06)\n",
      "('large skin impression sauropod', 5.168793060026513e-06)\n",
      "('long eared bat find', 6.516051520695608e-06)\n",
      "2017: event 3706 with total 5 events\n",
      "('percent year early report', 1.3119695587340292e-06)\n",
      "('year early report show', 1.3929730818055183e-06)\n",
      "('period percent year early', 1.6200224264501453e-06)\n",
      "('percent tally year early', 1.655658330256932e-06)\n",
      "('percent year early number', 1.7888291186924396e-06)\n",
      "2017: event 4733 with total 5 events\n",
      "('group south korean scientist', 1.615205657664319e-06)\n",
      "('south korean scientist develop', 2.357558060341668e-06)\n",
      "('south korean scientist discover', 9.268020844522943e-06)\n",
      "('ministry science ict future', 1.0335422901173877e-05)\n",
      "('science ict future planning', 1.054013281423317e-05)\n",
      "2017: event 5 with total 4 events\n",
      "('region currently way ministry', -0.03814820353800141)\n",
      "('highly pathogenic avian influenza', 1.5779978216132101e-06)\n",
      "('highly pathogenic bird flu', 1.780601220408725e-06)\n",
      "('south korea slaughter bird', 3.324322274689586e-06)\n",
      "('bad outbreak bird flu', 3.5271214245266998e-06)\n",
      "2017: event 6652 with total 4 events\n",
      "('south korean cargo ship', 1.4219188002384788e-06)\n",
      "('crew member south korean', 1.7345799696778696e-06)\n",
      "('stellar daisy south korean', 2.3908686328851376e-06)\n",
      "('search miss south korean', 2.4763466852109618e-06)\n",
      "('text south korean employer', 2.7122714734972575e-06)\n",
      "2017: event 813 with total 3 events\n",
      "('money later take win', -0.05478784706826415)\n",
      "('south korean mixed martial', 2.9353644631452435e-06)\n",
      "('korean mixed martial art', 3.2484877010167218e-06)\n",
      "('official warn fight fix', 3.554188223234895e-06)\n",
      "('kuntz hour fight ufc', 3.6642719746326815e-06)\n",
      "2017: event 911 with total 3 events\n",
      "('minister nominee hong jong', 8.17470549493636e-06)\n",
      "('family property inheritance past', 8.853782106079408e-06)\n",
      "('property inheritance past controversial', 9.324379270185949e-06)\n",
      "('issue family property inheritance', 9.450097604997885e-06)\n",
      "('complicate confirmation process past', 9.689437859516677e-06)\n",
      "2017: event 4110 with total 3 events\n",
      "('group say try find', -0.014753096337872895)\n",
      "('find less know art', -0.008315010285641891)\n",
      "('art seoul street find', 3.8166954971942866e-07)\n",
      "('city public art seoul', 7.22704347280942e-07)\n",
      "('seoul street find citizens', 9.929126830906717e-07)\n",
      "2017: event 5033 with total 3 events\n",
      "('korea confirm zika virus', 4.8602946109659365e-06)\n",
      "('confirm zika virus infection', 5.292223697008275e-06)\n",
      "('korea confirm case zika', 5.848489475039751e-06)\n",
      "('infection south korea confirm', 6.196780245758198e-06)\n",
      "('confirm case zika virus', 6.352873497044848e-06)\n"
     ]
    }
   ],
   "source": [
    "for index, (event_id, num_event) in enumerate(top10_event_2017):\n",
    "    text_list = df2017[tfidf_doc2017_event_labels == event_id]['agg_title_body'].tolist()\n",
    "    tokens = spacy_tokenizer(' '.join(text_list))\n",
    "    text = ' '.join(tokens)\n",
    "    print(\"2017: event {} with total {} events\".format(event_id, num_event))\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "    keywords = custom_kw_extractor.extract_keywords(text)\n",
    "    for kw in keywords:\n",
    "        print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb982f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016: event 16 with total 5 events\n",
      "('spy operate south korea', 1.0078800368346317e-06)\n",
      "('operate south korea cold', 1.2060656916029104e-06)\n",
      "('agent operate south korea', 1.3864058926159937e-06)\n",
      "('resume broadcast mysterious number', 1.3920171549790387e-06)\n",
      "('north korea state radio', 1.4047214852049752e-06)\n",
      "2016: event 5095 with total 4 events\n",
      "('turnout usually mean young', -0.04301549670150385)\n",
      "('election voter turnout record', 3.549755647525598e-06)\n",
      "('voter turnout general election', 3.6603067814212916e-06)\n",
      "('general election voter turnout', 3.660306781421292e-06)\n",
      "('turnout general election estimate', 4.60399560785653e-06)\n",
      "2016: event 431 with total 3 events\n",
      "('seong ju tell korea', -0.11565845212098688)\n",
      "('step right say park', -0.003313462655309559)\n",
      "('president park geun hye', 1.4723006036558038e-07)\n",
      "('demand president park geun', 4.32866979914615e-07)\n",
      "('park geun hye step', 5.71538235890432e-07)\n",
      "2016: event 518 with total 3 events\n",
      "('authories say far total', -0.12066525464540351)\n",
      "('report possible ai outbreak', -0.06069393390087305)\n",
      "('haenam south jeolla province', 1.1455343566366718e-05)\n",
      "('farm haenam south jeolla', 1.2302571727953134e-05)\n",
      "('highly pathogenic aviation influenza', 1.2657175215974146e-05)\n",
      "2016: event 706 with total 3 events\n",
      "('deoksugung stone wall walkway', 1.4107032431310406e-05)\n",
      "('opening deoksugung stone wall', 2.3881608890460063e-05)\n",
      "('stone wall surround deoksugung', 2.3973289890493747e-05)\n",
      "('deoksugung picturesque stone wall', 2.576966961405947e-05)\n",
      "('meter deoksugung stone wall', 2.5912541873542512e-05)\n",
      "2016: event 5782 with total 3 events\n",
      "('affect cloven hoofed animal', 4.722880180686282e-06)\n",
      "('disease affect cloven hoofed', 5.077500719930345e-06)\n",
      "('cloven hoofed animal accord', 5.2143841884651865e-06)\n",
      "('cloven hoofed animal cattle', 5.2143841884651865e-06)\n",
      "('meat cloven hoofed animal', 5.214384188465187e-06)\n",
      "2016: event 6234 with total 3 events\n",
      "('stance come the united', -0.08001705161858542)\n",
      "('denuclearization say new north', -0.0055472528433797895)\n",
      "('hold peace treaty talk', 6.889597124585786e-07)\n",
      "('proposal hold peace treaty', 1.2501778664396597e-06)\n",
      "('peace treaty talk north', 1.6156359264909074e-06)\n",
      "2016: event 7166 with total 3 events\n",
      "('educate accordingly say suh', -0.2298064137557486)\n",
      "('center gifted talented education', 1.3174848874784114e-05)\n",
      "('gifted class gifted education', 1.3647751830230798e-05)\n",
      "('gifted education center gifted', 1.632772339220463e-05)\n",
      "('education center gifted school', 2.3384948654519077e-05)\n",
      "2016: event 11 with total 2 events\n",
      "('democratic party people party', 0.00038608505904695915)\n",
      "('party people party', 0.0013565109134051308)\n",
      "('party friday agree hold', 0.0013778294294542772)\n",
      "('political party friday agree', 0.0013845372163570888)\n",
      "('floor leader democratic party', 0.0015110870164356945)\n",
      "2016: event 90 with total 2 events\n",
      "('accord independent counsel team', 8.379752581345896e-06)\n",
      "('independent counsel team local', 8.379752581345896e-06)\n",
      "('company independent counsel scandal', 8.816932051624894e-06)\n",
      "('money accord independent counsel', 9.026704881777385e-06)\n",
      "('scandal involve president park', 9.861611219108147e-06)\n"
     ]
    }
   ],
   "source": [
    "for index, (event_id, num_event) in enumerate(top10_event_2016):\n",
    "    text_list = df2016[tfidf_doc2016_event_labels == event_id]['agg_title_body'].tolist()\n",
    "    tokens = spacy_tokenizer(' '.join(text_list))\n",
    "    text = ' '.join(tokens)\n",
    "    print(\"2016: event {} with total {} events\".format(event_id, num_event))\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "    keywords = custom_kw_extractor.extract_keywords(text)\n",
    "    for kw in keywords:\n",
    "        print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c298b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015: event 2954 with total 6 events\n",
      "('country see new case', -0.010502048125275545)\n",
      "('rate percent south korea', 1.6372636795725765e-06)\n",
      "('rate stand percent people', 1.6804852510846144e-06)\n",
      "('disease diagnose mers discharge', 1.8711487209193375e-06)\n",
      "('day disease diagnose mers', 1.8786672956205054e-06)\n",
      "2015: event 132 with total 5 events\n",
      "('percent year follow percent', 6.171446027792915e-07)\n",
      "('bear percent month year', 9.081089942612817e-07)\n",
      "('drop percent year follow', 9.515318853764606e-07)\n",
      "('percent year drop follow', 9.515318853764608e-07)\n",
      "('year drop follow percent', 9.515318853764608e-07)\n",
      "2015: event 5034 with total 4 events\n",
      "('positive way say financial', -0.36664357961127664)\n",
      "('fsc say new loan', -0.2233996740199515)\n",
      "('commission say new loan', -0.21978961760021803)\n",
      "('long term fix rate', 3.804430652794157e-05)\n",
      "('term fix rate loan', 4.513793482703777e-05)\n",
      "2015: event 5452 with total 4 events\n",
      "('apology sex slavery issue', 1.7297192690602546e-06)\n",
      "('talk sex slavery major', 1.7427420894863642e-06)\n",
      "('launch talk sex slavery', 1.8737907699417378e-06)\n",
      "('apologize sex slavery issue', 1.89231107151814e-06)\n",
      "('sex slavery issue historian', 1.9721127335140906e-06)\n",
      "2015: event 6013 with total 4 events\n",
      "('flight attendant cabin crew', 7.692251001425633e-07)\n",
      "('female flight attendant cabin', 9.610998667234792e-07)\n",
      "('attendant cabin crew chief', 1.0144600022497302e-06)\n",
      "('korean air year flight', 1.021421494824462e-06)\n",
      "('air year flight attendant', 1.0862355846609576e-06)\n",
      "2015: event 1223 with total 3 events\n",
      "('jason ha say lawsuit', -0.04590123575190168)\n",
      "('volkswagen korea local dealer', 3.6326294202797175e-06)\n",
      "('group audi volkswagen korea', 3.995202966432145e-06)\n",
      "('audi volkswagen korea local', 4.217567346378786e-06)\n",
      "('volkswagen group audi volkswagen', 4.4769125465504815e-06)\n",
      "2015: event 1837 with total 3 events\n",
      "('toll coast guard official', 1.015243877953982e-05)\n",
      "('death toll coast guard', 1.0152438779539822e-05)\n",
      "('dolphin capsize september water', 1.0910230764731768e-05)\n",
      "('accident raise death toll', 1.1432848952118062e-05)\n",
      "('raise death toll coast', 1.1476126047758082e-05)\n",
      "2015: event 2166 with total 3 events\n",
      "('win nearly new job', -0.14157326804740802)\n",
      "('location say far natural', -0.027129677849984875)\n",
      "('county say despite rejection', -0.022851854216963247)\n",
      "('route say new location', -0.020661252932518564)\n",
      "('cable car mount seoraksan', 5.670471075726441e-07)\n",
      "2015: event 2864 with total 3 events\n",
      "('official say come north', -0.04037490682563553)\n",
      "('japan colonial rule north', 2.46609004075169e-06)\n",
      "('north preparatory group joint', 4.211215088982701e-06)\n",
      "('joint anniversary event liberation', 4.21131990747422e-06)\n",
      "('preparation joint event anniversary', 4.22004676630386e-06)\n",
      "2015: event 3613 with total 3 events\n",
      "('dog eat south korea', 1.017361171800775e-06)\n",
      "('south korea dog meat', 1.2940320449279415e-06)\n",
      "('free dog dog meat', 1.9932695245982467e-06)\n",
      "('meat farm south korea', 2.090225286764037e-06)\n",
      "('dog meat farm south', 2.101127013059688e-06)\n"
     ]
    }
   ],
   "source": [
    "for index, (event_id, num_event) in enumerate(top10_event_2015):\n",
    "    text_list = df2015[tfidf_doc2015_event_labels == event_id]['agg_title_body'].tolist()\n",
    "    tokens = spacy_tokenizer(' '.join(text_list))\n",
    "    text = ' '.join(tokens)\n",
    "    print(\"2015: event {} with total {} events\".format(event_id, num_event))\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "    keywords = custom_kw_extractor.extract_keywords(text)\n",
    "    for kw in keywords:\n",
    "        print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05ab69f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
