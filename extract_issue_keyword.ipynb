{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74e29a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import string\n",
    "import random\n",
    "import unicodedata\n",
    "from string import punctuation\n",
    "from string import digits\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "from rake_nltk import Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3f1769e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23769, 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Reading data\n",
    "data_dir = 'data/'\n",
    "filename_prefix = 'koreaherald_1517_'\n",
    "df = []\n",
    "\n",
    "for i in range(8):\n",
    "    df.append(pd.read_json(os.path.join(data_dir, filename_prefix + str(i) + '.json')))\n",
    "df = pd.concat(df)\n",
    "df.reset_index(inplace=True)\n",
    "df = df.rename(columns=dict(zip(df.columns,[df.columns[i].strip() for i in range(len(df.columns))])))\n",
    "df.drop('index', inplace=True, axis=1)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77c006b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load large spacy model \n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Aggregate title and content\n",
    "title_weight = 1\n",
    "df['agg_title_body'] = title_weight*(df['title']+'. ') + df['body']\n",
    "\n",
    "### Embed document and clustering\n",
    "df2017 = df['2017' < df['time']]\n",
    "df2016 = df[('2016' < df['time']) & (df['time'] < '2017')]\n",
    "df2015 = df[('2015' < df['time']) & (df['time'] < '2016')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd0e4f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lemmatization tool\n",
    "stemmer = WordNetLemmatizer()\n",
    "### Change similar words to the same word\n",
    "UN_WORD = \"The United Nations\"\n",
    "US_WORD = \"The United States\"\n",
    "NK_WORD = \"North Korea\"\n",
    "SK_WORD = \"South Korea\"\n",
    "\n",
    "similar_words = {\n",
    "    # Change to \"The United States\"\n",
    "    \"U.S.\": US_WORD,\n",
    "    \"US\": US_WORD,\n",
    "    \"USA\": US_WORD,\n",
    "    \"United States\": US_WORD,\n",
    "    \"United States'\": US_WORD,\n",
    "    \"The United States'\": US_WORD,\n",
    "    \n",
    "    # Change to \"North Korea\"\n",
    "    \"NK\": NK_WORD,\n",
    "    \"NK's\": NK_WORD,\n",
    "    \"N. Korea\": NK_WORD,\n",
    "    \"N. Korea's\": NK_WORD,\n",
    "    \"North Korea's\": NK_WORD,\n",
    "    \n",
    "    # Change to \"South Korea\"\n",
    "    \"SK\": SK_WORD,\n",
    "    \"SK's\": SK_WORD,\n",
    "    \"S. Korea\": SK_WORD,\n",
    "    \"S. Korea's\": SK_WORD,\n",
    "    \"South Korea's\": SK_WORD,\n",
    "    \n",
    "    # Change to \"The United Nations\"\n",
    "    \"United Nations\": UN_WORD,\n",
    "    \"United Nations'\": UN_WORD,\n",
    "    \"The United Nations'\": UN_WORD,\n",
    "    \"UN\": UN_WORD,\n",
    "}\n",
    "\n",
    "### Transform function\n",
    "def text_cleaning(s: str):\n",
    "        \n",
    "    def replace_strange_char(s: str):\n",
    "        non_en_chars = {\n",
    "            \"’\": \"'\",\n",
    "            \"‘\": \"'\"\n",
    "        }\n",
    "\n",
    "        def remove_non_en_chars(txt):\n",
    "            # remove non english characters\n",
    "            txt = convert_latin_chars(txt)\n",
    "            for char in non_en_chars.keys():\n",
    "                txt = re.sub(char, non_en_chars[char], txt)\n",
    "            txt = re.sub(r'[^\\x00-\\x7F]+', ' ', txt)\n",
    "            return txt\n",
    "\n",
    "        def convert_latin_chars(txt):\n",
    "            # convert latin characters\n",
    "            return ''.join(char for char in unicodedata.normalize('NFKD', txt) if unicodedata.category(char) != 'Mn')\n",
    "\n",
    "        s = remove_non_en_chars(s)\n",
    "        s = convert_latin_chars(s)\n",
    "        return s\n",
    "    s = replace_strange_char(s)\n",
    "    for key,value in similar_words.items():\n",
    "        s = re.sub(key, value, s)\n",
    "    return s\n",
    "\n",
    "# Using spacy to preprocess\n",
    "def preprocess_spacy(s: str):\n",
    "    # Change similar terms to the same term\n",
    "    new_str = text_cleaning(s)\n",
    "    doc = nlp(s)\n",
    "    # Group tokens\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    token_groupup_pattern = [\n",
    "        [{\"LOWER\": \"the\"}, {\"LOWER\": \"united\"}, {\"LOWER\": \"nations\"}],\n",
    "        [{\"LOWER\": \"the\"}, {\"LOWER\": \"united\"}, {\"LOWER\": \"states\"}],\n",
    "        [{\"LOWER\": \"north\"}, {\"LOWER\": \"korea\"}],\n",
    "        [{\"LOWER\": \"south\"}, {\"LOWER\": \"korea\"}],\n",
    "    ]\n",
    "    matcher.add(\"TermGroup\",token_groupup_pattern)\n",
    "    matches = matcher(doc)\n",
    "    merge_doc = []\n",
    "    for nid, start, end in matches:\n",
    "        merge_doc.append((start,end))\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for i in range(len(merge_doc)-1,-1,-1):\n",
    "            retokenizer.merge(doc[merge_doc[i][0]:merge_doc[i][1]])\n",
    "        \n",
    "    # Remove all stopword, punctuation, number\n",
    "    tokens = [ token for token in doc if not token.is_stop and not token.is_punct and not token.like_num]\n",
    "    new_str = ' '.join([ token.lemma_.lower() for token in tokens ])\n",
    "    return new_str, tokens, doc\n",
    "\n",
    "def spacy_tokenizer(s: str):\n",
    "    # Change similar terms to the same term\n",
    "    new_str = text_cleaning(s)\n",
    "    doc = nlp(s)\n",
    "    # Group tokens\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    token_groupup_pattern = [\n",
    "        [{\"LOWER\": \"the\"}, {\"LOWER\": \"united\"}, {\"LOWER\": \"nations\"}],\n",
    "        [{\"LOWER\": \"the\"}, {\"LOWER\": \"united\"}, {\"LOWER\": \"states\"}],\n",
    "        [{\"LOWER\": \"north\"}, {\"LOWER\": \"korea\"}],\n",
    "        [{\"LOWER\": \"south\"}, {\"LOWER\": \"korea\"}],\n",
    "    ]\n",
    "    matcher.add(\"TermGroup\",token_groupup_pattern)\n",
    "    matches = matcher(doc)\n",
    "    merge_doc = []\n",
    "    for nid, start, end in matches:\n",
    "        merge_doc.append((start,end))\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for i in range(len(merge_doc)-1,-1,-1):\n",
    "            retokenizer.merge(doc[merge_doc[i][0]:merge_doc[i][1]])\n",
    "        \n",
    "    # Remove all stopword, punctuation, number\n",
    "    tokens = [ token.lemma_.lower() for token in doc \\\n",
    "              if not token.is_stop and not token.is_punct and not token.like_num and token.lemma_.strip()!= '']\n",
    "    return tokens\n",
    "\n",
    "### Preprocess function for grouping similar topic\n",
    "def preprocess_manual(s: str):\n",
    "    # Change similar words to the same word\n",
    "    new_str = transform_to_similar_sentence(s)\n",
    "    # Remove punctuation\n",
    "    new_str = ''.join(ch if ch not in set(punctuation) else \" \" for ch in new_str)\n",
    "    # Remove all single characters\n",
    "    new_str = re.sub(r'\\W', ' ', new_str)\n",
    "    new_str = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', new_str)\n",
    "    new_str = re.sub(r'\\^[a-zA-Z]\\s+', ' ', new_str) \n",
    "    # Substituting multiple spaces with single space\n",
    "    new_str = re.sub(r'\\s+', ' ', new_str, flags=re.I)\n",
    "    # Removing prefixed 'b' - when data is in bytes format\n",
    "    new_str = re.sub(r'^b\\s+', '', new_str)\n",
    "    # Removing all numbers\n",
    "    new_str = new_str.translate(str.maketrans('', '', digits))\n",
    "    # Converting to Lowercase\n",
    "    new_str = new_str.lower()\n",
    "    # Lemmatization and remove stopwords\n",
    "    new_str = new_str.split()\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    tokens = [stemmer.lemmatize(word) for word in new_str if word not in stopwords]\n",
    "    new_str = ' '.join(tokens)\n",
    "    \n",
    "    return new_str, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57633ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make TF-IDF matrix\n",
    "def tfidf_embed(documents, dimension=None):\n",
    "    # documents: list of str\n",
    "    # dim: integer\n",
    "    embeddings_dict = {}\n",
    "    tfidf_vectorizer = TfidfVectorizer(input='content', tokenizer=spacy_tokenizer)\n",
    "    tfidf_vector = tfidf_vectorizer.fit_transform(documents)\n",
    "    \n",
    "    # Dimensionality Reduction\n",
    "    if dimension is not None:\n",
    "        svd_doc = TruncatedSVD(n_components=dimension, n_iter=5, random_state=42)\n",
    "        tfidf_vector = svd_doc.fit_transform(tfidf_vector)\n",
    "    return tfidf_vector\n",
    "\n",
    "### Make GloVe matrix\n",
    "glove_file = \"../glove.42B.300d.txt\"\n",
    "def glove_word_vector():\n",
    "    embeddings_dict = {}\n",
    "    with open(glove_file, 'r') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "    return embeddings_dict\n",
    "\n",
    "# Average sum of word vectors\n",
    "def sentence_embed(sentence, word_vectors, dimension):\n",
    "    sum_vector = np.zeros(dimension)\n",
    "    for w in sentence.split():\n",
    "        if w in word_vectors:\n",
    "            sum_vector += word_vectors[w]\n",
    "    return sum_vector/len(sentence)\n",
    "\n",
    "# Make document vector\n",
    "def document_embed(documents, embedding_technique='tfidf', dimension=None):\n",
    "    if embedding_technique=='tfidf':\n",
    "        doc_vector = tfidf_embed(documents, dimension)\n",
    "    elif embedding_technique=='glove':\n",
    "        word_vector = glove_word_vector()\n",
    "        if dimension is None:\n",
    "            dimension = 300\n",
    "        doc_vector = [ sentence_embed(s, word_vector, dimension).tolist() for s in documents ]\n",
    "    elif embedding_technique=='spacy':\n",
    "        doc_vector = [doc.vector for doc in documents]\n",
    "    \n",
    "    return doc_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "256dcc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Days difference between two datetime\n",
    "def days_between(d1, d2):\n",
    "    d1 = datetime.strptime(d1, \"%Y-%m-%d %H:%M:%S\")\n",
    "    d2 = datetime.strptime(d2, \"%Y-%m-%d %H:%M:%S\")\n",
    "    return abs((d2 - d1).days)\n",
    "\n",
    "# Function returns number of article, number of distinct authors, section of the issue, length\n",
    "def issue_indicator(news_index):\n",
    "    num_article = len(news_index)\n",
    "    num_author = len(df['author'][news_index].unique())\n",
    "    section = 0\n",
    "    length = days_between(df['time'][news_index].max(),df['time'][news_index].min())\n",
    "    return num_article, num_author, section, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d53112e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06d4dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clustering \n",
    "def document_clustering(doc_vectors, clustering_method='kmeans', evaluate=False):\n",
    "    if clustering_method=='kmeans':\n",
    "        # Hyperparameters\n",
    "        k_event = 10000\n",
    "        k_issue = 6000\n",
    "        \n",
    "        # Clustering event\n",
    "        kmeans_event = KMeans(n_clusters=k_event, random_state=69).fit(doc_vectors)\n",
    "        # Represent each event by average sum of related news\n",
    "        event_vectors = np.zeros((k_event, doc_vectors.shape[1]))\n",
    "        for i in range(k_event):\n",
    "            event_vectors[i] = sum(doc_vectors[kmeans_event.labels_ == i])\n",
    "        \n",
    "        # Clustering issue\n",
    "        kmeans_issue = KMeans(n_clusters=k_issue, random_state=69).fit(event_vectors)\n",
    "        # Represent each issue by average sum of related news\n",
    "        issue_vectors = np.zeros((k_issue, doc_vectors.shape[1]))\n",
    "        for i in range(k_issue):\n",
    "            issue_vectors[i] = sum(event_vectors[kmeans_issue.labels_ == i])\n",
    "\n",
    "        issue_labels = np.array([ kmeans_issue.labels_[kmeans_event.labels_[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        return k_issue, k_event, issue_labels, kmeans_event.labels_\n",
    "    \n",
    "    elif clustering_method=='DBSCAN':\n",
    "        \n",
    "        # Hyperparameters\n",
    "        doc_eps = 0.195\n",
    "        doc_neighbors = 1\n",
    "        event_eps = 0.40\n",
    "        event_neighbors = 1\n",
    "        \n",
    "        '''\n",
    "            Find best doc_eps and event_eps\n",
    "        '''\n",
    "        if evaluate:\n",
    "            # Find best eps to group same document\n",
    "            doc_eps_list = [ 0.15 + 0.001*i for i in range(1,101) ]\n",
    "            doc_score = []\n",
    "            doc_event = []\n",
    "            doc_best_score = 0\n",
    "            doc_best_eps = 0.0001\n",
    "            for doc_eps in doc_eps_list:\n",
    "                # Clustering event\n",
    "                db_event = DBSCAN(eps=doc_eps, min_samples=doc_neighbors).fit(doc_vectors)\n",
    "                # Number of clusters in labels, ignoring noise if present.\n",
    "                n_events_ = len(set(db_event.labels_)) - (1 if -1 in db_event.labels_ else 0)\n",
    "                if len(set(db_event.labels_)) >= 2 and len(set(db_event.labels_)) <= len(doc_vectors)-1:\n",
    "                    score_ = silhouette_score(doc_vectors, db_event.labels_)\n",
    "                else:\n",
    "                    score_ = -1\n",
    "                doc_event.append(n_events_)\n",
    "                doc_score.append(score_)\n",
    "                if score_ > doc_best_score:\n",
    "                    doc_best_score = score_\n",
    "                    doc_best_eps = doc_eps\n",
    "            print(\"Best Silhouete score is {} at eps: {} and number of events: {}\".format(doc_best_score, doc_eps, n_events_))\n",
    "            fig = plt.figure()\n",
    "            plt.plot(doc_eps_list, doc_score)\n",
    "            fig.suptitle('Doc eps and Silhouette score', fontsize=20)\n",
    "            plt.xlabel('eps', fontsize=18)\n",
    "            plt.ylabel('Silhouette score', fontsize=16)\n",
    "            plt.show()\n",
    "            \n",
    "            fig = plt.figure()\n",
    "            plt.plot(doc_eps_list, doc_event)\n",
    "            fig.suptitle('Doc eps and number of events', fontsize=20)\n",
    "            plt.xlabel('eps', fontsize=18)\n",
    "            plt.ylabel('number of events', fontsize=16)\n",
    "            plt.show()\n",
    "            \n",
    "            # Set doc_eps to the best value\n",
    "            doc_eps = doc_best_eps\n",
    "            # Find best eps to group same event\n",
    "            # Clustering event\n",
    "            db_event = DBSCAN(eps=doc_eps, min_samples=doc_neighbors).fit(doc_vectors)\n",
    "            n_events_ = len(set(db_event.labels_)) - (1 if -1 in db_event.labels_ else 0)\n",
    "            event_labels = np.array(list(map(lambda x: n_events_ if x==-1 else x, db_event.labels_)))\n",
    "            event_vectors = np.zeros((n_events_, doc_vectors.shape[1]))\n",
    "            for i in range(n_events_+1):\n",
    "                if np.sum(event_labels == i) != 0:\n",
    "                    event_vectors[i] = np.sum(doc_vectors[event_labels == i], axis=0)/np.sum(event_labels == i)\n",
    "            \n",
    "            \n",
    "#             # Clustering issue\n",
    "#             event_eps_list = [ 0.2 + 0.001*i for i in range(1,401) ]\n",
    "#             event_score = []\n",
    "#             event_issue = []\n",
    "#             event_best_score = 0\n",
    "#             event_best_eps = 0.001\n",
    "#             for event_eps in event_eps_list:\n",
    "#                 db_issue = DBSCAN(eps=event_eps, min_samples=event_neighbors).fit(event_vectors)\n",
    "#                 # Number of clusters in labels, ignoring noise if present.\n",
    "#                 n_issues_ = len(set(db_issue.labels_)) - (1 if -1 in db_issue.labels_ else 0)\n",
    "#                 if len(set(db_issue.labels_)) >= 2 and len(set(db_issue.labels_)) <= len(event_vectors)-1:\n",
    "#                     score_ = silhouette_score(event_vectors, db_issue.labels_)\n",
    "#                 else:\n",
    "#                     score_ = -1\n",
    "#                 event_issue.append(n_issues_)\n",
    "#                 event_score.append(score_)\n",
    "#                 if score_ > event_best_score:\n",
    "#                     event_best_score = score_\n",
    "#                     event_best_eps = event_eps\n",
    "#             print(\"Best Silhouete score is {} at eps: {} and number of issues: {}\".format(event_best_score, event_eps, n_issues_))\n",
    "#             fig = plt.figure()\n",
    "#             plt.plot(event_eps_list, event_score)\n",
    "#             fig.suptitle('Event eps and Silhouette score', fontsize=20)\n",
    "#             plt.xlabel('eps', fontsize=18)\n",
    "#             plt.ylabel('Silhouette score', fontsize=16)\n",
    "#             plt.show()\n",
    "            \n",
    "#             fig = plt.figure()\n",
    "#             plt.plot(event_eps_list, event_issue)\n",
    "#             fig.suptitle('Event eps and number of issues', fontsize=20)\n",
    "#             plt.xlabel('eps', fontsize=18)\n",
    "#             plt.ylabel('number of issues', fontsize=16)\n",
    "#             plt.show()\n",
    "            \n",
    "            # Set event_eps to best value\n",
    "            event_eps = 0.5\n",
    "            # Clustering issue\n",
    "            db_issue = DBSCAN(eps=event_eps, min_samples=event_neighbors).fit(event_vectors)\n",
    "            # Number of clusters in labels, ignoring noise if present.\n",
    "            n_issues_ = len(set(db_issue.labels_)) - (1 if -1 in db_issue.labels_ else 0)\n",
    "            n_noise_ = list(db_issue.labels_).count(-1)\n",
    "            print(n_issues_, n_noise_)\n",
    "            # Represent each issue by average sum of related news\n",
    "            issue_labels = np.array(list(map(lambda x: n_issues_ if x==-1 else x, db_issue.labels_)))\n",
    "            issue_vectors = np.zeros((n_issues_, doc_vectors.shape[1]))\n",
    "            for i in range(n_issues_+1):\n",
    "                if np.sum(issue_labels == i) != 0:\n",
    "                    issue_vectors[i] = np.sum(event_vectors[issue_labels == i], axis=0)/np.sum(issue_labels == i)\n",
    "       \n",
    "            issue_labels = np.array([ issue_labels[event_labels[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        else:\n",
    "            '''\n",
    "            Clustering using specific value\n",
    "            '''\n",
    "            # Clustering event\n",
    "            db_event = DBSCAN(eps=doc_eps, min_samples=doc_neighbors).fit(doc_vectors)\n",
    "            # Number of clusters in labels, ignoring noise if present.\n",
    "            n_events_ = len(set(db_event.labels_)) - (1 if -1 in db_event.labels_ else 0)\n",
    "            n_noise_ = list(db_event.labels_).count(-1)\n",
    "            print(n_events_, n_noise_)\n",
    "            # Represent each event by average sum of related news\n",
    "            event_labels = np.array(list(map(lambda x: n_events_ if x==-1 else x, db_event.labels_)))\n",
    "            event_vectors = np.zeros((n_events_, doc_vectors.shape[1]))\n",
    "            for i in range(n_events_+1):\n",
    "                if np.sum(event_labels == i) != 0:\n",
    "                    event_vectors[i] = np.sum(doc_vectors[event_labels == i], axis=0)/np.sum(event_labels == i)\n",
    "\n",
    "            # Clustering issue\n",
    "            db_issue = DBSCAN(eps=event_eps, min_samples=event_neighbors).fit(event_vectors)\n",
    "            # Number of clusters in labels, ignoring noise if present.\n",
    "            n_issues_ = len(set(db_issue.labels_)) - (1 if -1 in db_issue.labels_ else 0)\n",
    "            n_noise_ = list(db_issue.labels_).count(-1)\n",
    "            print(n_issues_, n_noise_)\n",
    "            # Represent each issue by average sum of related news\n",
    "            issue_labels = np.array(list(map(lambda x: n_issues_ if x==-1 else x, db_issue.labels_)))\n",
    "            issue_vectors = np.zeros((n_issues_, doc_vectors.shape[1]))\n",
    "            for i in range(n_issues_+1):\n",
    "                if np.sum(issue_labels == i) != 0:\n",
    "                    issue_vectors[i] = np.sum(event_vectors[issue_labels == i], axis=0)/np.sum(issue_labels == i)\n",
    "        \n",
    "            issue_labels = np.array([ issue_labels[event_labels[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        return n_issues_, n_events_, issue_labels, event_labels\n",
    "    \n",
    "    elif clustering_method=='agglomerative':\n",
    "        # Hyperparameters\n",
    "        n_events = 10000\n",
    "        n_issues = 6000\n",
    "        \n",
    "        # Clustering event\n",
    "        agg_event = AgglomerativeClustering(distance_threshold=0, n_clusters=n_events).fit(doc_vectors)\n",
    "        # Represent each event by average sum of related news\n",
    "        event_vectors = np.zeros((n_events, doc_vectors.shape[1]))\n",
    "        for i in range(n_events):\n",
    "            event_vectors[i] = sum(doc_vectors[agg_event.labels_ == i])\n",
    "        \n",
    "        plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "        # plot the top three levels of the dendrogram\n",
    "        plot_dendrogram(agg_event, truncate_mode=\"level\", p=3)\n",
    "        plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Clustering issue\n",
    "        agg_issue = AgglomerativeClustering(distance_threshold=0, n_clusters=n_issues).fit(event_vectors)\n",
    "        # Represent each issue by average sum of related news\n",
    "        issue_vectors = np.zeros((n_issues, doc_vectors.shape[1]))\n",
    "        for i in range(n_issues):\n",
    "            issue_vectors[i] = sum(event_vectors[agg_issue.labels_ == i])\n",
    "\n",
    "        issue_labels = np.array([ agg_issue.labels_[agg_event.labels_[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        return agg_issue, agg_event, issue_labels, agg_event.labels_\n",
    "    \n",
    "    elif clustering_method=='LDA':\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        assert(\"Doesn't support {}\".format(clustering_method))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "638b44c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "tfidf_doc2017_vectors = joblib.load('tfidf_titlebody_2017.csv')\n",
    "tfidf_doc2016_vectors = joblib.load('tfidf_titlebody_2016.csv')\n",
    "tfidf_doc2015_vectors = joblib.load('tfidf_titlebody_2015.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31e387b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6900 0\n",
      "4083 0\n"
     ]
    }
   ],
   "source": [
    "tfidf_doc2015_num_issue, tfidf_doc2015_num_event, tfidf_doc2015_issue_labels, tfidf_doc2015_event_labels = document_clustering(tfidf_doc2015_vectors, clustering_method='DBSCAN', evaluate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "220c7349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7327 0\n",
      "4682 0\n"
     ]
    }
   ],
   "source": [
    "tfidf_doc2016_num_issue, tfidf_doc2016_num_event, tfidf_doc2016_issue_labels, tfidf_doc2016_event_labels = document_clustering(tfidf_doc2016_vectors, clustering_method='DBSCAN', evaluate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "777567a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8985 0\n",
      "5588 0\n"
     ]
    }
   ],
   "source": [
    "tfidf_doc2017_num_issue, tfidf_doc2017_num_event, tfidf_doc2017_issue_labels, tfidf_doc2017_event_labels = document_clustering(tfidf_doc2017_vectors, clustering_method='DBSCAN', evaluate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "511b6fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of document in 2017: 9128 / 8985 clusters\n",
      "Number of document in 2016: 7485 / 7327 clusters\n",
      "Number of document in 2015: 7156 / 6900 clusters\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of document in 2017: {} / {} clusters\".format(len(tfidf_doc2017_vectors), tfidf_doc2017_num_event))\n",
    "print(\"Number of document in 2016: {} / {} clusters\".format(len(tfidf_doc2016_vectors), tfidf_doc2016_num_event))\n",
    "print(\"Number of document in 2015: {} / {} clusters\".format(len(tfidf_doc2015_vectors), tfidf_doc2015_num_event))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5c276e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counter_2017_event = Counter(tfidf_doc2017_event_labels)\n",
    "counter_2016_event = Counter(tfidf_doc2016_event_labels)\n",
    "counter_2015_event = Counter(tfidf_doc2015_event_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "323f5523",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_event_2017 = counter_2017_event.most_common(10)\n",
    "top10_event_2016 = counter_2016_event.most_common(10)\n",
    "top10_event_2015 = counter_2015_event.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "843acf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "language = \"en\"\n",
    "max_ngram_size = 3\n",
    "deduplication_threshold = 0.9\n",
    "numOfKeywords = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32a66daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017: event 67 with total 11 events\n",
      "('minus degree celsius', 9.219755014468854e-06)\n",
      "('minus minus degree', 1.91214118410618e-05)\n",
      "('seoul record minus', 2.0290254221821434e-05)\n",
      "('low minus minus', 2.5908955788797923e-05)\n",
      "('korea meteorological administration', 2.6445264346202868e-05)\n",
      "2017: event 617 with total 8 events\n",
      "('long eared bat', 2.1641010379165977e-05)\n",
      "('brown long eared', 2.5224330531459588e-05)\n",
      "('introduce amur leopard', 4.1999931200269735e-05)\n",
      "('large skin impression', 5.288360742214845e-05)\n",
      "('introduction amur leopard', 7.015091259583902e-05)\n",
      "2017: event 3702 with total 5 events\n",
      "('percent year early', 1.205172803891531e-05)\n",
      "('year early report', 2.5458099914630354e-05)\n",
      "('early report show', 2.864274110711425e-05)\n",
      "('year early accord', 3.1418735597870915e-05)\n",
      "('tally year early', 3.210785332786565e-05)\n",
      "2017: event 4724 with total 5 events\n",
      "('south korean scientist', 2.0591845163563497e-05)\n",
      "('group south korean', 3.922748931604556e-05)\n",
      "('korean scientist develop', 4.675891836271053e-05)\n",
      "('yonhap scientist develop', 7.058118832675124e-05)\n",
      "('ict future planning', 0.00012130325287960796)\n",
      "2017: event 5 with total 4 events\n",
      "('highly pathogenic avian', 2.1819910871743732e-05)\n",
      "('pathogenic avian influenza', 2.7504182724425383e-05)\n",
      "('highly pathogenic bird', 3.061410231418521e-05)\n",
      "('outbreak highly pathogenic', 3.0990994140131734e-05)\n",
      "('pathogenic bird flu', 3.1061555064146325e-05)\n",
      "2017: event 6642 with total 4 events\n",
      "('south korean cargo', 2.251516625451105e-05)\n",
      "('south korean employer', 2.3102836370507973e-05)\n",
      "('korean cargo ship', 2.7201381213883363e-05)\n",
      "('sink south atlantic', 3.106214699465214e-05)\n",
      "('miss south korean', 3.477632117171268e-05)\n",
      "2017: event 813 with total 3 events\n",
      "('ufc fight night', 2.0575851863445532e-05)\n",
      "('mixed martial art', 4.727454840882934e-05)\n",
      "('south korean mixed', 4.918743640301515e-05)\n",
      "('korean mixed martial', 5.1147288089201824e-05)\n",
      "('bang tae hyun', 5.438231228814869e-05)\n",
      "2017: event 911 with total 3 events\n",
      "('hong jong haak', 7.936692535970668e-05)\n",
      "('venture minister nominee', 0.00010403187702072047)\n",
      "('nominee hong jong', 0.00011724083210723539)\n",
      "('property inheritance past', 0.00012213237598659708)\n",
      "('family property inheritance', 0.00012377226842066557)\n",
      "2017: event 1342 with total 3 events\n",
      "('group chairman cho', 2.8932974080280798e-05)\n",
      "('chairman cho yang', 2.8932974080280798e-05)\n",
      "('hanjin group chairman', 3.156635981978823e-05)\n",
      "('company fund pay', 8.919557148000677e-05)\n",
      "('company work largely', 9.569941275513036e-05)\n",
      "2017: event 3137 with total 3 events\n",
      "('farm find contaminate', 2.0673063172908192e-05)\n",
      "('find contaminate pesticide', 2.9330706440835905e-05)\n",
      "('local farm find', 4.4387472569054215e-05)\n",
      "('egg local farm', 5.99093074364884e-05)\n",
      "('egg farm find', 6.560513116339458e-05)\n"
     ]
    }
   ],
   "source": [
    "for index, (event_id, num_event) in enumerate(top10_event_2017):\n",
    "    text_list = df2017[tfidf_doc2017_event_labels == event_id]['agg_title_body'].tolist()\n",
    "    tokens = spacy_tokenizer(' '.join(text_list))\n",
    "    text = ' '.join(tokens)\n",
    "    print(\"2017: event {} with total {} events\".format(event_id, num_event))\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "    keywords = custom_kw_extractor.extract_keywords(text)\n",
    "    for kw in keywords:\n",
    "        print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb982f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016: event 16 with total 5 events\n",
      "('operate south korea', 1.0180761183741554e-05)\n",
      "('encrypt number broadcast', 1.2637681937630137e-05)\n",
      "('broadcast mysterious number', 1.2916244897679014e-05)\n",
      "('south korea cold', 2.1824289723172455e-05)\n",
      "('confusion south korea', 2.2809464768852576e-05)\n",
      "2016: event 5089 with total 4 events\n",
      "('high voter turnout', 3.647952166480522e-05)\n",
      "('voter turnout south', 4.017305635869868e-05)\n",
      "('election voter turnout', 4.254788387805926e-05)\n",
      "('voter turnout record', 6.143088891028468e-05)\n",
      "('turnout general election', 6.210912132341609e-05)\n",
      "2016: event 430 with total 3 events\n",
      "('park geun hye', 1.7473955618365955e-06)\n",
      "('president park geun', 3.742740247361577e-06)\n",
      "('demand president park', 7.796943386785673e-06)\n",
      "('march presidential office', 9.548220927001613e-06)\n",
      "('president park step', 1.543518590450776e-05)\n",
      "2016: event 517 with total 3 events\n",
      "('haenam south jeolla', 0.0001278893722105576)\n",
      "('pathogenic aviation influenza', 0.00014025141782670082)\n",
      "('highly pathogenic aviation', 0.00015125366759665408)\n",
      "('south jeolla province', 0.0001591816344907264)\n",
      "('south korea report', 0.00016529055561510572)\n",
      "2016: event 705 with total 3 events\n",
      "('seoul metropolitan government', 7.803747634805222e-05)\n",
      "('stone wall walkway', 0.00012836736718471677)\n",
      "('deoksugung stonewall walkway', 0.00013579464397453966)\n",
      "('deoksugung stone wall', 0.00015011559956590805)\n",
      "('walkway deoksu palace', 0.00030504102670522834)\n",
      "2016: event 5776 with total 3 events\n",
      "('cloven hoofed animal', 3.618019689600726e-05)\n",
      "('south chungcheong province', 5.455525726134769e-05)\n",
      "('affect cloven hoofed', 7.035162624666826e-05)\n",
      "('meat cloven hoofed', 7.764503568273076e-05)\n",
      "('hoofed animal accord', 8.686287940725693e-05)\n",
      "2016: event 6228 with total 3 events\n",
      "('peace treaty talk', 5.577061910404876e-06)\n",
      "('hold peace treaty', 8.18953098149997e-06)\n",
      "('issue peace treaty', 1.6857534920083673e-05)\n",
      "('peace treaty negotiation', 3.336314291980825e-05)\n",
      "('discuss peace treaty', 3.4005355047500566e-05)\n",
      "2016: event 7158 with total 3 events\n",
      "('gifted talented education', 7.703841424137521e-05)\n",
      "('korea gifted education', 0.000159713504060335)\n",
      "('gifted education', 0.00017007648833079558)\n",
      "('gifted education center', 0.0001730028119716043)\n",
      "('education center gifted', 0.0001730028119716043)\n",
      "2016: event 11 with total 2 events\n",
      "('party people party', 0.0013565109134051308)\n",
      "('democratic party people', 0.0022019079807746355)\n",
      "('party', 0.00397307476004407)\n",
      "('assembly opposition party', 0.004045945120620259)\n",
      "('democratic party korea', 0.004411383486385555)\n",
      "2016: event 90 with total 2 events\n",
      "('independent counsel team', 0.00011277022037395873)\n",
      "('independent counsel scandal', 0.00011863192072563864)\n",
      "('scandal detention center', 0.00013842935465725528)\n",
      "('accord independent counsel', 0.0001385597020197578)\n",
      "('counsel team local', 0.0001385597020197578)\n"
     ]
    }
   ],
   "source": [
    "for index, (event_id, num_event) in enumerate(top10_event_2016):\n",
    "    text_list = df2016[tfidf_doc2016_event_labels == event_id]['agg_title_body'].tolist()\n",
    "    tokens = spacy_tokenizer(' '.join(text_list))\n",
    "    text = ' '.join(tokens)\n",
    "    print(\"2016: event {} with total {} events\".format(event_id, num_event))\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "    keywords = custom_kw_extractor.extract_keywords(text)\n",
    "    for kw in keywords:\n",
    "        print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c298b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015: event 2949 with total 6 events\n",
      "('disease diagnose mers', 2.7438471839288927e-05)\n",
      "('treatment disease report', 2.787500415329967e-05)\n",
      "('percent south korea', 2.839008136670725e-05)\n",
      "('stand percent people', 2.9134626699357003e-05)\n",
      "('percent people succumb', 2.9134626699357003e-05)\n",
      "2015: event 132 with total 5 events\n",
      "('percent year drop', 1.0212301092079983e-05)\n",
      "('percent year month', 1.4473654341860557e-05)\n",
      "('percent year early', 1.5997502085998573e-05)\n",
      "('follow percent year', 1.6071595994704207e-05)\n",
      "('percent year follow', 1.607159599470421e-05)\n",
      "2015: event 5027 with total 4 events\n",
      "('term fix rate', 0.00029215697085053604)\n",
      "('loan long term', 0.0002957165159838989)\n",
      "('long term fix', 0.0003143736608968223)\n",
      "('fix rate loan', 0.0003325720171449062)\n",
      "('quality household debt', 0.000790214884913063)\n",
      "2015: event 5185 with total 4 events\n",
      "('park tae hwan', 1.59087055786656e-05)\n",
      "('park time olympic', 2.8771386820050176e-05)\n",
      "('swimming champion park', 2.9866056056860903e-05)\n",
      "('world anti doping', 2.993293638462933e-05)\n",
      "('champion park tae', 3.056850351369427e-05)\n",
      "2015: event 5440 with total 4 events\n",
      "('sex slavery issue', 8.484122281210641e-06)\n",
      "('year mark anniversary', 1.939506234315656e-05)\n",
      "('talk sex slavery', 1.9995745705912866e-05)\n",
      "('apology sex slavery', 2.719092246013419e-05)\n",
      "('apologize sex slavery', 2.973746392186028e-05)\n",
      "2015: event 6001 with total 4 events\n",
      "('female flight attendant', 7.698070198182663e-06)\n",
      "('cabin crew chief', 8.111381533072797e-06)\n",
      "('korean air year', 1.5099756112364108e-05)\n",
      "('flight attendant cabin', 1.9738206723830467e-05)\n",
      "('attendant cabin crew', 1.990986494778767e-05)\n",
      "2015: event 1222 with total 3 events\n",
      "('korea local dealer', 5.5843178826162754e-05)\n",
      "('volkswagen korea local', 5.597274984839975e-05)\n",
      "('volkswagen group audi', 6.87722640896078e-05)\n",
      "('group audi volkswagen', 6.87722640896078e-05)\n",
      "('audi volkswagen korea', 6.97546180827566e-05)\n",
      "2015: event 1467 with total 3 events\n",
      "('current publication system', 4.7758194290973784e-05)\n",
      "('middle high school', 6.911373358746973e-05)\n",
      "('textbook publication system', 7.153851664915612e-05)\n",
      "('probe current publication', 8.316671543818134e-05)\n",
      "('history textbook government', 9.008175311119256e-05)\n",
      "2015: event 1835 with total 3 events\n",
      "('dolphin capsize september', 0.0001253528160380297)\n",
      "('toll coast guard', 0.00013177466789927804)\n",
      "('coast guard official', 0.00013177466789927807)\n",
      "('capsize fishing boat', 0.0001323294480679906)\n",
      "('korea coast guard', 0.00013628525406909827)\n",
      "2015: event 2163 with total 3 events\n",
      "('cable car project', 4.776690432131281e-06)\n",
      "('cable car route', 8.079926506595554e-06)\n",
      "('cable car mount', 1.0703679634162246e-05)\n",
      "('construction cable car', 1.4411074254312083e-05)\n",
      "('car mount seoraksan', 1.6612175509929323e-05)\n"
     ]
    }
   ],
   "source": [
    "for index, (event_id, num_event) in enumerate(top10_event_2015):\n",
    "    text_list = df2015[tfidf_doc2015_event_labels == event_id]['agg_title_body'].tolist()\n",
    "    tokens = spacy_tokenizer(' '.join(text_list))\n",
    "    text = ' '.join(tokens)\n",
    "    print(\"2015: event {} with total {} events\".format(event_id, num_event))\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "    keywords = custom_kw_extractor.extract_keywords(text)\n",
    "    for kw in keywords:\n",
    "        print(kw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
