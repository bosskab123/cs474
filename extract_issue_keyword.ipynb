{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74e29a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Modern\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Modern\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import string\n",
    "import random\n",
    "import unicodedata\n",
    "from string import punctuation\n",
    "from string import digits\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "from rake_nltk import Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3f1769e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23769, 7)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Reading data\n",
    "data_dir = 'data/'\n",
    "filename_prefix = 'koreaherald_1517_'\n",
    "df = []\n",
    "\n",
    "for i in range(8):\n",
    "    df.append(pd.read_json(os.path.join(data_dir, filename_prefix + str(i) + '.json')))\n",
    "df = pd.concat(df)\n",
    "df.reset_index(inplace=True)\n",
    "df = df.rename(columns=dict(zip(df.columns,[df.columns[i].strip() for i in range(len(df.columns))])))\n",
    "df.drop('index', inplace=True, axis=1)\n",
    "\n",
    "# Aggregate title and content\n",
    "title_weight = 1\n",
    "df['agg_title_body'] = title_weight*(df['title']+'. ') + df['body']\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77c006b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load large spacy model \n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "### Embed document and clustering\n",
    "df2017 = df['2017' < df['time']]\n",
    "df2016 = df[('2016' < df['time']) & (df['time'] < '2017')]\n",
    "df2015 = df[('2015' < df['time']) & (df['time'] < '2016')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd0e4f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lemmatization tool\n",
    "stemmer = WordNetLemmatizer()\n",
    "### Change similar words to the same word\n",
    "UN_WORD = \"The United Nations\"\n",
    "US_WORD = \"The United States\"\n",
    "NK_WORD = \"North Korea\"\n",
    "SK_WORD = \"South Korea\"\n",
    "\n",
    "similar_words = {\n",
    "    # Change to \"The United States\"\n",
    "    \"U.S.\": US_WORD,\n",
    "    \"US\": US_WORD,\n",
    "    \"USA\": US_WORD,\n",
    "    \"United States\": US_WORD,\n",
    "    \"United States'\": US_WORD,\n",
    "    \"The United States'\": US_WORD,\n",
    "    \n",
    "    # Change to \"North Korea\"\n",
    "    \"NK\": NK_WORD,\n",
    "    \"NK's\": NK_WORD,\n",
    "    \"N. Korea\": NK_WORD,\n",
    "    \"N. Korea's\": NK_WORD,\n",
    "    \"North Korea's\": NK_WORD,\n",
    "    \n",
    "    # Change to \"South Korea\"\n",
    "    \"SK\": SK_WORD,\n",
    "    \"SK's\": SK_WORD,\n",
    "    \"S. Korea\": SK_WORD,\n",
    "    \"S. Korea's\": SK_WORD,\n",
    "    \"South Korea's\": SK_WORD,\n",
    "    \n",
    "    # Change to \"The United Nations\"\n",
    "    \"United Nations\": UN_WORD,\n",
    "    \"United Nations'\": UN_WORD,\n",
    "    \"The United Nations'\": UN_WORD,\n",
    "    \"UN\": UN_WORD,\n",
    "}\n",
    "\n",
    "### Transform function\n",
    "def text_cleaning(s: str):\n",
    "        \n",
    "    def replace_strange_char(s: str):\n",
    "        non_en_chars = {\n",
    "            \"’\": \"'\",\n",
    "            \"‘\": \"'\"\n",
    "        }\n",
    "\n",
    "        def remove_non_en_chars(txt):\n",
    "            # remove non english characters\n",
    "            txt = convert_latin_chars(txt)\n",
    "            for char in non_en_chars.keys():\n",
    "                txt = re.sub(char, non_en_chars[char], txt)\n",
    "            txt = re.sub(r'[^\\x00-\\x7F]+', ' ', txt)\n",
    "            return txt\n",
    "\n",
    "        def convert_latin_chars(txt):\n",
    "            # convert latin characters\n",
    "            return ''.join(char for char in unicodedata.normalize('NFKD', txt) if unicodedata.category(char) != 'Mn')\n",
    "\n",
    "        s = remove_non_en_chars(s)\n",
    "        s = convert_latin_chars(s)\n",
    "        return s\n",
    "    s = replace_strange_char(s)\n",
    "    for key,value in similar_words.items():\n",
    "        s = re.sub(key, value, s)\n",
    "    return s\n",
    "\n",
    "def spacy_tokenizer(s: str):\n",
    "    # Change similar terms to the same term\n",
    "    new_str = text_cleaning(s)\n",
    "    doc = nlp(s)\n",
    "    # Group tokens\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    token_groupup_pattern = [\n",
    "        [{\"LOWER\": \"the\"}, {\"LOWER\": \"united\"}, {\"LOWER\": \"nations\"}],\n",
    "        [{\"LOWER\": \"the\"}, {\"LOWER\": \"united\"}, {\"LOWER\": \"states\"}],\n",
    "        [{\"LOWER\": \"north\"}, {\"LOWER\": \"korea\"}],\n",
    "        [{\"LOWER\": \"south\"}, {\"LOWER\": \"korea\"}],\n",
    "    ]\n",
    "    matcher.add(\"TermGroup\",token_groupup_pattern)\n",
    "    matches = matcher(doc)\n",
    "    merge_doc = []\n",
    "    for nid, start, end in matches:\n",
    "        merge_doc.append((start,end))\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for i in range(len(merge_doc)-1,-1,-1):\n",
    "            retokenizer.merge(doc[merge_doc[i][0]:merge_doc[i][1]])\n",
    "        \n",
    "    # Remove all stopword, punctuation, number\n",
    "    tokens = [ token.lemma_.lower() for token in doc \\\n",
    "              if not token.is_stop and not token.is_punct and not token.like_num and token.lemma_.strip()!= '']\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57633ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make TF-IDF matrix\n",
    "def tfidf_embed(documents, dimension=None):\n",
    "    # documents: list of str\n",
    "    # dim: integer\n",
    "    embeddings_dict = {}\n",
    "    tfidf_vectorizer = TfidfVectorizer(input='content', tokenizer=spacy_tokenizer)\n",
    "    tfidf_vector = tfidf_vectorizer.fit_transform(documents)\n",
    "    \n",
    "    # Dimensionality Reduction\n",
    "    if dimension is not None:\n",
    "        svd_doc = TruncatedSVD(n_components=dimension, n_iter=5, random_state=42)\n",
    "        tfidf_vector = svd_doc.fit_transform(tfidf_vector)\n",
    "    return tfidf_vector\n",
    "\n",
    "### Make GloVe matrix\n",
    "glove_file = \"../glove.42B.300d.txt\"\n",
    "def glove_word_vector():\n",
    "    embeddings_dict = {}\n",
    "    with open(glove_file, 'r') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "    return embeddings_dict\n",
    "\n",
    "# Average sum of word vectors\n",
    "def sentence_embed(sentence, word_vectors, dimension):\n",
    "    sum_vector = np.zeros(dimension)\n",
    "    for w in sentence.split():\n",
    "        if w in word_vectors:\n",
    "            sum_vector += word_vectors[w]\n",
    "    return sum_vector/len(sentence)\n",
    "\n",
    "# Make document vector\n",
    "def document_embed(documents, embedding_technique='tfidf', dimension=None):\n",
    "    if embedding_technique=='tfidf':\n",
    "        doc_vector = tfidf_embed(documents, dimension)\n",
    "    elif embedding_technique=='glove':\n",
    "        word_vector = glove_word_vector()\n",
    "        if dimension is None:\n",
    "            dimension = 300\n",
    "        doc_vector = [ sentence_embed(s, word_vector, dimension).tolist() for s in documents ]\n",
    "    elif embedding_technique=='spacy':\n",
    "        doc_vector = [doc.vector for doc in documents]\n",
    "    \n",
    "    return doc_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06d4dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clustering \n",
    "def document_clustering(doc_vectors, clustering_method='kmeans', evaluate=False):\n",
    "    if clustering_method=='kmeans':\n",
    "        # Hyperparameters\n",
    "        k_event = 10000\n",
    "        k_issue = 6000\n",
    "        \n",
    "        # Clustering event\n",
    "        kmeans_event = KMeans(n_clusters=k_event, random_state=69).fit(doc_vectors)\n",
    "        # Represent each event by average sum of related news\n",
    "        event_vectors = np.zeros((k_event, doc_vectors.shape[1]))\n",
    "        for i in range(k_event):\n",
    "            event_vectors[i] = sum(doc_vectors[kmeans_event.labels_ == i])\n",
    "        \n",
    "        # Clustering issue\n",
    "        kmeans_issue = KMeans(n_clusters=k_issue, random_state=69).fit(event_vectors)\n",
    "        # Represent each issue by average sum of related news\n",
    "        issue_vectors = np.zeros((k_issue, doc_vectors.shape[1]))\n",
    "        for i in range(k_issue):\n",
    "            issue_vectors[i] = sum(event_vectors[kmeans_issue.labels_ == i])\n",
    "\n",
    "        issue_labels = np.array([ kmeans_issue.labels_[kmeans_event.labels_[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        return k_issue, k_event, issue_labels, kmeans_event.labels_\n",
    "    \n",
    "    elif clustering_method=='DBSCAN':\n",
    "        \n",
    "        # Hyperparameters\n",
    "        doc_eps = 0.19\n",
    "        doc_neighbors = 1\n",
    "        event_eps = 0.50\n",
    "        event_neighbors = 1\n",
    "        \n",
    "        '''\n",
    "            Find best doc_eps and event_eps\n",
    "        '''\n",
    "        if evaluate:\n",
    "            # Find best eps to group same document\n",
    "            doc_eps_list = [ 0.10 + 0.001*i for i in range(1,301) ]\n",
    "            doc_score = []\n",
    "            doc_event = []\n",
    "            doc_best_score = 0\n",
    "            doc_best_eps = 0.0001\n",
    "            for doc_eps in doc_eps_list:\n",
    "                # Clustering event\n",
    "                db_event = DBSCAN(eps=doc_eps, min_samples=doc_neighbors).fit(doc_vectors)\n",
    "                # Number of clusters in labels, ignoring noise if present.\n",
    "                n_events_ = len(set(db_event.labels_)) - (1 if -1 in db_event.labels_ else 0)\n",
    "                if len(set(db_event.labels_)) >= 2 and len(set(db_event.labels_)) <= len(doc_vectors)-1:\n",
    "                    score_ = silhouette_score(doc_vectors, db_event.labels_)\n",
    "                else:\n",
    "                    score_ = -1\n",
    "                doc_event.append(n_events_)\n",
    "                doc_score.append(score_)\n",
    "                if score_ > doc_best_score:\n",
    "                    doc_best_score = score_\n",
    "                    doc_best_eps = doc_eps\n",
    "            print(\"Best Silhouete score is {} at eps: {} and number of events: {}\".format(doc_best_score, doc_eps, n_events_))\n",
    "            fig = plt.figure()\n",
    "            plt.plot(doc_eps_list, doc_score)\n",
    "            fig.suptitle('Doc eps and Silhouette score', fontsize=20)\n",
    "            plt.xlabel('eps', fontsize=18)\n",
    "            plt.ylabel('Silhouette score', fontsize=16)\n",
    "            plt.show()\n",
    "            \n",
    "            fig = plt.figure()\n",
    "            plt.plot(doc_eps_list, doc_event)\n",
    "            fig.suptitle('Doc eps and number of events', fontsize=20)\n",
    "            plt.xlabel('eps', fontsize=18)\n",
    "            plt.ylabel('number of events', fontsize=16)\n",
    "            plt.show()\n",
    "            \n",
    "            # Set doc_eps to the best value\n",
    "            doc_eps = doc_best_eps\n",
    "            # Find best eps to group same event\n",
    "            # Clustering event\n",
    "            db_event = DBSCAN(eps=doc_eps, min_samples=doc_neighbors).fit(doc_vectors)\n",
    "            n_events_ = len(set(db_event.labels_)) - (1 if -1 in db_event.labels_ else 0)\n",
    "            event_labels = np.array(list(map(lambda x: n_events_ if x==-1 else x, db_event.labels_)))\n",
    "            event_vectors = np.zeros((n_events_, doc_vectors.shape[1]))\n",
    "            for i in range(n_events_+1):\n",
    "                if np.sum(event_labels == i) != 0:\n",
    "                    event_vectors[i] = np.sum(doc_vectors[event_labels == i], axis=0)/np.sum(event_labels == i)\n",
    "            \n",
    "            \n",
    "#             # Clustering issue\n",
    "#             event_eps_list = [ 0.2 + 0.001*i for i in range(1,401) ]\n",
    "#             event_score = []\n",
    "#             event_issue = []\n",
    "#             event_best_score = 0\n",
    "#             event_best_eps = 0.001\n",
    "#             for event_eps in event_eps_list:\n",
    "#                 db_issue = DBSCAN(eps=event_eps, min_samples=event_neighbors).fit(event_vectors)\n",
    "#                 # Number of clusters in labels, ignoring noise if present.\n",
    "#                 n_issues_ = len(set(db_issue.labels_)) - (1 if -1 in db_issue.labels_ else 0)\n",
    "#                 if len(set(db_issue.labels_)) >= 2 and len(set(db_issue.labels_)) <= len(event_vectors)-1:\n",
    "#                     score_ = silhouette_score(event_vectors, db_issue.labels_)\n",
    "#                 else:\n",
    "#                     score_ = -1\n",
    "#                 event_issue.append(n_issues_)\n",
    "#                 event_score.append(score_)\n",
    "#                 if score_ > event_best_score:\n",
    "#                     event_best_score = score_\n",
    "#                     event_best_eps = event_eps\n",
    "#             print(\"Best Silhouete score is {} at eps: {} and number of issues: {}\".format(event_best_score, event_eps, n_issues_))\n",
    "#             fig = plt.figure()\n",
    "#             plt.plot(event_eps_list, event_score)\n",
    "#             fig.suptitle('Event eps and Silhouette score', fontsize=20)\n",
    "#             plt.xlabel('eps', fontsize=18)\n",
    "#             plt.ylabel('Silhouette score', fontsize=16)\n",
    "#             plt.show()\n",
    "            \n",
    "#             fig = plt.figure()\n",
    "#             plt.plot(event_eps_list, event_issue)\n",
    "#             fig.suptitle('Event eps and number of issues', fontsize=20)\n",
    "#             plt.xlabel('eps', fontsize=18)\n",
    "#             plt.ylabel('number of issues', fontsize=16)\n",
    "#             plt.show()\n",
    "            \n",
    "            # Set event_eps to best value\n",
    "            event_eps = 0.5\n",
    "            # Clustering issue\n",
    "            db_issue = DBSCAN(eps=event_eps, min_samples=event_neighbors).fit(event_vectors)\n",
    "            # Number of clusters in labels, ignoring noise if present.\n",
    "            n_issues_ = len(set(db_issue.labels_)) - (1 if -1 in db_issue.labels_ else 0)\n",
    "            n_noise_ = list(db_issue.labels_).count(-1)\n",
    "            # Represent each issue by average sum of related news\n",
    "            issue_labels = np.array(list(map(lambda x: n_issues_ if x==-1 else x, db_issue.labels_)))\n",
    "            issue_vectors = np.zeros((n_issues_, doc_vectors.shape[1]))\n",
    "            for i in range(n_issues_+1):\n",
    "                if np.sum(issue_labels == i) != 0:\n",
    "                    issue_vectors[i] = np.sum(event_vectors[issue_labels == i], axis=0)/np.sum(issue_labels == i)\n",
    "       \n",
    "            issue_labels = np.array([ issue_labels[event_labels[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        else:\n",
    "            '''\n",
    "            Clustering using specific value\n",
    "            '''\n",
    "            # Clustering event\n",
    "            db_event = DBSCAN(eps=doc_eps, min_samples=doc_neighbors).fit(doc_vectors)\n",
    "            # Number of clusters in labels, ignoring noise if present.\n",
    "            n_events_ = len(set(db_event.labels_)) - (1 if -1 in db_event.labels_ else 0)\n",
    "            n_noise_ = list(db_event.labels_).count(-1)\n",
    "            print(\"1st cluster:\\n\\tThe number of cluster is {}\".format(n_events_))\n",
    "            # Represent each event by average sum of related news\n",
    "            event_labels = np.array(list(map(lambda x: n_events_ if x==-1 else x, db_event.labels_)))\n",
    "            event_vectors = np.zeros((n_events_, doc_vectors.shape[1]))\n",
    "            for i in range(n_events_+1):\n",
    "                if np.sum(event_labels == i) != 0:\n",
    "                    event_vectors[i] = np.sum(doc_vectors[event_labels == i], axis=0)/np.sum(event_labels == i)\n",
    "\n",
    "            # Clustering issue\n",
    "            db_issue = DBSCAN(eps=event_eps, min_samples=event_neighbors).fit(event_vectors)\n",
    "            # Number of clusters in labels, ignoring noise if present.\n",
    "            n_issues_ = len(set(db_issue.labels_)) - (1 if -1 in db_issue.labels_ else 0)\n",
    "            n_noise_ = list(db_issue.labels_).count(-1)\n",
    "            print(\"2nd cluster:\\n\\tThe number of cluster is {}\".format(n_issues_))\n",
    "            # Represent each issue by average sum of related news\n",
    "            issue_labels = np.array(list(map(lambda x: n_issues_ if x==-1 else x, db_issue.labels_)))\n",
    "            issue_vectors = np.zeros((n_issues_, doc_vectors.shape[1]))\n",
    "            for i in range(n_issues_+1):\n",
    "                if np.sum(issue_labels == i) != 0:\n",
    "                    issue_vectors[i] = np.sum(event_vectors[issue_labels == i], axis=0)/np.sum(issue_labels == i)\n",
    "        \n",
    "            issue_labels = np.array([ issue_labels[event_labels[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        return n_issues_, n_events_, issue_labels, event_labels\n",
    "    \n",
    "    elif clustering_method=='agglomerative':\n",
    "        # Hyperparameters\n",
    "        n_events = 10000\n",
    "        n_issues = 6000\n",
    "        \n",
    "        # Clustering event\n",
    "        agg_event = AgglomerativeClustering(distance_threshold=0, n_clusters=n_events).fit(doc_vectors)\n",
    "        # Represent each event by average sum of related news\n",
    "        event_vectors = np.zeros((n_events, doc_vectors.shape[1]))\n",
    "        for i in range(n_events):\n",
    "            event_vectors[i] = sum(doc_vectors[agg_event.labels_ == i])\n",
    "        \n",
    "        plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "        # plot the top three levels of the dendrogram\n",
    "        plot_dendrogram(agg_event, truncate_mode=\"level\", p=3)\n",
    "        plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Clustering issue\n",
    "        agg_issue = AgglomerativeClustering(distance_threshold=0, n_clusters=n_issues).fit(event_vectors)\n",
    "        # Represent each issue by average sum of related news\n",
    "        issue_vectors = np.zeros((n_issues, doc_vectors.shape[1]))\n",
    "        for i in range(n_issues):\n",
    "            issue_vectors[i] = sum(event_vectors[agg_issue.labels_ == i])\n",
    "\n",
    "        issue_labels = np.array([ agg_issue.labels_[agg_event.labels_[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        return agg_issue, agg_event, issue_labels, agg_event.labels_\n",
    "    \n",
    "    elif clustering_method=='LDA':\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        assert(\"Doesn't support {}\".format(clustering_method))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c1fcb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/tfidf_titlebody_2015.csv']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf_doc2017_vectors = document_embed(df2017['agg_title_body'], embedding_technique='tfidf', dimension=300)\n",
    "# tfidf_doc2016_vectors = document_embed(df2016['agg_title_body'], embedding_technique='tfidf', dimension=300)\n",
    "# tfidf_doc2015_vectors = document_embed(df2015['agg_title_body'], embedding_technique='tfidf', dimension=300)\n",
    "# joblib.dump(tfidf_doc2017_vectors,'../data/tfidf_titlebody_2017.csv')\n",
    "# joblib.dump(tfidf_doc2016_vectors,'../data/tfidf_titlebody_2016.csv')\n",
    "# joblib.dump(tfidf_doc2015_vectors,'../data/tfidf_titlebody_2015.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "638b44c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "tfidf_doc2017_vectors = joblib.load('../data/tfidf_titlebody_2017.csv')\n",
    "tfidf_doc2016_vectors = joblib.load('../data/tfidf_titlebody_2016.csv')\n",
    "tfidf_doc2015_vectors = joblib.load('../data/tfidf_titlebody_2015.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31e387b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st cluster:\n",
      "\tThe number of cluster is 6919\n",
      "2nd cluster:\n",
      "\tThe number of cluster is 1190\n"
     ]
    }
   ],
   "source": [
    "tfidf_doc2015_num_issue, tfidf_doc2015_num_event, tfidf_doc2015_issue_labels, tfidf_doc2015_event_labels = document_clustering(tfidf_doc2015_vectors, clustering_method='DBSCAN', evaluate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "220c7349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st cluster:\n",
      "\tThe number of cluster is 7335\n",
      "2nd cluster:\n",
      "\tThe number of cluster is 1264\n"
     ]
    }
   ],
   "source": [
    "tfidf_doc2016_num_issue, tfidf_doc2016_num_event, tfidf_doc2016_issue_labels, tfidf_doc2016_event_labels = document_clustering(tfidf_doc2016_vectors, clustering_method='DBSCAN', evaluate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "777567a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st cluster:\n",
      "\tThe number of cluster is 9000\n",
      "2nd cluster:\n",
      "\tThe number of cluster is 1547\n"
     ]
    }
   ],
   "source": [
    "tfidf_doc2017_num_issue, tfidf_doc2017_num_event, tfidf_doc2017_issue_labels, tfidf_doc2017_event_labels = document_clustering(tfidf_doc2017_vectors, clustering_method='DBSCAN', evaluate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "511b6fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of document in 2017: 9128 / 9000 clusters\n",
      "Number of document in 2016: 7485 / 7335 clusters\n",
      "Number of document in 2015: 7156 / 6919 clusters\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of document in 2017: {} / {} clusters\".format(len(tfidf_doc2017_vectors), tfidf_doc2017_num_event))\n",
    "print(\"Number of document in 2016: {} / {} clusters\".format(len(tfidf_doc2016_vectors), tfidf_doc2016_num_event))\n",
    "print(\"Number of document in 2015: {} / {} clusters\".format(len(tfidf_doc2015_vectors), tfidf_doc2015_num_event))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5c276e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counter_2017_event = Counter(tfidf_doc2017_event_labels)\n",
    "counter_2016_event = Counter(tfidf_doc2016_event_labels)\n",
    "counter_2015_event = Counter(tfidf_doc2015_event_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "323f5523",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_event_2017 = counter_2017_event.most_common(10)\n",
    "top10_event_2016 = counter_2016_event.most_common(10)\n",
    "top10_event_2015 = counter_2015_event.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "843acf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "kw_extractor = yake.KeywordExtractor()\n",
    "language = \"en\"\n",
    "max_ngram_size = 4\n",
    "deduplication_threshold = 0.9\n",
    "numOfKeywords = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32a66daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017: event 67 with total 11 events\n",
      "('range c c return', -0.03593167801496952)\n",
      "('range c c. yonhap', -0.023620655108837686)\n",
      "('minus c c. freezing', -0.012916800744155098)\n",
      "('minus c c low', -0.009976584174158016)\n",
      "('minus c c cold', -0.006944431600824337)\n",
      "2017: event 617 with total 7 events\n",
      "('brown long eared bat', 1.3080837792958838e-06)\n",
      "('skin impression sauropod dinosaur', 4.130366357793372e-06)\n",
      "('impression sauropod dinosaur footprint', 4.731878946092821e-06)\n",
      "('large skin impression sauropod', 5.119234751513881e-06)\n",
      "('long eared bat find', 6.4363671707718436e-06)\n",
      "2017: event 3708 with total 5 events\n",
      "('percent year early report', 0.0004503729419082002)\n",
      "('year early report show', 0.0005080550969691897)\n",
      "('percent year early number', 0.0005625446324858537)\n",
      "('percent tally year early', 0.0005781321142472548)\n",
      "('year early accord datum', 0.000682364252266683)\n",
      "2017: event 4735 with total 5 events\n",
      "('group south korean scientist', 1.639719649119828e-06)\n",
      "('south korean scientist develop', 2.3959526456623667e-06)\n",
      "('south korean scientist discover', 9.393336560098534e-06)\n",
      "('ministry science ict future', 1.0488173471635689e-05)\n",
      "('science ict future planning', 1.0682345125423776e-05)\n",
      "2017: event 5 with total 4 events\n",
      "('highly pathogenic bird flu', 0.000774446241938374)\n",
      "('highly pathogenic avian influenza', 0.001051594209497281)\n",
      "('south korea slaughter bird', 0.0016021115657879358)\n",
      "('korea confirm highly pathogenic', 0.0020273291431861865)\n",
      "('outbreak highly pathogenic bird', 0.00224571819281019)\n",
      "2017: event 6654 with total 4 events\n",
      "('south korean cargo ship', 1.4581108739820286e-06)\n",
      "('crew member south korean', 1.7791046514528233e-06)\n",
      "('stellar daisy south korean', 2.454640132405829e-06)\n",
      "('text south korean employer', 2.7885330964295493e-06)\n",
      "('carry south korean filipino', 2.837338181501211e-06)\n",
      "2017: event 813 with total 3 events\n",
      "('south korean mixed martial', 0.0011170116289066698)\n",
      "('korean mixed martial art', 0.0011690622431536834)\n",
      "('fighter bang tae hyun', 0.0014223148223336292)\n",
      "('mixed martial art fighter', 0.001779093443964616)\n",
      "('bout ufc fight night', 0.0022635725295402373)\n",
      "2017: event 911 with total 3 events\n",
      "('minister nominee hong jong', 0.0026689233206573194)\n",
      "('nominee hong jong haak', 0.0032987961024042636)\n",
      "('hong jong haak', 0.0055557838642212075)\n",
      "('report outcome confirmation hearing', 0.006154739212864287)\n",
      "('confirmation hearing venture minister', 0.0064108574574443635)\n",
      "2017: event 4112 with total 3 events\n",
      "('group say try find', -0.014810040364435246)\n",
      "('city public art seoul', 7.295777259757906e-07)\n",
      "('art seoul streets find', 7.831974744198227e-07)\n",
      "('seoul streets find citizens', 1.0182961727648715e-06)\n",
      "('saturday seoul metropolitan government', 1.1126038327137094e-06)\n",
      "2017: event 5035 with total 3 events\n",
      "('korea confirm zika virus', 4.971018137400932e-06)\n",
      "('confirm zika virus infection', 5.405708300881392e-06)\n",
      "('korea confirm case zika', 5.963639708338238e-06)\n",
      "('infection south korea confirm', 6.321644513008059e-06)\n",
      "('confirm case zika virus', 6.485753160866529e-06)\n"
     ]
    }
   ],
   "source": [
    "for index, (event_id, num_event) in enumerate(top10_event_2017):\n",
    "    text_list = df2017[tfidf_doc2017_event_labels == event_id]['agg_title_body'].tolist()\n",
    "    tokens = spacy_tokenizer(' '.join(text_list))\n",
    "    text = ' '.join(tokens)\n",
    "    print(\"2017: event {} with total {} events\".format(event_id, num_event))\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "    keywords = custom_kw_extractor.extract_keywords(text)\n",
    "    for kw in keywords:\n",
    "        print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb982f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016: event 16 with total 5 events\n",
      "('north korea state radio', 0.0015266477311652229)\n",
      "('resume broadcast mysterious number', 0.001565467269316015)\n",
      "('resume encrypt number broadcast', 0.0016716454876568127)\n",
      "('spy operate south korea', 0.0016796182808237845)\n",
      "('korea state radio station', 0.001739900331525132)\n",
      "2016: event 518 with total 4 events\n",
      "('report possible ai outbreak', -5.387900363418817)\n",
      "('highly pathogenic aviation influenza', 0.0015295464328062567)\n",
      "('haenam south jeolla province', 0.001744719081315853)\n",
      "('chicken farm haenam south', 0.001909884689482229)\n",
      "('highly pathogenic bird flu', 0.0020454707622211575)\n",
      "2016: event 5095 with total 4 events\n",
      "('turnout usually mean young', -0.04279908574965713)\n",
      "('election voter turnout record', 3.506989294504152e-06)\n",
      "('general election voter turnout', 3.6177306509450238e-06)\n",
      "('voter turnout general election', 3.6177306509450246e-06)\n",
      "('turnout general election estimate', 4.561081686934742e-06)\n",
      "2016: event 431 with total 3 events\n",
      "('seong ju tell korea', -0.11566742539920344)\n",
      "('step right say park', -0.003313732815829161)\n",
      "('president park geun hye', 1.472523427755616e-07)\n",
      "('demand president park geun', 4.329271694603253e-07)\n",
      "('park geun hye step', 5.716200865089018e-07)\n",
      "2016: event 705 with total 3 events\n",
      "('deoksugung stone wall walkway', 1.530639297296742e-05)\n",
      "('opening deoksugung stone wall', 2.5709540062862202e-05)\n",
      "('stone wall surround deoksugung', 2.5845615185359767e-05)\n",
      "('october seoul metropolitan government', 2.743030274370599e-05)\n",
      "('august seoul metropolitan government', 2.7443422115533203e-05)\n",
      "2016: event 5782 with total 3 events\n",
      "('meat cloven hoofed animal', 5.420766130203182e-06)\n",
      "('cloven hoofed animal accord', 5.4207661302031825e-06)\n",
      "('cloven hoofed animal cattle', 5.4207661302031825e-06)\n",
      "('export meat cloven hoofed', 5.5191016907034975e-06)\n",
      "('affect cloven hoofed animal', 5.822626444277019e-06)\n",
      "2016: event 6234 with total 3 events\n",
      "('stance come the united', -0.08001705161858542)\n",
      "('denuclearization say new north', -0.0055472528433797895)\n",
      "('hold peace treaty talk', 6.889597124585786e-07)\n",
      "('proposal hold peace treaty', 1.2501778664396597e-06)\n",
      "('peace treaty talk north', 1.6156359264909074e-06)\n",
      "2016: event 7166 with total 3 events\n",
      "('educate accordingly say suh', -0.23277391741658807)\n",
      "('center gifted talented education', 1.3322873217447334e-05)\n",
      "('gifted class gifted education', 1.371206575908387e-05)\n",
      "('gifted education center gifted', 1.643256435620984e-05)\n",
      "('education center gifted school', 2.3395696449526375e-05)\n",
      "2016: event 11 with total 2 events\n",
      "('democratic party people party', 0.0010050721112910637)\n",
      "('party people party', 0.0026926824132196495)\n",
      "('party people party demand', 0.004017284014048907)\n",
      "('party friday agree hold', 0.004057792829964679)\n",
      "('political party friday agree', 0.004427666205540341)\n",
      "2016: event 90 with total 2 events\n",
      "('accord independent counsel team', 8.379752581345896e-06)\n",
      "('independent counsel team local', 8.379752581345896e-06)\n",
      "('company independent counsel scandal', 8.816932051624894e-06)\n",
      "('money accord independent counsel', 9.026704881777385e-06)\n",
      "('scandal involve president park', 9.861611219108147e-06)\n"
     ]
    }
   ],
   "source": [
    "for index, (event_id, num_event) in enumerate(top10_event_2016):\n",
    "    text_list = df2016[tfidf_doc2016_event_labels == event_id]['agg_title_body'].tolist()\n",
    "    tokens = spacy_tokenizer(' '.join(text_list))\n",
    "    text = ' '.join(tokens)\n",
    "    print(\"2016: event {} with total {} events\".format(event_id, num_event))\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "    keywords = custom_kw_extractor.extract_keywords(text)\n",
    "    for kw in keywords:\n",
    "        print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c298b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015: event 2957 with total 6 events\n",
      "('country see new case', -0.010565646044537904)\n",
      "('rate percent south korea', 1.6534229134971894e-06)\n",
      "('rate stand percent people', 1.6968917291301697e-06)\n",
      "('day disease diagnose mers', 1.90129440690986e-06)\n",
      "('percent south korea fatality', 1.916076676240595e-06)\n",
      "2015: event 132 with total 5 events\n",
      "('percent year follow percent', 0.00013664755882343462)\n",
      "('bear percent month year', 0.00017117455907262021)\n",
      "('percent year drop follow', 0.00018955922174981044)\n",
      "('year drop follow percent', 0.00018955922174981044)\n",
      "('drop percent year follow', 0.00018955922174981044)\n",
      "2015: event 5039 with total 4 events\n",
      "('positive way say financial', -0.36843413479517373)\n",
      "('fsc say new loan', -0.22535548844226266)\n",
      "('commission say new loan', -0.22215715284915521)\n",
      "('long term fix rate', 3.9005679739970884e-05)\n",
      "('term fix rate loan', 4.631372751020724e-05)\n",
      "2015: event 5457 with total 4 events\n",
      "('apology sex slavery issue', 0.0003796920941752618)\n",
      "('talk sex slavery major', 0.0003887610881032965)\n",
      "('tokyo hold sex slavery', 0.0004164095267482431)\n",
      "('sex slavery issue historian', 0.00041956280609104364)\n",
      "('hold sex slavery talk', 0.0004224729165791269)\n",
      "2015: event 6018 with total 4 events\n",
      "('flight attendant cabin crew', 0.0017077721600786888)\n",
      "('korean air year flight', 0.0019579461554997775)\n",
      "('attendant cabin crew chief', 0.002178253304769014)\n",
      "('air year flight attendant', 0.002185285514405928)\n",
      "('cabin crew chief', 0.0022887922821514437)\n",
      "2015: event 1224 with total 3 events\n",
      "('jason ha say lawsuit', -0.04603016096872609)\n",
      "('volkswagen korea local dealer', 3.6560726861471367e-06)\n",
      "('group audi volkswagen korea', 4.026650156806493e-06)\n",
      "('audi volkswagen korea local', 4.248379632561813e-06)\n",
      "('volkswagen group audi volkswagen', 4.514217598379007e-06)\n",
      "2015: event 1840 with total 3 events\n",
      "('toll coast guard official', 0.001636614955107019)\n",
      "('death toll coast guard', 0.0016366149551070193)\n",
      "('accord korea coast guard', 0.001820518838315756)\n",
      "('raise death toll coast', 0.0019149763174713395)\n",
      "('site accident raise death', 0.0020398607201687767)\n",
      "2015: event 2169 with total 3 events\n",
      "('location say far natural', -0.6099781083858562)\n",
      "('route say new location', -0.48030331637381307)\n",
      "('cable car mount seoraksan', 0.00013604890707739057)\n",
      "('diverse travel route mountain', 0.00017963028942099828)\n",
      "('cable car project mount', 0.00019058566279229493)\n",
      "2015: event 2867 with total 3 events\n",
      "('north preparatory group joint', 0.004820693041130968)\n",
      "('preparatory group joint anniversary', 0.005218230297847224)\n",
      "('south korean civic group', 0.005354684237969306)\n",
      "('joint event anniversary korea', 0.006089929877955144)\n",
      "('preparatory talk joint event', 0.006125522260915301)\n",
      "2015: event 3617 with total 3 events\n",
      "('dog eat south korea', 1.016096745212761e-06)\n",
      "('south korea dog meat', 1.2923424129524915e-06)\n",
      "('free dog dog meat', 1.990763143345254e-06)\n",
      "('meat farm south korea', 2.0879888172797637e-06)\n",
      "('dog meat farm south', 2.098729139917066e-06)\n"
     ]
    }
   ],
   "source": [
    "for index, (event_id, num_event) in enumerate(top10_event_2015):\n",
    "    text_list = df2015[tfidf_doc2015_event_labels == event_id]['agg_title_body'].tolist()\n",
    "    tokens = spacy_tokenizer(' '.join(text_list))\n",
    "    text = ' '.join(tokens)\n",
    "    print(\"2015: event {} with total {} events\".format(event_id, num_event))\n",
    "    custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "    keywords = custom_kw_extractor.extract_keywords(text)\n",
    "    for kw in keywords:\n",
    "        print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05ab69f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
