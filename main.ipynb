{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8df1d90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import string\n",
    "from string import punctuation\n",
    "from string import digits\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00c26fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading data\n",
    "data_dir = './data/'\n",
    "filename_prefix = 'koreaherald_1517'\n",
    "df0 = pd.read_json(os.path.join(data_dir, filename_prefix+'_0.json'))\n",
    "df1 = pd.read_json(os.path.join(data_dir, filename_prefix+'_1.json'))\n",
    "df2 = pd.read_json(os.path.join(data_dir, filename_prefix+'_2.json'))\n",
    "df3 = pd.read_json(os.path.join(data_dir, filename_prefix+'_3.json'))\n",
    "df4 = pd.read_json(os.path.join(data_dir, filename_prefix+'_4.json'))\n",
    "df5 = pd.read_json(os.path.join(data_dir, filename_prefix+'_5.json'))\n",
    "df6 = pd.read_json(os.path.join(data_dir, filename_prefix+'_6.json'))\n",
    "df7 = pd.read_json(os.path.join(data_dir, filename_prefix+'_7.json'))\n",
    "df = pd.concat([df0,df1,df2,df3,df4,df5,df6,df7])\n",
    "df.reset_index(inplace=True)\n",
    "df = df.rename(columns=dict(zip(df.columns,[df.columns[i].strip() for i in range(len(df.columns))])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3843b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>time</th>\n",
       "      <th>description</th>\n",
       "      <th>body</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A snapshot of multiculturalism in South Korea</td>\n",
       "      <td>Lee Sun-young</td>\n",
       "      <td>2018-01-01 17:07:00</td>\n",
       "      <td>With birthrates persistently low and the senio...</td>\n",
       "      <td>With birthrates persistently low and the senio...</td>\n",
       "      <td>Social affairs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Weekender] Korea’s dynamic 2017</td>\n",
       "      <td>Choi He-suk</td>\n",
       "      <td>2018-01-01 13:22:00</td>\n",
       "      <td>From North Korea’s nuclear weapons program nea...</td>\n",
       "      <td>From North Korea’s nuclear weapons program nea...</td>\n",
       "      <td>Social affairs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>People's Party members support Ahn's push for ...</td>\n",
       "      <td>Yonhap</td>\n",
       "      <td>2017-12-31 16:18:00</td>\n",
       "      <td>The leader of the center-left People's Party g...</td>\n",
       "      <td>The leader of the center-left People's Party g...</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[Newsmaker] Panamanian vessel probed over susp...</td>\n",
       "      <td>Yonhap</td>\n",
       "      <td>2017-12-31 14:55:00</td>\n",
       "      <td>PYEONGTAEK  -- South Korea has seized and insp...</td>\n",
       "      <td>PYEONGTAEK  -- South Korea has seized and insp...</td>\n",
       "      <td>North Korea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Hong Kong ship crew questioned in S. Korea for...</td>\n",
       "      <td>AFP</td>\n",
       "      <td>2017-12-30 15:44:00</td>\n",
       "      <td>The crew of a Hong Kong-registered ship have b...</td>\n",
       "      <td>The crew of a Hong Kong-registered ship have b...</td>\n",
       "      <td>North Korea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23764</th>\n",
       "      <td>2765</td>\n",
       "      <td>N. Korean leader's speech arouses cautious opt...</td>\n",
       "      <td>KH디지털2</td>\n",
       "      <td>2015-01-01 13:36:00</td>\n",
       "      <td>North Korean leader Kim Jong-un's New Year's D...</td>\n",
       "      <td>North Korean leader Kim Jong-un's New Year's D...</td>\n",
       "      <td>North Korea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23765</th>\n",
       "      <td>2766</td>\n",
       "      <td>N. Korean leader open to inter-Korean summit t...</td>\n",
       "      <td>KH디지털2</td>\n",
       "      <td>2015-01-01 10:05:00</td>\n",
       "      <td>North Korean leader Kim Jong-un said Thursday ...</td>\n",
       "      <td>North Korean leader Kim Jong-un said Thursday ...</td>\n",
       "      <td>North Korea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23766</th>\n",
       "      <td>2767</td>\n",
       "      <td>Ex-U.S. envoy calls for clearer communication ...</td>\n",
       "      <td>KH디지털2</td>\n",
       "      <td>2015-01-01 09:27:00</td>\n",
       "      <td>The United States should make its thoughts on ...</td>\n",
       "      <td>The United States should make its thoughts on ...</td>\n",
       "      <td>North Korea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23767</th>\n",
       "      <td>2768</td>\n",
       "      <td>U.S. imposes sanctions on N. Korean firm</td>\n",
       "      <td>KH디지털2</td>\n",
       "      <td>2015-01-01 09:25:00</td>\n",
       "      <td>The United States has imposed sanctions on a N...</td>\n",
       "      <td>The United States has imposed sanctions on a N...</td>\n",
       "      <td>North Korea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23768</th>\n",
       "      <td>2769</td>\n",
       "      <td>Park calls for military readiness amid tension...</td>\n",
       "      <td>KH디지털2</td>\n",
       "      <td>2015-01-01 09:24:00</td>\n",
       "      <td>President Park Geun-hye called on the military...</td>\n",
       "      <td>President Park Geun-hye called on the military...</td>\n",
       "      <td>Defense</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23769 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                              title  \\\n",
       "0          0      A snapshot of multiculturalism in South Korea   \n",
       "1          1                   [Weekender] Korea’s dynamic 2017   \n",
       "2          2  People's Party members support Ahn's push for ...   \n",
       "3          3  [Newsmaker] Panamanian vessel probed over susp...   \n",
       "4          4  Hong Kong ship crew questioned in S. Korea for...   \n",
       "...      ...                                                ...   \n",
       "23764   2765  N. Korean leader's speech arouses cautious opt...   \n",
       "23765   2766  N. Korean leader open to inter-Korean summit t...   \n",
       "23766   2767  Ex-U.S. envoy calls for clearer communication ...   \n",
       "23767   2768           U.S. imposes sanctions on N. Korean firm   \n",
       "23768   2769  Park calls for military readiness amid tension...   \n",
       "\n",
       "              author                 time  \\\n",
       "0      Lee Sun-young  2018-01-01 17:07:00   \n",
       "1        Choi He-suk  2018-01-01 13:22:00   \n",
       "2             Yonhap  2017-12-31 16:18:00   \n",
       "3             Yonhap  2017-12-31 14:55:00   \n",
       "4                AFP  2017-12-30 15:44:00   \n",
       "...              ...                  ...   \n",
       "23764         KH디지털2  2015-01-01 13:36:00   \n",
       "23765         KH디지털2  2015-01-01 10:05:00   \n",
       "23766         KH디지털2  2015-01-01 09:27:00   \n",
       "23767         KH디지털2  2015-01-01 09:25:00   \n",
       "23768         KH디지털2  2015-01-01 09:24:00   \n",
       "\n",
       "                                             description  \\\n",
       "0      With birthrates persistently low and the senio...   \n",
       "1      From North Korea’s nuclear weapons program nea...   \n",
       "2      The leader of the center-left People's Party g...   \n",
       "3      PYEONGTAEK  -- South Korea has seized and insp...   \n",
       "4      The crew of a Hong Kong-registered ship have b...   \n",
       "...                                                  ...   \n",
       "23764  North Korean leader Kim Jong-un's New Year's D...   \n",
       "23765  North Korean leader Kim Jong-un said Thursday ...   \n",
       "23766  The United States should make its thoughts on ...   \n",
       "23767  The United States has imposed sanctions on a N...   \n",
       "23768  President Park Geun-hye called on the military...   \n",
       "\n",
       "                                                    body         section  \n",
       "0      With birthrates persistently low and the senio...  Social affairs  \n",
       "1      From North Korea’s nuclear weapons program nea...  Social affairs  \n",
       "2      The leader of the center-left People's Party g...        Politics  \n",
       "3      PYEONGTAEK  -- South Korea has seized and insp...     North Korea  \n",
       "4      The crew of a Hong Kong-registered ship have b...     North Korea  \n",
       "...                                                  ...             ...  \n",
       "23764  North Korean leader Kim Jong-un's New Year's D...     North Korea  \n",
       "23765  North Korean leader Kim Jong-un said Thursday ...     North Korea  \n",
       "23766  The United States should make its thoughts on ...     North Korea  \n",
       "23767  The United States has imposed sanctions on a N...     North Korea  \n",
       "23768  President Park Geun-hye called on the military...         Defense  \n",
       "\n",
       "[23769 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "613d9292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load large spacy model \n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Aggregate title and content\n",
    "title_weight = 4\n",
    "df['agg_title_body'] = title_weight*(df['title']+'. ') + df['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "551df770",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lemmatization tool\n",
    "stemmer = WordNetLemmatizer()\n",
    "### Change similar words to the same word\n",
    "UN_WORD = \"The United Nations\"\n",
    "US_WORD = \"The United States\"\n",
    "NK_WORD = \"North Korea\"\n",
    "SK_WORD = \"South Korea\"\n",
    "\n",
    "similar_words = {\n",
    "    # Change to \"The United States\"\n",
    "    \"U.S.\": US_WORD,\n",
    "    \"US\": US_WORD,\n",
    "    \"USA\": US_WORD,\n",
    "    \"United States\": US_WORD,\n",
    "    \"United States'\": US_WORD,\n",
    "    \"The United States'\": US_WORD,\n",
    "    \n",
    "    \n",
    "    # Change to \"North Korea\"\n",
    "    \"NK\": NK_WORD,\n",
    "    \"NK's\": NK_WORD,\n",
    "    \"N. Korea\": NK_WORD,\n",
    "    \"N. Korea's\": NK_WORD,\n",
    "    \"North Korea's\": NK_WORD,\n",
    "    \n",
    "    # Change to \"South Korea\"\n",
    "    \"SK\": SK_WORD,\n",
    "    \"SK's\": SK_WORD,\n",
    "    \"S. Korea\": SK_WORD,\n",
    "    \"S. Korea's\": SK_WORD,\n",
    "    \"South Korea's\": SK_WORD,\n",
    "    \n",
    "    # Change to \"The United Nations\"\n",
    "    \"United Nations\": UN_WORD,\n",
    "    \"United Nations'\": UN_WORD,\n",
    "    \"The United Nations'\": UN_WORD,\n",
    "    \"UN\": UN_WORD,\n",
    "}\n",
    "\n",
    "### Transform function\n",
    "def transform_to_similar_sentence(s: str):\n",
    "    new_str = s\n",
    "    for key,value in similar_words.items():\n",
    "        new_str = re.sub(key, value, new_str)\n",
    "    return new_str\n",
    "\n",
    "# Using spacy to preprocess\n",
    "def preprocess_spacy(s: str):\n",
    "    # Change similar terms to the same term\n",
    "    new_str = transform_to_similar_sentence(s)\n",
    "    doc = nlp(s)\n",
    "    # Group tokens\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    token_groupup_pattern = [\n",
    "        [{\"LOWER\": \"the\"}, {\"LOWER\": \"united\"}, {\"LOWER\": \"nations\"}],\n",
    "        [{\"LOWER\": \"the\"}, {\"LOWER\": \"united\"}, {\"LOWER\": \"states\"}],\n",
    "        [{\"LOWER\": \"north\"}, {\"LOWER\": \"korea\"}],\n",
    "        [{\"LOWER\": \"south\"}, {\"LOWER\": \"korea\"}],\n",
    "    ]\n",
    "    matcher.add(\"TermGroup\",token_groupup_pattern)\n",
    "    matches = matcher(doc)\n",
    "    merge_doc = []\n",
    "    for nid, start, end in matches:\n",
    "        merge_doc.append((start,end))\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for i in range(len(merge_doc)-1,-1,-1):\n",
    "            retokenizer.merge(doc[merge_doc[i][0]:merge_doc[i][1]])\n",
    "        \n",
    "    # Remove all stopword, punctuation, number\n",
    "    tokens = [ token for token in doc if not token.is_stop and not token.is_punct and not token.like_num]\n",
    "    new_str = ' '.join([ token.lemma_.lower() for token in tokens ])\n",
    "    return new_str, tokens, doc\n",
    "\n",
    "### Preprocess function for grouping similar topic\n",
    "def preprocess_manual(s: str):\n",
    "    # Change similar words to the same word\n",
    "    new_str = transform_to_similar_sentence(s)\n",
    "    # Remove punctuation\n",
    "    new_str = ''.join(ch if ch not in set(punctuation) else \" \" for ch in new_str)\n",
    "    # Remove all single characters\n",
    "    new_str = re.sub(r'\\W', ' ', new_str)\n",
    "    new_str = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', new_str)\n",
    "    new_str = re.sub(r'\\^[a-zA-Z]\\s+', ' ', new_str) \n",
    "    # Substituting multiple spaces with single space\n",
    "    new_str = re.sub(r'\\s+', ' ', new_str, flags=re.I)\n",
    "    # Removing prefixed 'b' - when data is in bytes format\n",
    "    new_str = re.sub(r'^b\\s+', '', new_str)\n",
    "    # Removing all numbers\n",
    "    new_str = new_str.translate(str.maketrans('', '', digits))\n",
    "    # Converting to Lowercase\n",
    "    new_str = new_str.lower()\n",
    "    # Lemmatization and remove stopwords\n",
    "    new_str = new_str.split()\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    tokens = [stemmer.lemmatize(word) for word in new_str if word not in stopwords]\n",
    "    new_str = ' '.join(tokens)\n",
    "    \n",
    "    return new_str, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12821891",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = df['agg_title_body'].apply(lambda x: preprocess_spacy(x))\n",
    "# df[['ppcm_title_body','ppcm_tokens']] = df['agg_title_body'].apply(lambda x: preprocess_manual(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2f0b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " Preprocessed_data[0]: new text\n",
    " Preprocessed_data[1]: token\n",
    " Preprocessed_data[2]: doc\n",
    "'''\n",
    "preprocessed_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242c5afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 2000)\n",
    "df['ppcs_title_body'] = pd.Series([preprocessed_data[i][0] for i in range(len(preprocessed_data))])\n",
    "df['ppcs_title_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab08dd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make TF-IDF matrix\n",
    "def tfidf_embed(documents, dimension):\n",
    "    # documents: Doc class in spacy\n",
    "    embeddings_dict = {}\n",
    "    all_word = set()\n",
    "    word_counter = [ dict() for i in range(len(documents)) ]\n",
    "    for i in range(len(documents)):\n",
    "        for tok in documents[i]:\n",
    "            if tok.text not in word_counter[i]:\n",
    "                word_counter[i][tok.text] = 1\n",
    "            else:\n",
    "                word_counter[i][tok.text] += 1\n",
    "            if tok.text not in all_word:\n",
    "                all_word.add(tok.text)\n",
    "    word_count = []\n",
    "    for i in range(len(documents)):\n",
    "        l = []\n",
    "        for w in all_word:\n",
    "            if w not in word_counter[i]:\n",
    "                l.append(0)\n",
    "            else:\n",
    "                l.append(word_counter[i][w])\n",
    "        word_count.append(l)\n",
    "    word_count = np.array(word_count)\n",
    "#     count = CountVectorizer()\n",
    "#     word_count=count.fit_transform(documents)\n",
    "    tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "    tfidf_transformer.fit(word_count)\n",
    "    tfidf_vector=tfidf_transformer.transform(word_count)\n",
    "    tfidf_feature_names = count.get_feature_names_()\n",
    "    \n",
    "    # Dimensionality Reduction\n",
    "    svd_word = TruncatedSVD(n_components=dimension, n_iter=3, random_state=42)\n",
    "    svd_doc = TruncatedSVD(n_components=dimension, n_iter=3, random_state=42)\n",
    "    tfidf_word_vector = svd_word.fit_transform(tfidf_vector.T)\n",
    "    tfidf_doc_vector = svd_doc.fit_transform(tfidf_vector)\n",
    "    for i in range(len(tfidf_feature_names)):\n",
    "        name = tfidf_feature_names[i]\n",
    "        embeddings_dict[name] = tfidf_word_vector[i,:]\n",
    "        \n",
    "    return embeddings_dict, tfidf_doc_vector\n",
    "\n",
    "### Make GloVe matrix\n",
    "glove_file = \"../glove.42B.300d.txt\"\n",
    "def glove_embed():\n",
    "    embeddings_dict = {}\n",
    "    with open(glove_file, 'r') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "    return embeddings_dict\n",
    "\n",
    "# Average sum of word vectors\n",
    "def sentence_embed(sentence, word_vectors, dimension):\n",
    "    sum_vector = np.zeros(dimension)\n",
    "    for w in sentence.split():\n",
    "        if w in word_vectors:\n",
    "            sum_vector += word_vectors[w]\n",
    "    return sum_vector/len(sentence)\n",
    "\n",
    "# Make document vector\n",
    "def document_embed(documents, embedding_technique='tfidf', dimension=300):\n",
    "    if embedding_technique=='tfidf':\n",
    "        _, doc_vector = tfidf_embed(documents, dimension)\n",
    "    elif embedding_technique=='glove':\n",
    "        word_vector = glove_embed()\n",
    "        doc_vector = [ sentence_embed(s, word_vector, dimension).tolist() for s in documents ]\n",
    "    elif embedding_technique=='spacy':\n",
    "        doc_vector = [doc.vector for doc in documents]\n",
    "    \n",
    "    return doc_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a951018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Days difference between two datetime\n",
    "def days_between(d1, d2):\n",
    "    d1 = datetime.strptime(d1, \"%Y-%m-%d %H:%M:%S\")\n",
    "    d2 = datetime.strptime(d2, \"%Y-%m-%d %H:%M:%S\")\n",
    "    return abs((d2 - d1).days)\n",
    "\n",
    "# Function returns number of article, number of distinct authors, section of the issue, length\n",
    "def issue_indicator(news_index):\n",
    "    num_article = len(news_index)\n",
    "    num_author = len(df['author'][news_index].unique())\n",
    "    section = 0\n",
    "    length = days_between(df['time'][news_index].max(),df['time'][news_index].min())\n",
    "    return num_article, num_author, section, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86e517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clustering \n",
    "def document_clustering(doc_vectors, clustering_method='kmeans'):\n",
    "    if clustering_method=='kmeans':\n",
    "        # Hyperparameters\n",
    "        k_event = 10000\n",
    "        k_issue = 6000\n",
    "        \n",
    "        # Clustering event\n",
    "        kmeans_event = KMeans(n_clusters=k_event, random_state=69).fit(doc_vectors)\n",
    "        # Represent each event by average sum of related news\n",
    "        event_vectors = np.zeros((k_event, doc_vectors.shape[1]))\n",
    "        for i in range(k_event):\n",
    "            event_vectors[i] = sum(doc_vectors[kmeans_event.labels_ == i])\n",
    "        \n",
    "        # Clustering issue\n",
    "        kmeans_issue = KMeans(n_clusters=k_issue, random_state=69).fit(event_vectors)\n",
    "        # Represent each issue by average sum of related news\n",
    "        issue_vectors = np.zeros((k_issue, doc_vectors.shape[1]))\n",
    "        for i in range(k_issue):\n",
    "            issue_vectors[i] = sum(event_vectors[kmeans_issue.labels_ == i])\n",
    "\n",
    "        issue_labels = np.array([ kmeans_issue.labels_[kmeans_event.labels_[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        return k_issue, k_event, issue_labels, kmeans_event.labels_\n",
    "    \n",
    "    elif clustering_method=='DBSCAN':\n",
    "        eps = 0.1\n",
    "        min_samples = 1\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        assert(\"Doesn't support {}\".format(clustering_method))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ac45f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Embed document and clustering\n",
    "### Doc class in spacy\n",
    "preprocessed_docs = [preprocessed_data[i][2] for i in range(len(preprocessed_data))]\n",
    "tfidf_doc_vectors = document_embed(preprocessed_docs, embedding_technique='tfidf', dimension=300)\n",
    "glove_doc_vectors = document_embed(df['ppcs_title_body'], embedding_technique='glove', dimension=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dfc63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "num_clusters = 10000\n",
    "pca_num_components = 2\n",
    "tsne_num_components = 2\n",
    "\n",
    "color_list = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "             for i in range(number_of_colors)]\n",
    "labels_color_map = map(lambda (i,x): {i: x}, enumerate(color_list))\n",
    "\n",
    "X = tfidf_doc_vectors.todense()\n",
    "clustering_model = KMeans(\n",
    "    n_clusters=num_clusters,\n",
    "    max_iter=max_iterations,\n",
    "    precompute_distances=\"auto\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "labels = clustering_model.fit_predict(tfidf_doc_vectors)\n",
    "\n",
    "reduced_data = PCA(n_components=pca_num_components).fit_transform(X)\n",
    "fig, ax = plt.subplots()\n",
    "for index, instance in enumerate(reduced_data):\n",
    "    # print instance, index, labels[index]\n",
    "    pca_comp_1, pca_comp_2 = reduced_data[index]\n",
    "    color = labels_color_map[labels[index]]\n",
    "    ax.scatter(pca_comp_1, pca_comp_2, c=color)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c84dd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = TSNE(n_components=tsne_num_components)\n",
    "Y = embeddings.fit_transform(X)\n",
    "plt.scatter(Y[:, 0], Y[:, 1], cmap=plt.cm.Spectral)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9887e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = NearestNeighbors(n_neighbors=20)\n",
    "neighbors_fit = neighbors.fit(X)\n",
    "distances, indices = neighbors_fit.kneighbors(dataset)\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]\n",
    "plt.plot(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7098c08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_num_issue, tfidf_num_event, tfidf_issue_labels, tfidf_event_labels = document_clustering(tfidf_doc_vectors, clustering_method='kmeans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66a45b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434f48a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function returns number of article, number of distinct authors, section of the issue, length\n",
    "def issue_indicator(news_index):\n",
    "    num_article = len(news_index)\n",
    "    num_author = len(df['author'][news_index].unique())\n",
    "    section = 0\n",
    "    length = days_between(df['time'][news_index].max(),df['time'][news_index].min())\n",
    "    return num_article, num_author, section, length\n",
    "\n",
    "### Rank issues based on significance factors\n",
    "issue_significance = []\n",
    "for issue_index in range(tfidf_num_issue):\n",
    "    news_index = np.argwhere(tfidf_issue_labels == issue_index).reshape(-1)\n",
    "    if len(news_index) > 0:\n",
    "        issue_significance.append((issue_indicator(news_index),issue_index))\n",
    "issue_significance = sorted(issue_significance, key=lambda k: (-k[0][0],-k[0][1],-k[0][3]))\n",
    "top_10_issue = [ info[1] for info in issue_significance[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df863e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b168bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
