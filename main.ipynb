{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8df1d90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import string\n",
    "import random\n",
    "from string import punctuation\n",
    "from string import digits\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00c26fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading data\n",
    "data_dir = './data/'\n",
    "filename_prefix = 'koreaherald_1517'\n",
    "df0 = pd.read_json(os.path.join(data_dir, filename_prefix+'_0.json'))\n",
    "df1 = pd.read_json(os.path.join(data_dir, filename_prefix+'_1.json'))\n",
    "df2 = pd.read_json(os.path.join(data_dir, filename_prefix+'_2.json'))\n",
    "df3 = pd.read_json(os.path.join(data_dir, filename_prefix+'_3.json'))\n",
    "df4 = pd.read_json(os.path.join(data_dir, filename_prefix+'_4.json'))\n",
    "df5 = pd.read_json(os.path.join(data_dir, filename_prefix+'_5.json'))\n",
    "df6 = pd.read_json(os.path.join(data_dir, filename_prefix+'_6.json'))\n",
    "df7 = pd.read_json(os.path.join(data_dir, filename_prefix+'_7.json'))\n",
    "df = pd.concat([df0,df1,df2,df3,df4,df5,df6,df7])\n",
    "df.reset_index(inplace=True)\n",
    "df = df.rename(columns=dict(zip(df.columns,[df.columns[i].strip() for i in range(len(df.columns))])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "613d9292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load large spacy model \n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# Aggregate title and content\n",
    "title_weight = 1\n",
    "df['agg_title_body'] = title_weight*(df['title']+'. ') + df['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "551df770",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lemmatization tool\n",
    "stemmer = WordNetLemmatizer()\n",
    "### Change similar words to the same word\n",
    "UN_WORD = \"The United Nations\"\n",
    "US_WORD = \"The United States\"\n",
    "NK_WORD = \"North Korea\"\n",
    "SK_WORD = \"South Korea\"\n",
    "\n",
    "similar_words = {\n",
    "    # Change to \"The United States\"\n",
    "    \"U.S.\": US_WORD,\n",
    "    \"US\": US_WORD,\n",
    "    \"USA\": US_WORD,\n",
    "    \"United States\": US_WORD,\n",
    "    \"United States'\": US_WORD,\n",
    "    \"The United States'\": US_WORD,\n",
    "    \n",
    "    # Change to \"North Korea\"\n",
    "    \"NK\": NK_WORD,\n",
    "    \"NK's\": NK_WORD,\n",
    "    \"N. Korea\": NK_WORD,\n",
    "    \"N. Korea's\": NK_WORD,\n",
    "    \"North Korea's\": NK_WORD,\n",
    "    \n",
    "    # Change to \"South Korea\"\n",
    "    \"SK\": SK_WORD,\n",
    "    \"SK's\": SK_WORD,\n",
    "    \"S. Korea\": SK_WORD,\n",
    "    \"S. Korea's\": SK_WORD,\n",
    "    \"South Korea's\": SK_WORD,\n",
    "    \n",
    "    # Change to \"The United Nations\"\n",
    "    \"United Nations\": UN_WORD,\n",
    "    \"United Nations'\": UN_WORD,\n",
    "    \"The United Nations'\": UN_WORD,\n",
    "    \"UN\": UN_WORD,\n",
    "}\n",
    "\n",
    "### Transform function\n",
    "def transform_to_similar_sentence(s: str):\n",
    "    new_str = s\n",
    "    for key,value in similar_words.items():\n",
    "        new_str = re.sub(key, value, new_str)\n",
    "    return new_str\n",
    "\n",
    "# Using spacy to preprocess\n",
    "def preprocess_spacy(s: str):\n",
    "    # Change similar terms to the same term\n",
    "    new_str = transform_to_similar_sentence(s)\n",
    "    doc = nlp(s)\n",
    "    # Group tokens\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    token_groupup_pattern = [\n",
    "        [{\"LOWER\": \"the\"}, {\"LOWER\": \"united\"}, {\"LOWER\": \"nations\"}],\n",
    "        [{\"LOWER\": \"the\"}, {\"LOWER\": \"united\"}, {\"LOWER\": \"states\"}],\n",
    "        [{\"LOWER\": \"north\"}, {\"LOWER\": \"korea\"}],\n",
    "        [{\"LOWER\": \"south\"}, {\"LOWER\": \"korea\"}],\n",
    "    ]\n",
    "    matcher.add(\"TermGroup\",token_groupup_pattern)\n",
    "    matches = matcher(doc)\n",
    "    merge_doc = []\n",
    "    for nid, start, end in matches:\n",
    "        merge_doc.append((start,end))\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for i in range(len(merge_doc)-1,-1,-1):\n",
    "            retokenizer.merge(doc[merge_doc[i][0]:merge_doc[i][1]])\n",
    "        \n",
    "    # Remove all stopword, punctuation, number\n",
    "    tokens = [ token for token in doc if not token.is_stop and not token.is_punct and not token.like_num]\n",
    "    new_str = ' '.join([ token.lemma_.lower() for token in tokens ])\n",
    "    return new_str, tokens, doc\n",
    "\n",
    "def spacy_tokenizer(s: str):\n",
    "    # Change similar terms to the same term\n",
    "    new_str = transform_to_similar_sentence(s)\n",
    "    doc = nlp(s)\n",
    "    # Group tokens\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    token_groupup_pattern = [\n",
    "        [{\"LOWER\": \"the\"}, {\"LOWER\": \"united\"}, {\"LOWER\": \"nations\"}],\n",
    "        [{\"LOWER\": \"the\"}, {\"LOWER\": \"united\"}, {\"LOWER\": \"states\"}],\n",
    "        [{\"LOWER\": \"north\"}, {\"LOWER\": \"korea\"}],\n",
    "        [{\"LOWER\": \"south\"}, {\"LOWER\": \"korea\"}],\n",
    "    ]\n",
    "    matcher.add(\"TermGroup\",token_groupup_pattern)\n",
    "    matches = matcher(doc)\n",
    "    merge_doc = []\n",
    "    for nid, start, end in matches:\n",
    "        merge_doc.append((start,end))\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for i in range(len(merge_doc)-1,-1,-1):\n",
    "            retokenizer.merge(doc[merge_doc[i][0]:merge_doc[i][1]])\n",
    "        \n",
    "    # Remove all stopword, punctuation, number\n",
    "    tokens = [ token.lemma_.lower() for token in doc \\\n",
    "              if not token.is_stop and not token.is_punct and not token.like_num and token.lemma_.strip()!= '']\n",
    "    return tokens\n",
    "\n",
    "### Preprocess function for grouping similar topic\n",
    "def preprocess_manual(s: str):\n",
    "    # Change similar words to the same word\n",
    "    new_str = transform_to_similar_sentence(s)\n",
    "    # Remove punctuation\n",
    "    new_str = ''.join(ch if ch not in set(punctuation) else \" \" for ch in new_str)\n",
    "    # Remove all single characters\n",
    "    new_str = re.sub(r'\\W', ' ', new_str)\n",
    "    new_str = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', new_str)\n",
    "    new_str = re.sub(r'\\^[a-zA-Z]\\s+', ' ', new_str) \n",
    "    # Substituting multiple spaces with single space\n",
    "    new_str = re.sub(r'\\s+', ' ', new_str, flags=re.I)\n",
    "    # Removing prefixed 'b' - when data is in bytes format\n",
    "    new_str = re.sub(r'^b\\s+', '', new_str)\n",
    "    # Removing all numbers\n",
    "    new_str = new_str.translate(str.maketrans('', '', digits))\n",
    "    # Converting to Lowercase\n",
    "    new_str = new_str.lower()\n",
    "    # Lemmatization and remove stopwords\n",
    "    new_str = new_str.split()\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    tokens = [stemmer.lemmatize(word) for word in new_str if word not in stopwords]\n",
    "    new_str = ' '.join(tokens)\n",
    "    \n",
    "    return new_str, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12821891",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tokens = df['agg_title_body'].apply(lambda x: spacy_tokenizer(x))\n",
    "# df[['ppcm_title_body','ppcm_tokens']] = df['agg_title_body'].apply(lambda x: preprocess_manual(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "242c5afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                                                                                                                                                                                                                                                                                snapshot multiculturalism south korea birthrate persistently low senior population grow south koreaâ€˜s work age population project shrink onward backdrop steady inflow immigrant foreign resident potent factor bring major change fabric south korean society long consider culturally ethnically homogeneous look multiculturalism grow hold visible mainstream migrant worker foreign national reside south korea visa scheme lion share total migrant worker bring china southeast asian country job shun educate south korean worker majority worker manufacturing noticeable rise agricultural fishery sector struggle chronic labor shortage tally statistics korea leave thousand undocumented foreign laborer live work valid visa foreign wife late 1990 international marriage rise rapidly major route immigration korea recent year pace growth moderate percent marriage involve non korean national ratio percent percent international union foreign wife korean husband time year vietnam take china country birth foreign wife percent china come percent percent wife come philippines percent japan union average age marriage korean husband wife foreign studentsthe number international student receive high education top mark time chinese overwhelming group percent vietnam mongolia japan the united states follow share marginal percent rise new generation yonhap)in baby mixed heritage bear south korea account percent birth country rural province jeolla record high number multicultural baby bear follow southern island province jeju education ministry datum put number multicultural student attend elementary middle high school august surpass mark time mark fivefold increase number lee sun young milaya@heraldcorp.com\n",
       "1        weekender korea dynamic north korea nuclear weapon program near completion unprecedented impeachment president major earthquake year draw close difficult south korea corruption scandal break lead country impeachment president president park geun hye ouster march turn lead close ally arrest trial host charge wake park fall grace conservative split faction liberal president moon jae win presidential election moon election spark series event hint sweeping change limited time korean society yonhap)moon quickly launch war accumulate ill new administration refer unfair practice condition society reform drive raise allegation wrongdoing go lee myung bak administration come office economic moon reform drive far result large minimum wage hike high taxis high earn company individual fiercely resist criticize conservative bloc month moon presidency far see threat pose north korea rise new dimension september north korea conduct nuclear test claim successfully develop hydrogen bomb north korea conduct number intercontinental ballistic missile test start combination imply pyongyang cross red line moon draw north korea policy press conference mark day office august moon state north korea capable build icbm tip nuclear warhead mean red line cross year see south korea experience friction large trading partner china us.the trouble china begin follow park administration decision deploy terminal high altitude area defense anti missile system decision deploy thaad prompt economic retaliation china deal massive blow south korean company tourism industry seoul beijing relation thaw height tension side narrow difference thaad beijing consider threat national security less scale seoul trade relation hit rut korea free trade agreement president donald trump term horrible deal despite seoul attempt avoid renegotiation trump administration press issue numerous occasion side currently renegotiate deal year see major domestic issue unfold march ferry sewol raise seabed nearly year sink coun...\n",
       "2                            people party member support ahn push merger bareun party leader center leave people party garner overwhelming support member push merger minor conservative party party announce sunday election committee party say percent member participate week internal vote support ahn cheol soo proposal unite country large party center right bareun party poll conduct wednesday saturday phone online turnout percent total member take vote panel say percent oppose ahn ahn cheol soo press conference sunday follow people party vote result yonhap)ahn december propose vote merger aim beef centrist force ahead local election slate june ahn pledge resign lose vote announcement presidential candidate vow push merger swiftly receive overwhelming support member people party ahn say news conference accept party member express vote aspiration change look way forward path integration say ahn rival party call vote invalid cite low turnout say ahn fail win vote step promise \"this clear indication vote nonconfidence ahn group lawmaker say press conference launch group oust ahn block call conservative collusion dissenter largely base southwestern region traditionally oppose rightist politician merger conservative bareun party dent prospect future election include june gubernatorial mayoral election low turnout far quorum member require party constitution determine crucial issue merger party poll lead ahn faction directly merger plan question respondent support ahn push merger ahn point voter ratio close percent turnout party election august ahn pick chairman early day man barge party headquarters attack rep. lee dong seop chief election committee announce vote result man kick podium leg get tussle party official take police custody later integration party see strategy combine centrist political force counterbalance major party rule democratic party seat main opposition liberty korea party seat 299-member national assembly people party hold seat bareun party has11 lawmaker yonhap\n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         newsmaker panamanian vessel probe suspect oil supply n. korea pyeongtaek south korea seize inspect panama flag ship suspect sell oil north korea sea violation u.n. sanction maritimeauthorities say 5,100-ton koti hold western port pyeongtaek dangjin gyeonggi province security custom official hold meeting december decide allow vessel depart port local maritime office say file photo pyeongtaek harbor yonhap)it know vessel probe south korea suspicion ship ship supply petroleum product north korea ban international sanction nuclear missile program authority say crew ship koti chinese myanmarese decline provide detail probe foreign ministry say receive formal notification related agency inquiry seoul government announce friday investigate hong kong flag ship name lighthouse winmore allegedly transfer oil north korean vessel international water october 19.the ship seize yeosu port november suspect transfer ton refined petroleum north korean ship name samjong unsc resolution adopt september ban member country ship ship transfer good north korea resolution adopt week early allow country capture look vessel suspect engage prohibit activity north korea yonhap\n",
       "4        hong kong ship crew question s. korea oil transfer north crew hong kong register ship detain question south korea tanker impound november transfer oil north korean vessel breach un sanction custom official say saturday lighthouse winmore charter taiwanese company impound south korean custom authority port yeosu november follow inspection \"since inspector come board question crew korea customs service official tell afp lighthouse winmore impound south korean custom authority port yeosu november follow inspection afp yonhap)the lighthouse winmore crew member include mainland chinese citizen myanmar national custom official yeosu say tanker charter taiwanese company billions bunker group corp. previously visit yeosu october load tonne japanese refined oil head purport destination taiwan instead go taiwan vessel transfer tonne oil north sam jong international water china return yeosu custom service official say early foreign ministry official seoul say ship seize briefly custom authority inspect result investigation report un security council sanction committee foreign ministry official say lighthouse winmore ship ask security council blacklist violate sanction north korea taipei say billions bunker group incorporate taiwan marshall islands continue fully comply un sanction north korea taiwan transport ministry say investigate taiwanese entity involve ship own hong kong register company call win shipping limited friday address give firm hong kong company registry ship north korean vessel palau flag oil tanker block international port un security council thursday suspicion carry transport good ban sanction target pyongyang weapon ambition accord final list adopt world body sam jong ban vessel appear list ship suspect transport illicit cargo lighthouse winmore ask security council blacklist vessel china object proposal diplomat say agree blacklist ship thursday security council impose set sanction north korea year august target iron coal fishing industry set september...\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "23764    n. korean leader speech arouse cautious optimism north korean leader kim jong un new year day speech herald aggressive peace offensive regime early optimistic landmark breakthrough expert say thursday appear state television kim raise possibility summit talk south korean president park geun hye depend mood circumstance create reason hold high level talk say time talk publicly inter korean summit spend unusually lot time stress significance improve seoul pyongyang relation kim speak need improve inter korean tie year version spend time specific address issue year kim park totally different strategy political situation home share key long term goal reunification korea importance agenda grow year korea mark anniversary liberation japan colonial rule begin improve inter korean tie big accomplishment leader kim enter year power park enter year north korean leader kim express improve south north relation new year speech specific manner year say chang yong seok senior researcher seoul national university institute north korea expect aggressively propose dialogue year yoo ho yeol professor north korean study korea university agree say kim practical decision choose dialogue face reality resolve economic problem address inter korean relation new year message south president pledge effort peace preparation reunification time park kim need accomplishment inter korean relation say yang moo jin professor university north korean studies chance high side break deadlock show flexible attitude north expect accept south offer dialogue presidential panel propose early week minister level talk january seoul exist offer vice ministerial meeting kim say communist regime seek high level talk type negotiation open new chapter bilateral relation analyst park kim meet russia invite attend local ceremony mark end world war ii remain uncertain side produce significant breakthrough come month number hurdle leave kim reiterate pyongyang suspension routine joint military drill south korea the ...\n",
       "23765    n. korean leader open inter korean summit talk north korean leader kim jong un say thursday willing hold summit talk south korean president park geun hye year aim make big shift inter korean tie new year day address say pyongyang effort advance dialogue cooperation seoul depend mood circumstance create reason hold high level talk say nationally televise speech 30-minute speech emphasize need improved relation rival koreas importance develop communist nation military capability living standard people yonhap)kim say high level talk kind negotiation side resume south want improve bilateral tie dialogue say actively seek development mount kumgang tourist zone draw foreign visitor bid diversity country external economic relation remark raise hope north respond positively south new offer ministerial talk early week south presidential panel reunification propose talk north january discuss pende issue include reunion family separate korean war south korea hope significant progress inter korean tie year mark anniversary korea liberation japan 35-year colonial rule south president eager bear fruit push ease military tension peninsula enter year tenure kim echo park state goal lay groundwork reunification korea say tragic division long tolerable acceptable condemn regular joint defense drill south korea u.s. deepen tension korean peninsula demand washington end anachronistic hostile policy pyongyang need twice tense mood war preparatory exercise trust base dialogue possible north south relation forward say kim add regime stick military policy strategy simultaneously develop economy nuclear program deal resolutely provocation war infringe sovereignty dignity punitive steps,\"he say mark kim new year day speech usual year address close watch outside world clue pyongyang external policy early day begin official activity visit kumsusan palace sun mausoleum pyongyang commemorate late father kim jong il grandfather kim il sung analyst say north flamboyant leader step effort tight...\n",
       "23766                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ex u.s. envoy call clear communication china n. korea the united states thought north korea future korean peninsula clearly know china order win beijing cooperation resolve series problem involve north american nuclear negotiator say wednesday assistant secretary state christopher hill double washington chief negotiator party talk north nuclear program say china u.s. address issue north korea formulaic way chinese declare support dialogue u.s. urge china specify u.s. goal persuade china deter north korean rogue behavior high priority mean communicate chinese clearly north korea stand priority hill say article contribute project syndicate particular china need know u.s. view future arrangement korea peninsula china worry north korea regime collapse probably follow state failure perceive chinese defeat victory korea reunify alliance system say give access deep thinking issue good mean encourage cooperation important doctrine surprise chinese today frequently discuss policy great country relation win win arrangement work concept say washington encourage well relation seoul beijing say note widespread perception region u.s. frown close seoul beijing relation china south korea future mean america hill stress soon china feel comfortable south korea well u.s. hollywood human right cyber security issue china particularly comfortable address confluence attest need well channel sino american cooperation north korea say time come strategic reengagement china north korea problem solve yonhap\n",
       "23767                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        u.s. impose sanction n. korean firm the united states impose sanction north korean firm entity china iran nation involvement proliferation weapon mass destruction state department say north ryongaksan general trading corp. list entity individual slap sanction iran north korea syria nonproliferation act accord state department notice federal register notice date tuesday provide detail include north korean firm specifically company suspect provide assistance communist nation export ballistic missile measure u.s. government agency ban dealing sanction entity sanction north korean firm symbolic believe dealing u.s. government agency sanction list include individual company china russian company sudanese company agency company syria venezuelan company yonhap\n",
       "23768                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                park call military readiness amid tension n. korea president park geun hye call military thursday maintain readiness say robust security lay groundwork potential unification north korea end history division put end cold war korean peninsula park say new year message country 650,000-strong military korean peninsula divide capitalistic south communist north liberation japan colonial rule comment come day south korea propose rival koreas hold ministerial talk january discuss bilateral issue reunion family separate war remain unclear north korea accept south korea offer express hope military receive public trust love establish new military culture remark apparently aim boost morale military south korea military come fire year series death blame deep rooted culture abuse bullying barrack june army sergeant bully comrade go shooting rampage near border north korea kill people wound able bodied south korean man carry compulsory military service year country face north korea heavily fortify border yonhap\n",
       "Name: ppcs_title_body, Length: 23769, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 2000)\n",
    "df['ppcs_title_body'] = pd.Series([' '.join(data_tokens[i]) for i in range(len(data_tokens))])\n",
    "df['ppcs_title_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab08dd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make TF-IDF matrix\n",
    "def tfidf_embed(documents, dimension=None):\n",
    "    # documents: list of str\n",
    "    # dim: integer\n",
    "    embeddings_dict = {}\n",
    "    tfidf_vectorizer = TfidfVectorizer(input='content', tokenizer=spacy_tokenizer)\n",
    "    tfidf_vector = tfidf_vectorizer.fit_transform(documents)\n",
    "    \n",
    "    # Dimensionality Reduction\n",
    "    if dimension is not None:\n",
    "        svd_doc = TruncatedSVD(n_components=dimension, n_iter=5, random_state=42)\n",
    "        tfidf_vector = svd_doc.fit_transform(tfidf_vector)\n",
    "    return tfidf_vector\n",
    "\n",
    "### Make GloVe matrix\n",
    "glove_file = \"../glove.42B.300d.txt\"\n",
    "def glove_word_vector():\n",
    "    embeddings_dict = {}\n",
    "    with open(glove_file, 'r') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "    return embeddings_dict\n",
    "\n",
    "# Average sum of word vectors\n",
    "def sentence_embed(sentence, word_vectors, dimension):\n",
    "    sum_vector = np.zeros(dimension)\n",
    "    for w in sentence.split():\n",
    "        if w in word_vectors:\n",
    "            sum_vector += word_vectors[w]\n",
    "    return sum_vector/len(sentence)\n",
    "\n",
    "# Make document vector\n",
    "def document_embed(documents, embedding_technique='tfidf', dimension=None):\n",
    "    if embedding_technique=='tfidf':\n",
    "        doc_vector = tfidf_embed(documents, dimension)\n",
    "    elif embedding_technique=='glove':\n",
    "        word_vector = glove_word_vector()\n",
    "        if dimension is None:\n",
    "            dimension = 300\n",
    "        doc_vector = [ sentence_embed(s, word_vector, dimension).tolist() for s in documents ]\n",
    "    elif embedding_technique=='spacy':\n",
    "        doc_vector = [doc.vector for doc in documents]\n",
    "    \n",
    "    return doc_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a951018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Days difference between two datetime\n",
    "def days_between(d1, d2):\n",
    "    d1 = datetime.strptime(d1, \"%Y-%m-%d %H:%M:%S\")\n",
    "    d2 = datetime.strptime(d2, \"%Y-%m-%d %H:%M:%S\")\n",
    "    return abs((d2 - d1).days)\n",
    "\n",
    "# Function returns number of article, number of distinct authors, section of the issue, length\n",
    "def issue_indicator(news_index):\n",
    "    num_article = len(news_index)\n",
    "    num_author = len(df['author'][news_index].unique())\n",
    "    section = 0\n",
    "    length = days_between(df['time'][news_index].max(),df['time'][news_index].min())\n",
    "    return num_article, num_author, section, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d86e517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clustering \n",
    "def document_clustering(doc_vectors, clustering_method='kmeans', evaluate=False):\n",
    "    if clustering_method=='kmeans':\n",
    "        # Hyperparameters\n",
    "        k_event = 10000\n",
    "        k_issue = 6000\n",
    "        \n",
    "        # Clustering event\n",
    "        kmeans_event = KMeans(n_clusters=k_event, random_state=69).fit(doc_vectors)\n",
    "        # Represent each event by average sum of related news\n",
    "        event_vectors = np.zeros((k_event, doc_vectors.shape[1]))\n",
    "        for i in range(k_event):\n",
    "            event_vectors[i] = sum(doc_vectors[kmeans_event.labels_ == i])\n",
    "        \n",
    "        # Clustering issue\n",
    "        kmeans_issue = KMeans(n_clusters=k_issue, random_state=69).fit(event_vectors)\n",
    "        # Represent each issue by average sum of related news\n",
    "        issue_vectors = np.zeros((k_issue, doc_vectors.shape[1]))\n",
    "        for i in range(k_issue):\n",
    "            issue_vectors[i] = sum(event_vectors[kmeans_issue.labels_ == i])\n",
    "\n",
    "        issue_labels = np.array([ kmeans_issue.labels_[kmeans_event.labels_[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        return k_issue, k_event, issue_labels, kmeans_event.labels_\n",
    "    \n",
    "    elif clustering_method=='DBSCAN':\n",
    "        \n",
    "        # Hyperparameters\n",
    "        doc_eps = 0.255\n",
    "        doc_neighbors = 1\n",
    "        event_eps = 0.40\n",
    "        event_neighbors = 1\n",
    "        \n",
    "        '''\n",
    "            Find best doc_eps and event_eps\n",
    "        '''\n",
    "        if evaluate:\n",
    "            # Find best eps to group same document\n",
    "            doc_eps_list = [ 0.2 + 0.001*i for i in range(1,201) ]\n",
    "            doc_score = []\n",
    "            doc_event = []\n",
    "            doc_best_score = 0\n",
    "            doc_best_eps = 0.0001\n",
    "            for doc_eps in doc_eps_list:\n",
    "                # Clustering event\n",
    "                db_event = DBSCAN(eps=doc_eps, min_samples=doc_neighbors).fit(doc_vectors)\n",
    "                # Number of clusters in labels, ignoring noise if present.\n",
    "                n_events_ = len(set(db_event.labels_)) - (1 if -1 in db_event.labels_ else 0)\n",
    "                if len(set(db_event.labels_)) >= 2 and len(set(db_event.labels_)) <= len(doc_vectors)-1:\n",
    "                    score_ = silhouette_score(doc_vectors, db_event.labels_)\n",
    "                else:\n",
    "                    score_ = -1\n",
    "                doc_event.append(n_events_)\n",
    "                doc_score.append(score_)\n",
    "                if score_ > doc_best_score:\n",
    "                    doc_best_score = score_\n",
    "                    doc_best_eps = doc_eps\n",
    "            print(\"Best Silhouete score is {} at eps: {} and number of events: {}\".format(doc_best_score, doc_eps, n_events_))\n",
    "            fig = plt.figure()\n",
    "            plt.plot(doc_eps_list, doc_score)\n",
    "            fig.suptitle('Doc eps and Silhouette score', fontsize=20)\n",
    "            plt.xlabel('eps', fontsize=18)\n",
    "            plt.ylabel('Silhouette score', fontsize=16)\n",
    "            plt.show()\n",
    "            \n",
    "            fig = plt.figure()\n",
    "            plt.plot(doc_eps_list, doc_event)\n",
    "            fig.suptitle('Doc eps and number of events', fontsize=20)\n",
    "            plt.xlabel('eps', fontsize=18)\n",
    "            plt.ylabel('number of events', fontsize=16)\n",
    "            plt.show()\n",
    "            \n",
    "            # Set doc_eps to the best value\n",
    "            doc_eps = doc_best_eps\n",
    "            # Find best eps to group same event\n",
    "            # Clustering event\n",
    "            db_event = DBSCAN(eps=doc_eps, min_samples=doc_neighbors).fit(doc_vectors)\n",
    "            n_events_ = len(set(db_event.labels_)) - (1 if -1 in db_event.labels_ else 0)\n",
    "            event_labels = np.array(list(map(lambda x: n_events_ if x==-1 else x, db_event.labels_)))\n",
    "            event_vectors = np.zeros((n_events_, doc_vectors.shape[1]))\n",
    "            for i in range(n_events_+1):\n",
    "                if np.sum(event_labels == i) != 0:\n",
    "                    event_vectors[i] = np.sum(doc_vectors[event_labels == i], axis=0)/np.sum(event_labels == i)\n",
    "            \n",
    "            # Clustering issue\n",
    "            event_eps_list = [ 0.2 + 0.001*i for i in range(1,401) ]\n",
    "            event_score = []\n",
    "            event_issue = []\n",
    "            event_best_score = 0\n",
    "            event_best_eps = 0.001\n",
    "            for event_eps in event_eps_list:\n",
    "                db_issue = DBSCAN(eps=event_eps, min_samples=event_neighbors).fit(event_vectors)\n",
    "                # Number of clusters in labels, ignoring noise if present.\n",
    "                n_issues_ = len(set(db_issue.labels_)) - (1 if -1 in db_issue.labels_ else 0)\n",
    "                if len(set(db_issue.labels_)) >= 2 and len(set(db_issue.labels_)) <= len(event_vectors)-1:\n",
    "                    score_ = silhouette_score(event_vectors, db_issue.labels_)\n",
    "                else:\n",
    "                    score_ = -1\n",
    "                event_issue.append(n_issues_)\n",
    "                event_score.append(score_)\n",
    "                if score_ > event_best_score:\n",
    "                    event_best_score = score_\n",
    "                    event_best_eps = event_eps\n",
    "            print(\"Best Silhouete score is {} at eps: {} and number of issues: {}\".format(event_best_score, event_eps, n_issues_))\n",
    "            fig = plt.figure()\n",
    "            plt.plot(event_eps_list, event_score)\n",
    "            fig.suptitle('Event eps and Silhouette score', fontsize=20)\n",
    "            plt.xlabel('eps', fontsize=18)\n",
    "            plt.ylabel('Silhouette score', fontsize=16)\n",
    "            plt.show()\n",
    "            \n",
    "            fig = plt.figure()\n",
    "            plt.plot(event_eps_list, event_issue)\n",
    "            fig.suptitle('Event eps and number of issues', fontsize=20)\n",
    "            plt.xlabel('eps', fontsize=18)\n",
    "            plt.ylabel('number of issues', fontsize=16)\n",
    "            plt.show()\n",
    "            \n",
    "            # Set event_eps to best value\n",
    "            event_eps = event_best_eps\n",
    "            # Clustering issue\n",
    "            db_issue = DBSCAN(eps=event_eps, min_samples=event_neighbors).fit(event_vectors)\n",
    "            # Number of clusters in labels, ignoring noise if present.\n",
    "            n_issues_ = len(set(db_issue.labels_)) - (1 if -1 in db_issue.labels_ else 0)\n",
    "            n_noise_ = list(db_issue.labels_).count(-1)\n",
    "            print(n_issues_, n_noise_)\n",
    "            # Represent each issue by average sum of related news\n",
    "            issue_labels = np.array(list(map(lambda x: n_issues_ if x==-1 else x, db_issue.labels_)))\n",
    "            issue_vectors = np.zeros((n_issues_, doc_vectors.shape[1]))\n",
    "            for i in range(n_issues_+1):\n",
    "                if np.sum(issue_labels == i) != 0:\n",
    "                    issue_vectors[i] = np.sum(event_vectors[issue_labels == i], axis=0)/np.sum(issue_labels == i)\n",
    "       \n",
    "            issue_labels = np.array([ issue_labels[event_labels[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        else:\n",
    "            '''\n",
    "            Clustering using specific value\n",
    "            '''\n",
    "            # Clustering event\n",
    "            db_event = DBSCAN(eps=doc_eps, min_samples=doc_neighbors).fit(doc_vectors)\n",
    "            # Number of clusters in labels, ignoring noise if present.\n",
    "            n_events_ = len(set(db_event.labels_)) - (1 if -1 in db_event.labels_ else 0)\n",
    "            n_noise_ = list(db_event.labels_).count(-1)\n",
    "            print(n_events_, n_noise_)\n",
    "            # Represent each event by average sum of related news\n",
    "            event_labels = np.array(list(map(lambda x: n_events_ if x==-1 else x, db_event.labels_)))\n",
    "            event_vectors = np.zeros((n_events_, doc_vectors.shape[1]))\n",
    "            for i in range(n_events_+1):\n",
    "                if np.sum(event_labels == i) != 0:\n",
    "                    event_vectors[i] = np.sum(doc_vectors[event_labels == i], axis=0)/np.sum(event_labels == i)\n",
    "\n",
    "            # Clustering issue\n",
    "            db_issue = DBSCAN(eps=event_eps, min_samples=event_neighbors).fit(event_vectors)\n",
    "            # Number of clusters in labels, ignoring noise if present.\n",
    "            n_issues_ = len(set(db_issue.labels_)) - (1 if -1 in db_issue.labels_ else 0)\n",
    "            n_noise_ = list(db_issue.labels_).count(-1)\n",
    "            print(n_issues_, n_noise_)\n",
    "            # Represent each issue by average sum of related news\n",
    "            issue_labels = np.array(list(map(lambda x: n_issues_ if x==-1 else x, db_issue.labels_)))\n",
    "            issue_vectors = np.zeros((n_issues_, doc_vectors.shape[1]))\n",
    "            for i in range(n_issues_+1):\n",
    "                if np.sum(issue_labels == i) != 0:\n",
    "                    issue_vectors[i] = np.sum(event_vectors[issue_labels == i], axis=0)/np.sum(issue_labels == i)\n",
    "        \n",
    "            issue_labels = np.array([ issue_labels[event_labels[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        return n_issues_, n_events_, issue_labels, event_labels\n",
    "    \n",
    "    elif clustering_method=='agglomerative':\n",
    "        # Hyperparameters\n",
    "        n_events = 10000\n",
    "        n_issues = 6000\n",
    "        \n",
    "        # Clustering event\n",
    "        agg_event = AgglomerativeClustering(distance_threshold=0, n_clusters=n_events).fit(doc_vectors)\n",
    "        # Represent each event by average sum of related news\n",
    "        event_vectors = np.zeros((n_events, doc_vectors.shape[1]))\n",
    "        for i in range(n_events):\n",
    "            event_vectors[i] = sum(doc_vectors[agg_event.labels_ == i])\n",
    "        \n",
    "        plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "        # plot the top three levels of the dendrogram\n",
    "        plot_dendrogram(agg_event, truncate_mode=\"level\", p=3)\n",
    "        plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Clustering issue\n",
    "        agg_issue = AgglomerativeClustering(distance_threshold=0, n_clusters=n_issues).fit(event_vectors)\n",
    "        # Represent each issue by average sum of related news\n",
    "        issue_vectors = np.zeros((n_issues, doc_vectors.shape[1]))\n",
    "        for i in range(n_issues):\n",
    "            issue_vectors[i] = sum(event_vectors[agg_issue.labels_ == i])\n",
    "\n",
    "        issue_labels = np.array([ agg_issue.labels_[agg_event.labels_[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        return agg_issue, agg_event, issue_labels, agg_event.labels_\n",
    "    \n",
    "    elif clustering_method=='LDA':\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        assert(\"Doesn't support {}\".format(clustering_method))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c93a2959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ac45f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Embed document and clustering\n",
    "df2017 = df['2017' < df['time']]\n",
    "df2016 = df[('2016' < df['time']) & (df['time'] < '2017')]\n",
    "df2015 = df[('2015' < df['time']) & (df['time'] < '2016')]\n",
    "tfidf_doc2017_vectors = document_embed(df2017['agg_title_body'], embedding_technique='tfidf')\n",
    "tfidf_doc2016_vectors = document_embed(df2016['agg_title_body'], embedding_technique='tfidf')\n",
    "tfidf_doc2015_vectors = document_embed(df2015['agg_title_body'], embedding_technique='tfidf')\n",
    "# glove_doc2017_vectors = document_embed(df2017['ppcs_title_body'], embedding_technique='glove', dimension=300)\n",
    "# glove_doc2016_vectors = document_embed(df2016['ppcs_title_body'], embedding_technique='glove', dimension=300)\n",
    "# glove_doc2015_vectors = document_embed(df2015['ppcs_title_body'], embedding_technique='glove', dimension=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b742a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "num_clusters = 1000\n",
    "pca_num_components = 2\n",
    "tsne_num_components = 2\n",
    "max_iterations = 100\n",
    "\n",
    "color_list = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "             for i in range(num_clusters)]\n",
    "labels_color_map = dict([(i,x) for (i,x) in enumerate(color_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dfc63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2017\n",
    "### PCA 2D visualization\n",
    "X = tfidf_doc2017_vectors.todense()\n",
    "clustering_model = KMeans(\n",
    "    n_clusters=num_clusters,\n",
    "    max_iter=max_iterations,\n",
    "    precompute_distances=\"auto\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "labels = clustering_model.fit_predict(X)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle('PCA document 2017', fontsize=20)\n",
    "reduced_data = PCA(n_components=pca_num_components).fit_transform(X)\n",
    "fig, ax = plt.subplots()\n",
    "for index, instance in enumerate(reduced_data):\n",
    "    # print instance, index, labels[index]\n",
    "    pca_comp_1, pca_comp_2 = reduced_data[index]\n",
    "    color = labels_color_map[labels[index]]\n",
    "    ax.scatter(pca_comp_1, pca_comp_2, c=color)\n",
    "plt.show()\n",
    "\n",
    "### TSNE 2D visualization\n",
    "fig = plt.figure()\n",
    "fig.suptitle('TSNE document 2017', fontsize=20)\n",
    "embeddings = TSNE(n_components=tsne_num_components)\n",
    "Y = embeddings.fit_transform(X)\n",
    "plt.scatter(Y[:, 0], Y[:, 1], cmap=plt.cm.Spectral)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ce9be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2016\n",
    "### PCA 2D visualization\n",
    "X = tfidf_doc2016_vectors.todense()\n",
    "clustering_model = KMeans(\n",
    "    n_clusters=num_clusters,\n",
    "    max_iter=max_iterations,\n",
    "    precompute_distances=\"auto\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "labels = clustering_model.fit_predict(X)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle('PCA document 2016', fontsize=20)\n",
    "reduced_data = PCA(n_components=pca_num_components).fit_transform(X)\n",
    "fig, ax = plt.subplots()\n",
    "for index, instance in enumerate(reduced_data):\n",
    "    # print instance, index, labels[index]\n",
    "    pca_comp_1, pca_comp_2 = reduced_data[index]\n",
    "    color = labels_color_map[labels[index]]\n",
    "    ax.scatter(pca_comp_1, pca_comp_2, c=color)\n",
    "plt.show()\n",
    "\n",
    "### TSNE 2D visualization\n",
    "fig = plt.figure()\n",
    "fig.suptitle('TSNE document 2016', fontsize=20)\n",
    "embeddings = TSNE(n_components=tsne_num_components)\n",
    "Y = embeddings.fit_transform(X)\n",
    "plt.scatter(Y[:, 0], Y[:, 1], cmap=plt.cm.Spectral)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9c7b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2015\n",
    "### PCA 2D visualization\n",
    "X = tfidf_doc2015_vectors.todense()\n",
    "clustering_model = KMeans(\n",
    "    n_clusters=num_clusters,\n",
    "    max_iter=max_iterations,\n",
    "    precompute_distances=\"auto\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "labels = clustering_model.fit_predict(X)\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.suptitle('PCA document 2015', fontsize=20)\n",
    "reduced_data = PCA(n_components=pca_num_components).fit_transform(X)\n",
    "fig, ax = plt.subplots()\n",
    "for index, instance in enumerate(reduced_data):\n",
    "    # print instance, index, labels[index]\n",
    "    pca_comp_1, pca_comp_2 = reduced_data[index]\n",
    "    color = labels_color_map[labels[index]]\n",
    "    ax.scatter(pca_comp_1, pca_comp_2, c=color)\n",
    "plt.show()\n",
    "\n",
    "### TSNE 2D visualization\n",
    "fig = plt.figure()\n",
    "fig.suptitle('TSNE document 2015', fontsize=20)\n",
    "embeddings = TSNE(n_components=tsne_num_components)\n",
    "Y = embeddings.fit_transform(X)\n",
    "plt.scatter(Y[:, 0], Y[:, 1], cmap=plt.cm.Spectral)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9887e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf_doc2017_vectors.todense()\n",
    "neighbors = NearestNeighbors(n_neighbors=100)\n",
    "neighbors_fit = neighbors.fit(X)\n",
    "distances, indices = neighbors_fit.kneighbors(X)\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]\n",
    "plt.plot(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7098c08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_doc2017_num_issue, tfidf_doc2017_num_event, tfidf_doc2017_issue_labels, tfidf_doc2017_event_labels = document_clustering(tfidf_doc2017_vectors.todense(), clustering_method='DBSCAN', evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9789224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_doc2016_num_issue, tfidf_doc2016_num_event, tfidf_doc2016_issue_labels, tfidf_doc2016_event_labels = document_clustering(tfidf_doc2016_vectors.todense(), clustering_method='DBSCAN', evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_doc2015_num_issue, tfidf_doc2015_num_event, tfidf_doc2015_issue_labels, tfidf_event_labels = document_clustering(tfidf_doc2015_vectors.todense(), clustering_method='DBSCAN', evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434f48a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function returns number of article, number of distinct authors, section of the issue, length\n",
    "def issue_indicator(news_index):\n",
    "    num_article = len(news_index)\n",
    "    num_author = len(df['author'][news_index].unique())\n",
    "    section = 0\n",
    "    length = days_between(df['time'][news_index].max(),df['time'][news_index].min())\n",
    "    return num_article, num_author, section, length\n",
    "\n",
    "### Rank issues based on significance factors\n",
    "issue_doc2017_significance = []\n",
    "for issue_index in range(tfidf_doc2017_num_issue):\n",
    "    news_index = np.argwhere(tfidf_doc2017_issue_labels == issue_index).reshape(-1)\n",
    "    if len(news_index) > 0:\n",
    "        issue_doc2017_significance.append((issue_indicator(news_index),issue_index))\n",
    "issue_doc2017_significance = sorted(issue_doc2017_significance, key=lambda k: (-k[0][0],-k[0][1],-k[0][3]))\n",
    "top_doc2017_10_issue = [ info[1] for info in issue_doc2017_significance[:10]]\n",
    "\n",
    "### Rank issues based on significance factors\n",
    "issue_doc2016_significance = []\n",
    "for issue_index in range(tfidf_doc2016_num_issue):\n",
    "    news_index = np.argwhere(tfidf_doc2016_issue_labels == issue_index).reshape(-1)\n",
    "    if len(news_index) > 0:\n",
    "        issue_doc2016_significance.append((issue_indicator(news_index),issue_index))\n",
    "issue_doc2016_significance = sorted(issue_doc2016_significance, key=lambda k: (-k[0][0],-k[0][1],-k[0][3]))\n",
    "top_doc2016_10_issue = [ info[1] for info in issue_doc2016_significance[:10]]\n",
    "\n",
    "### Rank issues based on significance factors\n",
    "issue_doc2015_significance = []\n",
    "for issue_index in range(tfidf_doc2015_num_issue):\n",
    "    news_index = np.argwhere(tfidf_doc2015_issue_labels == issue_index).reshape(-1)\n",
    "    if len(news_index) > 0:\n",
    "        issue_doc2015_significance.append((issue_indicator(news_index),issue_index))\n",
    "issue_doc2015_significance = sorted(issue_doc2015_significance, key=lambda k: (-k[0][0],-k[0][1],-k[0][3]))\n",
    "top_doc2015_10_issue = [ info[1] for info in issue_doc2015_significance[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df863e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_doc2017_10_issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b168bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_doc2016_10_issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232193ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_doc2015_10_issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c315dce7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
