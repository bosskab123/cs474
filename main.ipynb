{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8df1d90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import string\n",
    "from string import punctuation\n",
    "from string import digits\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c26fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reading data\n",
    "data_dir = './data/'\n",
    "filename_prefix = 'koreaherald_1517'\n",
    "df0 = pd.read_json(os.path.join(data_dir, filename_prefix+'_0.json'))\n",
    "df1 = pd.read_json(os.path.join(data_dir, filename_prefix+'_1.json'))\n",
    "df2 = pd.read_json(os.path.join(data_dir, filename_prefix+'_2.json'))\n",
    "df3 = pd.read_json(os.path.join(data_dir, filename_prefix+'_3.json'))\n",
    "df4 = pd.read_json(os.path.join(data_dir, filename_prefix+'_4.json'))\n",
    "df5 = pd.read_json(os.path.join(data_dir, filename_prefix+'_5.json'))\n",
    "df6 = pd.read_json(os.path.join(data_dir, filename_prefix+'_6.json'))\n",
    "df7 = pd.read_json(os.path.join(data_dir, filename_prefix+'_7.json'))\n",
    "df = pd.concat([df0,df1,df2,df3,df4,df5,df6,df7])\n",
    "df.reset_index(inplace=True)\n",
    "df = df.rename(columns=dict(zip(df.columns,[df.columns[i].strip() for i in range(len(df.columns))])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3843b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551df770",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lemmatization tool\n",
    "stemmer = WordNetLemmatizer()\n",
    "### Change similar words to the same word\n",
    "US_WORD = \"USA\"\n",
    "NK_WORD = \"North Korea\"\n",
    "SK_WORD = \"South Korea\"\n",
    "\n",
    "similar_words = {\n",
    "    \"US\": US_WORD,\n",
    "    \"USA\": US_WORD,\n",
    "    \"U.S.A.\": US_WORD,\n",
    "    \"United State\": US_WORD,\n",
    "    \"N. Korea\": NK_WORD,\n",
    "    \"S. Korea\": SK_WORD\n",
    "    \n",
    "}\n",
    "\n",
    "### Transform function\n",
    "def transform_to_similar_sentence(s: str):\n",
    "    new_str = s\n",
    "    for key,value in similar_words.items():\n",
    "        new_str = re.sub(key, value, new_str)\n",
    "    return new_str\n",
    "\n",
    "### Preprocess function for grouping similar topic\n",
    "def preprocess_1(s: str):\n",
    "    # Change similar words to the same word\n",
    "    new_str = transform_to_similar_sentence(s)\n",
    "    # Remove punctuation\n",
    "    new_str = ''.join(ch if ch not in set(punctuation) else \" \" for ch in new_str)\n",
    "    # Remove all single characters\n",
    "    new_str = re.sub(r'\\W', ' ', new_str)\n",
    "    new_str = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', new_str)\n",
    "    new_str = re.sub(r'\\^[a-zA-Z]\\s+', ' ', new_str) \n",
    "    # Substituting multiple spaces with single space\n",
    "    new_str = re.sub(r'\\s+', ' ', new_str, flags=re.I)\n",
    "    # Removing prefixed 'b' - when data is in bytes format\n",
    "    new_str = re.sub(r'^b\\s+', '', new_str)\n",
    "    # Removing all numbers\n",
    "    new_str = new_str.translate(str.maketrans('', '', digits))\n",
    "    # Converting to Lowercase\n",
    "    new_str = new_str.lower()\n",
    "    # Lemmatization and remove stopwords\n",
    "    new_str = new_str.split()\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    new_str = [stemmer.lemmatize(word) for word in new_str if word not in stopwords]\n",
    "    new_str = ' '.join(new_str)\n",
    "    \n",
    "    return new_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12821891",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_body'] = df['body'].apply(lambda x: preprocess_1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242c5afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 2000)\n",
    "df['preprocessed_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab08dd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make TF-IDF matrix\n",
    "def tfidf_embed(documents, dimension):\n",
    "    # documents: df['preprocessed_body']\n",
    "    embeddings_dict = {}\n",
    "    count = CountVectorizer()\n",
    "    word_count=count.fit_transform(documents)\n",
    "    tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "    tfidf_transformer.fit(word_count)\n",
    "    tfidf_vector=tfidf_transformer.transform(word_count)\n",
    "    tfidf_feature_names = count.get_feature_names()\n",
    "    \n",
    "    # Dimensionality Reduction\n",
    "    svd = TruncatedSVD(n_components=dimension, n_iter=3, random_state=42)\n",
    "    tfidf_vector = svd.fit_transform(tfidf_vector.T)\n",
    "    for i in range(len(tfidf_feature_names)):\n",
    "        name = tfidf_feature_names[i]\n",
    "        embeddings_dict[name] = tfidf_vector[:,i]\n",
    "        \n",
    "    return embeddings_dict\n",
    "\n",
    "### Make GloVe matrix\n",
    "glove_file = \"../glove.42B.300d.txt\"\n",
    "def glove_embed():\n",
    "    embeddings_dict = {}\n",
    "    with open(glove_file, 'r') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict[word] = vector\n",
    "    return embeddings_dict\n",
    "\n",
    "### Make FastText matrix\n",
    "# fasttext_file = \"../cc.en.300.bin\"\n",
    "# def fasttext_embed(documents):\n",
    "#     wv = load_facebook_model(fasttext_file)\n",
    "#     return wv\n",
    "# ft.get_word_vector(\"another\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a84d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = tfidf_embed(df['preprocessed_body'], 300) # 23769 d\n",
    "# glove_embed(df['preprocessed_body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e84a30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_vectors['military'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaa1507",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors['military']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe757f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Grouping document with same topic \n",
    "# Approach 1: using K-means and coherent measurement to find best K\n",
    "\n",
    "# Approach 2: using DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea57fcd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
