{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ed88c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import string\n",
    "import random\n",
    "import unicodedata\n",
    "from string import punctuation\n",
    "from string import digits\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "from rake_nltk import Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40289b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23769, 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Reading data\n",
    "data_dir = 'data/'\n",
    "filename_prefix = 'koreaherald_1517_'\n",
    "df = []\n",
    "\n",
    "for i in range(8):\n",
    "    df.append(pd.read_json(os.path.join(data_dir, filename_prefix + str(i) + '.json')))\n",
    "df = pd.concat(df)\n",
    "df.reset_index(inplace=True)\n",
    "df = df.rename(columns=dict(zip(df.columns,[df.columns[i].strip() for i in range(len(df.columns))])))\n",
    "df.drop('index', inplace=True, axis=1)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3a6643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fd940ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lemmatization tool\n",
    "stemmer = WordNetLemmatizer()\n",
    "### Change similar words to the same word\n",
    "UN_WORD = \"The United Nations\"\n",
    "US_WORD = \"The United States\"\n",
    "NK_WORD = \"North Korea\"\n",
    "SK_WORD = \"South Korea\"\n",
    "\n",
    "similar_words = {\n",
    "    # Change to \"The United States\"\n",
    "    \"U.S.\": US_WORD,\n",
    "    \"US\": US_WORD,\n",
    "    \"USA\": US_WORD,\n",
    "    \"United States\": US_WORD,\n",
    "    \"United States'\": US_WORD,\n",
    "    \"The United States'\": US_WORD,\n",
    "    \n",
    "    # Change to \"North Korea\"\n",
    "    \"NK\": NK_WORD,\n",
    "    \"NK's\": NK_WORD,\n",
    "    \"N. Korea\": NK_WORD,\n",
    "    \"N. Korea's\": NK_WORD,\n",
    "    \"North Korea's\": NK_WORD,\n",
    "    \n",
    "    # Change to \"South Korea\"\n",
    "    \"SK\": SK_WORD,\n",
    "    \"SK's\": SK_WORD,\n",
    "    \"S. Korea\": SK_WORD,\n",
    "    \"S. Korea's\": SK_WORD,\n",
    "    \"South Korea's\": SK_WORD,\n",
    "    \n",
    "    # Change to \"The United Nations\"\n",
    "    \"United Nations\": UN_WORD,\n",
    "    \"United Nations'\": UN_WORD,\n",
    "    \"The United Nations'\": UN_WORD,\n",
    "    \"UN\": UN_WORD,\n",
    "}\n",
    "\n",
    "### Transform function\n",
    "def text_cleaning(s: str):\n",
    "        \n",
    "    def replace_strange_char(s: str):\n",
    "        non_en_chars = {\n",
    "            \"’\": \"'\",\n",
    "            \"‘\": \"'\"\n",
    "        }\n",
    "\n",
    "        def remove_non_en_chars(txt):\n",
    "            # remove non english characters\n",
    "            txt = convert_latin_chars(txt)\n",
    "            for char in non_en_chars.keys():\n",
    "                txt = re.sub(char, non_en_chars[char], txt)\n",
    "            txt = re.sub(r'[^\\x00-\\x7F]+', ' ', txt)\n",
    "            return txt\n",
    "\n",
    "        def convert_latin_chars(txt):\n",
    "            # convert latin characters\n",
    "            return ''.join(char for char in unicodedata.normalize('NFKD', txt) if unicodedata.category(char) != 'Mn')\n",
    "\n",
    "        s = remove_non_en_chars(s)\n",
    "        s = convert_latin_chars(s)\n",
    "        return s\n",
    "    s = replace_strange_char(s)\n",
    "    for key,value in similar_words.items():\n",
    "        s = re.sub(key, value, s)\n",
    "    return s\n",
    "\n",
    "# Using spacy to preprocess\n",
    "def preprocess_spacy(s: str):\n",
    "    # Change similar terms to the same term\n",
    "    new_str = text_cleaning(s)\n",
    "    doc = nlp(s)\n",
    "    # Group tokens\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    token_groupup_pattern = [\n",
    "        [{\"LOWER\": \"the\"}, {\"LOWER\": \"united\"}, {\"LOWER\": \"nations\"}],\n",
    "        [{\"LOWER\": \"the\"}, {\"LOWER\": \"united\"}, {\"LOWER\": \"states\"}],\n",
    "        [{\"LOWER\": \"north\"}, {\"LOWER\": \"korea\"}],\n",
    "        [{\"LOWER\": \"south\"}, {\"LOWER\": \"korea\"}],\n",
    "    ]\n",
    "    matcher.add(\"TermGroup\",token_groupup_pattern)\n",
    "    matches = matcher(doc)\n",
    "    merge_doc = []\n",
    "    for nid, start, end in matches:\n",
    "        merge_doc.append((start,end))\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for i in range(len(merge_doc)-1,-1,-1):\n",
    "            retokenizer.merge(doc[merge_doc[i][0]:merge_doc[i][1]])\n",
    "        \n",
    "    # Remove all stopword, punctuation, number\n",
    "    tokens = [ token for token in doc if not token.is_stop and not token.is_punct and not token.like_num]\n",
    "    new_str = ' '.join([ token.lemma_.lower() for token in tokens ])\n",
    "    return new_str, tokens, doc\n",
    "\n",
    "def spacy_tokenizer(s: str):\n",
    "    # Change similar terms to the same term\n",
    "    new_str = text_cleaning(s)\n",
    "    doc = nlp(s)\n",
    "    # Group tokens\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    token_groupup_pattern = [\n",
    "        [{\"LOWER\": \"the\"}, {\"LOWER\": \"united\"}, {\"LOWER\": \"nations\"}],\n",
    "        [{\"LOWER\": \"the\"}, {\"LOWER\": \"united\"}, {\"LOWER\": \"states\"}],\n",
    "        [{\"LOWER\": \"north\"}, {\"LOWER\": \"korea\"}],\n",
    "        [{\"LOWER\": \"south\"}, {\"LOWER\": \"korea\"}],\n",
    "    ]\n",
    "    matcher.add(\"TermGroup\",token_groupup_pattern)\n",
    "    matches = matcher(doc)\n",
    "    merge_doc = []\n",
    "    for nid, start, end in matches:\n",
    "        merge_doc.append((start,end))\n",
    "    with doc.retokenize() as retokenizer:\n",
    "        for i in range(len(merge_doc)-1,-1,-1):\n",
    "            retokenizer.merge(doc[merge_doc[i][0]:merge_doc[i][1]])\n",
    "        \n",
    "    # Remove all stopword, punctuation, number\n",
    "    tokens = [ token.lemma_.lower() for token in doc \\\n",
    "              if not token.is_stop and not token.is_punct and not token.like_num and token.lemma_.strip()!= '']\n",
    "    return tokens\n",
    "\n",
    "### Preprocess function for grouping similar topic\n",
    "def preprocess_manual(s: str):\n",
    "    # Change similar words to the same word\n",
    "    new_str = transform_to_similar_sentence(s)\n",
    "    # Remove punctuation\n",
    "    new_str = ''.join(ch if ch not in set(punctuation) else \" \" for ch in new_str)\n",
    "    # Remove all single characters\n",
    "    new_str = re.sub(r'\\W', ' ', new_str)\n",
    "    new_str = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', new_str)\n",
    "    new_str = re.sub(r'\\^[a-zA-Z]\\s+', ' ', new_str) \n",
    "    # Substituting multiple spaces with single space\n",
    "    new_str = re.sub(r'\\s+', ' ', new_str, flags=re.I)\n",
    "    # Removing prefixed 'b' - when data is in bytes format\n",
    "    new_str = re.sub(r'^b\\s+', '', new_str)\n",
    "    # Removing all numbers\n",
    "    new_str = new_str.translate(str.maketrans('', '', digits))\n",
    "    # Converting to Lowercase\n",
    "    new_str = new_str.lower()\n",
    "    # Lemmatization and remove stopwords\n",
    "    new_str = new_str.split()\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    tokens = [stemmer.lemmatize(word) for word in new_str if word not in stopwords]\n",
    "    new_str = ' '.join(tokens)\n",
    "    \n",
    "    return new_str, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36ad75b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clustering \n",
    "def document_clustering(doc_vectors, clustering_method='kmeans', evaluate=False):\n",
    "    if clustering_method=='kmeans':\n",
    "        # Hyperparameters\n",
    "        k_event = 10000\n",
    "        k_issue = 6000\n",
    "        \n",
    "        # Clustering event\n",
    "        kmeans_event = KMeans(n_clusters=k_event, random_state=69).fit(doc_vectors)\n",
    "        # Represent each event by average sum of related news\n",
    "        event_vectors = np.zeros((k_event, doc_vectors.shape[1]))\n",
    "        for i in range(k_event):\n",
    "            event_vectors[i] = sum(doc_vectors[kmeans_event.labels_ == i])\n",
    "        \n",
    "        # Clustering issue\n",
    "        kmeans_issue = KMeans(n_clusters=k_issue, random_state=69).fit(event_vectors)\n",
    "        # Represent each issue by average sum of related news\n",
    "        issue_vectors = np.zeros((k_issue, doc_vectors.shape[1]))\n",
    "        for i in range(k_issue):\n",
    "            issue_vectors[i] = sum(event_vectors[kmeans_issue.labels_ == i])\n",
    "\n",
    "        issue_labels = np.array([ kmeans_issue.labels_[kmeans_event.labels_[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        return k_issue, k_event, issue_labels, kmeans_event.labels_\n",
    "    \n",
    "    elif clustering_method=='DBSCAN':\n",
    "        \n",
    "        # Hyperparameters\n",
    "        doc_eps = 0.19\n",
    "        doc_neighbors = 1\n",
    "        event_eps = 0.50\n",
    "        event_neighbors = 1\n",
    "        \n",
    "        '''\n",
    "            Find best doc_eps and event_eps\n",
    "        '''\n",
    "        if evaluate:\n",
    "            # Find best eps to group same document\n",
    "            doc_eps_list = [ 0.10 + 0.001*i for i in range(1,301) ]\n",
    "            doc_score = []\n",
    "            doc_event = []\n",
    "            doc_best_score = 0\n",
    "            doc_best_eps = 0.0001\n",
    "            for doc_eps in doc_eps_list:\n",
    "                # Clustering event\n",
    "                db_event = DBSCAN(eps=doc_eps, min_samples=doc_neighbors).fit(doc_vectors)\n",
    "                # Number of clusters in labels, ignoring noise if present.\n",
    "                n_events_ = len(set(db_event.labels_)) - (1 if -1 in db_event.labels_ else 0)\n",
    "                if len(set(db_event.labels_)) >= 2 and len(set(db_event.labels_)) <= len(doc_vectors)-1:\n",
    "                    score_ = silhouette_score(doc_vectors, db_event.labels_)\n",
    "                else:\n",
    "                    score_ = -1\n",
    "                doc_event.append(n_events_)\n",
    "                doc_score.append(score_)\n",
    "                if score_ > doc_best_score:\n",
    "                    doc_best_score = score_\n",
    "                    doc_best_eps = doc_eps\n",
    "            print(\"Best Silhouete score is {} at eps: {} and number of events: {}\".format(doc_best_score, doc_eps, n_events_))\n",
    "            fig = plt.figure()\n",
    "            plt.plot(doc_eps_list, doc_score)\n",
    "            fig.suptitle('Doc eps and Silhouette score', fontsize=20)\n",
    "            plt.xlabel('eps', fontsize=18)\n",
    "            plt.ylabel('Silhouette score', fontsize=16)\n",
    "            plt.show()\n",
    "            \n",
    "            fig = plt.figure()\n",
    "            plt.plot(doc_eps_list, doc_event)\n",
    "            fig.suptitle('Doc eps and number of events', fontsize=20)\n",
    "            plt.xlabel('eps', fontsize=18)\n",
    "            plt.ylabel('number of events', fontsize=16)\n",
    "            plt.show()\n",
    "            \n",
    "            # Set doc_eps to the best value\n",
    "            doc_eps = doc_best_eps\n",
    "            # Find best eps to group same event\n",
    "            # Clustering event\n",
    "            db_event = DBSCAN(eps=doc_eps, min_samples=doc_neighbors).fit(doc_vectors)\n",
    "            n_events_ = len(set(db_event.labels_)) - (1 if -1 in db_event.labels_ else 0)\n",
    "            event_labels = np.array(list(map(lambda x: n_events_ if x==-1 else x, db_event.labels_)))\n",
    "            event_vectors = np.zeros((n_events_, doc_vectors.shape[1]))\n",
    "            for i in range(n_events_+1):\n",
    "                if np.sum(event_labels == i) != 0:\n",
    "                    event_vectors[i] = np.sum(doc_vectors[event_labels == i], axis=0)/np.sum(event_labels == i)\n",
    "            \n",
    "            \n",
    "#             # Clustering issue\n",
    "#             event_eps_list = [ 0.2 + 0.001*i for i in range(1,401) ]\n",
    "#             event_score = []\n",
    "#             event_issue = []\n",
    "#             event_best_score = 0\n",
    "#             event_best_eps = 0.001\n",
    "#             for event_eps in event_eps_list:\n",
    "#                 db_issue = DBSCAN(eps=event_eps, min_samples=event_neighbors).fit(event_vectors)\n",
    "#                 # Number of clusters in labels, ignoring noise if present.\n",
    "#                 n_issues_ = len(set(db_issue.labels_)) - (1 if -1 in db_issue.labels_ else 0)\n",
    "#                 if len(set(db_issue.labels_)) >= 2 and len(set(db_issue.labels_)) <= len(event_vectors)-1:\n",
    "#                     score_ = silhouette_score(event_vectors, db_issue.labels_)\n",
    "#                 else:\n",
    "#                     score_ = -1\n",
    "#                 event_issue.append(n_issues_)\n",
    "#                 event_score.append(score_)\n",
    "#                 if score_ > event_best_score:\n",
    "#                     event_best_score = score_\n",
    "#                     event_best_eps = event_eps\n",
    "#             print(\"Best Silhouete score is {} at eps: {} and number of issues: {}\".format(event_best_score, event_eps, n_issues_))\n",
    "#             fig = plt.figure()\n",
    "#             plt.plot(event_eps_list, event_score)\n",
    "#             fig.suptitle('Event eps and Silhouette score', fontsize=20)\n",
    "#             plt.xlabel('eps', fontsize=18)\n",
    "#             plt.ylabel('Silhouette score', fontsize=16)\n",
    "#             plt.show()\n",
    "            \n",
    "#             fig = plt.figure()\n",
    "#             plt.plot(event_eps_list, event_issue)\n",
    "#             fig.suptitle('Event eps and number of issues', fontsize=20)\n",
    "#             plt.xlabel('eps', fontsize=18)\n",
    "#             plt.ylabel('number of issues', fontsize=16)\n",
    "#             plt.show()\n",
    "            \n",
    "            # Set event_eps to best value\n",
    "            event_eps = 0.5\n",
    "            # Clustering issue\n",
    "            db_issue = DBSCAN(eps=event_eps, min_samples=event_neighbors).fit(event_vectors)\n",
    "            # Number of clusters in labels, ignoring noise if present.\n",
    "            n_issues_ = len(set(db_issue.labels_)) - (1 if -1 in db_issue.labels_ else 0)\n",
    "            n_noise_ = list(db_issue.labels_).count(-1)\n",
    "            # Represent each issue by average sum of related news\n",
    "            issue_labels = np.array(list(map(lambda x: n_issues_ if x==-1 else x, db_issue.labels_)))\n",
    "            issue_vectors = np.zeros((n_issues_, doc_vectors.shape[1]))\n",
    "            for i in range(n_issues_+1):\n",
    "                if np.sum(issue_labels == i) != 0:\n",
    "                    issue_vectors[i] = np.sum(event_vectors[issue_labels == i], axis=0)/np.sum(issue_labels == i)\n",
    "       \n",
    "            issue_labels = np.array([ issue_labels[event_labels[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        else:\n",
    "            '''\n",
    "            Clustering using specific value\n",
    "            '''\n",
    "            # Clustering event\n",
    "            db_event = DBSCAN(eps=doc_eps, min_samples=doc_neighbors).fit(doc_vectors)\n",
    "            # Number of clusters in labels, ignoring noise if present.\n",
    "            n_events_ = len(set(db_event.labels_)) - (1 if -1 in db_event.labels_ else 0)\n",
    "            n_noise_ = list(db_event.labels_).count(-1)\n",
    "            print(\"1st cluster:\\n\\tThe number of cluster is {}\".format(n_events_))\n",
    "            # Represent each event by average sum of related news\n",
    "            event_labels = np.array(list(map(lambda x: n_events_ if x==-1 else x, db_event.labels_)))\n",
    "            event_vectors = np.zeros((n_events_, doc_vectors.shape[1]))\n",
    "            for i in range(n_events_+1):\n",
    "                if np.sum(event_labels == i) != 0:\n",
    "                    event_vectors[i] = np.sum(doc_vectors[event_labels == i], axis=0)/np.sum(event_labels == i)\n",
    "\n",
    "            # Clustering issue\n",
    "            db_issue = DBSCAN(eps=event_eps, min_samples=event_neighbors).fit(event_vectors)\n",
    "            # Number of clusters in labels, ignoring noise if present.\n",
    "            n_issues_ = len(set(db_issue.labels_)) - (1 if -1 in db_issue.labels_ else 0)\n",
    "            n_noise_ = list(db_issue.labels_).count(-1)\n",
    "            print(\"2nd cluster:\\n\\tThe number of cluster is {}\".format(n_issues_))\n",
    "            # Represent each issue by average sum of related news\n",
    "            issue_labels = np.array(list(map(lambda x: n_issues_ if x==-1 else x, db_issue.labels_)))\n",
    "            issue_vectors = np.zeros((n_issues_, doc_vectors.shape[1]))\n",
    "            for i in range(n_issues_+1):\n",
    "                if np.sum(issue_labels == i) != 0:\n",
    "                    issue_vectors[i] = np.sum(event_vectors[issue_labels == i], axis=0)/np.sum(issue_labels == i)\n",
    "        \n",
    "            issue_labels = np.array([ issue_labels[event_labels[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        return n_issues_, n_events_, issue_labels, event_labels\n",
    "    \n",
    "    elif clustering_method=='agglomerative':\n",
    "        # Hyperparameters\n",
    "        n_events = 10000\n",
    "        n_issues = 6000\n",
    "        \n",
    "        # Clustering event\n",
    "        agg_event = AgglomerativeClustering(distance_threshold=0, n_clusters=n_events).fit(doc_vectors)\n",
    "        # Represent each event by average sum of related news\n",
    "        event_vectors = np.zeros((n_events, doc_vectors.shape[1]))\n",
    "        for i in range(n_events):\n",
    "            event_vectors[i] = sum(doc_vectors[agg_event.labels_ == i])\n",
    "        \n",
    "        plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "        # plot the top three levels of the dendrogram\n",
    "        plot_dendrogram(agg_event, truncate_mode=\"level\", p=3)\n",
    "        plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Clustering issue\n",
    "        agg_issue = AgglomerativeClustering(distance_threshold=0, n_clusters=n_issues).fit(event_vectors)\n",
    "        # Represent each issue by average sum of related news\n",
    "        issue_vectors = np.zeros((n_issues, doc_vectors.shape[1]))\n",
    "        for i in range(n_issues):\n",
    "            issue_vectors[i] = sum(event_vectors[agg_issue.labels_ == i])\n",
    "\n",
    "        issue_labels = np.array([ agg_issue.labels_[agg_event.labels_[i]] for i in range(doc_vectors.shape[0]) ])\n",
    "        \n",
    "        return agg_issue, agg_event, issue_labels, agg_event.labels_\n",
    "    \n",
    "    elif clustering_method=='LDA':\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        assert(\"Doesn't support {}\".format(clustering_method))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7cc0b198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/data_vectorized.csv']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Find all doc contains \"North Korea\"\n",
    "# vectorizer = CountVectorizer(tokenizer=spacy_tokenizer)\n",
    "# data_vectorized = vectorizer.fit_transform(df['body'])\n",
    "# joblib.dump(vectorizer, '../data/vectorizer.csv')\n",
    "# joblib.dump(data_vectorized, '../data/data_vectorized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0b71634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = joblib.load('../data/vectorizer.csv')\n",
    "data_vectorized = joblib.load('../data/data_vectorized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c4c6014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nk_index = np.argwhere(vectorizer.get_feature_names_out() == 'north korea')[0,0]\n",
    "nk_doc_index = [ i for i,j in np.argwhere(X[:, nk_index]>0)]\n",
    "nk_df = df.iloc[nk_doc_index].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ee3eafc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>time</th>\n",
       "      <th>description</th>\n",
       "      <th>body</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[Weekender] Korea’s dynamic 2017</td>\n",
       "      <td>Choi He-suk</td>\n",
       "      <td>2018-01-01 13:22:00</td>\n",
       "      <td>From North Korea’s nuclear weapons program nea...</td>\n",
       "      <td>From North Korea’s nuclear weapons program nea...</td>\n",
       "      <td>Social affairs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>[Newsmaker] Panamanian vessel probed over susp...</td>\n",
       "      <td>Yonhap</td>\n",
       "      <td>2017-12-31 14:55:00</td>\n",
       "      <td>PYEONGTAEK  -- South Korea has seized and insp...</td>\n",
       "      <td>PYEONGTAEK  -- South Korea has seized and insp...</td>\n",
       "      <td>North Korea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Hong Kong ship crew questioned in S. Korea for...</td>\n",
       "      <td>AFP</td>\n",
       "      <td>2017-12-30 15:44:00</td>\n",
       "      <td>The crew of a Hong Kong-registered ship have b...</td>\n",
       "      <td>The crew of a Hong Kong-registered ship have b...</td>\n",
       "      <td>North Korea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>Secret Sauce? Kim Jong-un applies science to k...</td>\n",
       "      <td>AP</td>\n",
       "      <td>2017-12-30 12:10:00</td>\n",
       "      <td>Kim Jong Un wants to turn the art of kimchi-ma...</td>\n",
       "      <td>Kim Jong Un wants to turn the art of kimchi-ma...</td>\n",
       "      <td>North Korea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>N. Korea says there will be no change to its n...</td>\n",
       "      <td>Yonhap</td>\n",
       "      <td>2017-12-30 10:31:00</td>\n",
       "      <td>North Korea will continue to enhance its nucle...</td>\n",
       "      <td>North Korea will continue to enhance its nucle...</td>\n",
       "      <td>North Korea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9103</th>\n",
       "      <td>23760</td>\n",
       "      <td>‘Responsibility to protect does not apply to N...</td>\n",
       "      <td>Korea Herald</td>\n",
       "      <td>2015-01-01 21:21:00</td>\n",
       "      <td>This is the second installment in a special Ne...</td>\n",
       "      <td>This is the second installment in a special Ne...</td>\n",
       "      <td>Defense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9104</th>\n",
       "      <td>23764</td>\n",
       "      <td>N. Korean leader's speech arouses cautious opt...</td>\n",
       "      <td>KH디지털2</td>\n",
       "      <td>2015-01-01 13:36:00</td>\n",
       "      <td>North Korean leader Kim Jong-un's New Year's D...</td>\n",
       "      <td>North Korean leader Kim Jong-un's New Year's D...</td>\n",
       "      <td>North Korea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9105</th>\n",
       "      <td>23766</td>\n",
       "      <td>Ex-U.S. envoy calls for clearer communication ...</td>\n",
       "      <td>KH디지털2</td>\n",
       "      <td>2015-01-01 09:27:00</td>\n",
       "      <td>The United States should make its thoughts on ...</td>\n",
       "      <td>The United States should make its thoughts on ...</td>\n",
       "      <td>North Korea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9106</th>\n",
       "      <td>23767</td>\n",
       "      <td>U.S. imposes sanctions on N. Korean firm</td>\n",
       "      <td>KH디지털2</td>\n",
       "      <td>2015-01-01 09:25:00</td>\n",
       "      <td>The United States has imposed sanctions on a N...</td>\n",
       "      <td>The United States has imposed sanctions on a N...</td>\n",
       "      <td>North Korea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9107</th>\n",
       "      <td>23768</td>\n",
       "      <td>Park calls for military readiness amid tension...</td>\n",
       "      <td>KH디지털2</td>\n",
       "      <td>2015-01-01 09:24:00</td>\n",
       "      <td>President Park Geun-hye called on the military...</td>\n",
       "      <td>President Park Geun-hye called on the military...</td>\n",
       "      <td>Defense</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9108 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                              title        author  \\\n",
       "0         1                   [Weekender] Korea’s dynamic 2017   Choi He-suk   \n",
       "1         3  [Newsmaker] Panamanian vessel probed over susp...        Yonhap   \n",
       "2         4  Hong Kong ship crew questioned in S. Korea for...           AFP   \n",
       "3         7  Secret Sauce? Kim Jong-un applies science to k...            AP   \n",
       "4         8  N. Korea says there will be no change to its n...        Yonhap   \n",
       "...     ...                                                ...           ...   \n",
       "9103  23760  ‘Responsibility to protect does not apply to N...  Korea Herald   \n",
       "9104  23764  N. Korean leader's speech arouses cautious opt...        KH디지털2   \n",
       "9105  23766  Ex-U.S. envoy calls for clearer communication ...        KH디지털2   \n",
       "9106  23767           U.S. imposes sanctions on N. Korean firm        KH디지털2   \n",
       "9107  23768  Park calls for military readiness amid tension...        KH디지털2   \n",
       "\n",
       "                     time                                        description  \\\n",
       "0     2018-01-01 13:22:00  From North Korea’s nuclear weapons program nea...   \n",
       "1     2017-12-31 14:55:00  PYEONGTAEK  -- South Korea has seized and insp...   \n",
       "2     2017-12-30 15:44:00  The crew of a Hong Kong-registered ship have b...   \n",
       "3     2017-12-30 12:10:00  Kim Jong Un wants to turn the art of kimchi-ma...   \n",
       "4     2017-12-30 10:31:00  North Korea will continue to enhance its nucle...   \n",
       "...                   ...                                                ...   \n",
       "9103  2015-01-01 21:21:00  This is the second installment in a special Ne...   \n",
       "9104  2015-01-01 13:36:00  North Korean leader Kim Jong-un's New Year's D...   \n",
       "9105  2015-01-01 09:27:00  The United States should make its thoughts on ...   \n",
       "9106  2015-01-01 09:25:00  The United States has imposed sanctions on a N...   \n",
       "9107  2015-01-01 09:24:00  President Park Geun-hye called on the military...   \n",
       "\n",
       "                                                   body         section  \n",
       "0     From North Korea’s nuclear weapons program nea...  Social affairs  \n",
       "1     PYEONGTAEK  -- South Korea has seized and insp...     North Korea  \n",
       "2     The crew of a Hong Kong-registered ship have b...     North Korea  \n",
       "3     Kim Jong Un wants to turn the art of kimchi-ma...     North Korea  \n",
       "4     North Korea will continue to enhance its nucle...     North Korea  \n",
       "...                                                 ...             ...  \n",
       "9103  This is the second installment in a special Ne...         Defense  \n",
       "9104  North Korean leader Kim Jong-un's New Year's D...     North Korea  \n",
       "9105  The United States should make its thoughts on ...     North Korea  \n",
       "9106  The United States has imposed sanctions on a N...     North Korea  \n",
       "9107  President Park Geun-hye called on the military...         Defense  \n",
       "\n",
       "[9108 rows x 7 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64daa772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/lda.csv']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lda = LatentDirichletAllocation(n_components=50, random_state=0)\n",
    "# lda.fit(data_vectorized)\n",
    "# joblib.dump(lda, '../data/lda.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07d681d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = joblib.load('../data/lda.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3a69755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, vectorizer, n_top_words):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"\\nTopic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3717913e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic #0: police say old child man victim find suspect kim death year family case woman court mother take report father abuse lee kill arrest body die\n",
      "\n",
      "Topic #1: korean soldier military say north south border army line fire official north korea south korea guard near area troop zone time take dmz command staff cross accord\n",
      "\n",
      "Topic #2: north north korea sanction nuclear test resolution say council u.n pyongyang security international korean missile launch china un ban rocket country new long report range adopt\n",
      "\n",
      "Topic #3: korean south say ministry country foreign chinese south korea koreans refugee official government yonhap people china group visa fishing boat year accord national international authority war\n",
      "\n",
      "Topic #4: minister ministry foreign meeting seoul south korea official talk hold defense say discuss korean cooperation country vice issue security yonhap visit deputy south counterpart affair bilateral\n",
      "\n",
      "Topic #5: trump nuclear say president south korea donald earthquake iran country korean quake deal trade south security seoul administration weapon national korea north korea magnitude pay washington official\n",
      "\n",
      "Topic #6: hwang chinese airport tourist south korea china kyo tour say visit yonhap number ahn korean busan tourism south parade international seoul visitor government military incheon traveler\n",
      "\n",
      "Topic #7: say woman law sex crime korea case public sexual people year man right kim tell time korean think social act gender drug like young violence\n",
      "\n",
      "Topic #8: right human say u.n international abuse report year rights violation call situation people seoul government camp hold commission issue office prison world official ministry country\n",
      "\n",
      "Topic #9: park president choi court say geun presidential hye scandal impeachment office state soon constitutional sil corruption trial affair justice lee hearing decision day chief public\n",
      "\n",
      "Topic #10: country trade economic korea development cooperation international say economy global world south korea investment asia year new $ policy project growth agreement china business market infrastructure\n",
      "\n",
      "Topic #11: military defense say official agency ministry service program system plan information intelligence project south korea year technology country cyber nis air force drone jet national attack\n",
      "\n",
      "Topic #12: winter pyeongchang event sport olympics game olympic say korean world committee international host team south south korea athlete hold games competition czech organizer taekwondo player medal\n",
      "\n",
      "Topic #13: thaad defense system say deployment missile south korea deploy area seoul decision government battery ministry terminal altitude high seongju korea plan resident official radar launcher u.s\n",
      "\n",
      "Topic #14: force military air exercise navy drill say south command joint korean operation army south korea commander aircraft naval korea troop sea gen training u.s base submarine\n",
      "\n",
      "Topic #15: say claim dokdo japan korean islet country history year south korea textbook release warmbier sea detain territory state government korea = paper yonhap ministry japanese american\n",
      "\n",
      "Topic #16: prosecutor charge court prosecution seoul lee arrest won suspect investigation district office probe company allegation win say $ kim group money central receive year fund\n",
      "\n",
      "Topic #17: party moon president election presidential say political conservative people park candidate korea ahn kim democratic liberal jae opposition national leader lead hong year government ban\n",
      "\n",
      "Topic #18: korea korean lee country joel year culture ambassador world say seoul cultural industry international food event technology people tourism embassy company joel@heraldcorp.com business national add\n",
      "\n",
      "Topic #19: school student education university say ministry teacher textbook high government professor college history state english year program language study class seoul private author korea system\n",
      "\n",
      "Topic #20: city seoul say area metropolitan government bus province driver official capital mayor year district won road resident market $ tourist traffic new public plan taxi\n",
      "\n",
      "Topic #21: north korean north korea say south war military inter family koreas border pyongyang peace seoul talk tension reunion hold separate cross south korea nuclear end unification peninsula\n",
      "\n",
      "Topic #22: island jeju river water resort bridge southern jejudo hangang beach be irish tourist han camera ireland typhoon taiwan byel province silverstar@heraldcorp.com say eun near wind\n",
      "\n",
      "Topic #23: say north korea nuclear north pyongyang talk u.s trump state korean china president washington weapon sanction program regime pressure denuclearization the united states policy think kim international un\n",
      "\n",
      "Topic #24: say farm bird south system province kilometer south korea military country artillery north official ministry fire air animal range area missile detect vehicle agriculture flu target\n",
      "\n",
      "Topic #25: german lee germany kim korea funeral new georgia europe european berlin joel say national french year soviet late chancellor zealand mongolia kazakhstan palace country merkel\n",
      "\n",
      "Topic #26: say korea seoul center people art kim public work like korean life year culture young new program park help world old design asean time experience\n",
      "\n",
      "Topic #27: kim leader north party korean say country jong un central korea state year people agency congress news il official rule worker report medium yonhap north korea\n",
      "\n",
      "Topic #28: percent party poll survey point rating week percentage moon voter approval election conduct support vote show korea people south early say jae ahn president province\n",
      "\n",
      "Topic #29: korea ambassador lee embassy country year joel korean people sri national turkey independence peace indonesia say turkish award indonesian seoul war lanka anniversary pakistan world\n",
      "\n",
      "Topic #30: ferry sewol ship say disaster family government april miss sink search find accident rescue operation sinking year people official victim high coast salvage ministry lee\n",
      "\n",
      "Topic #31: presidential office wa dae cheong official lee national say government election agency dog president report animal moon nis administration public service jae information law pet\n",
      "\n",
      "Topic #32: rally police protest say seoul water fire hold group saturday government protester central union street square people baek civic gwanghwamun protestor near public stage anti\n",
      "\n",
      "Topic #33: missile launch test say north ballistic range nuclear north korea fire rocket pyongyang submarine korea long conduct develop claim korean warhead military kim new icbm intercontinental\n",
      "\n",
      "Topic #34: flight air passenger say worker subway train work safety station seoul korean line plane accident airline carrier cho company female airport year employee ministry hour\n",
      "\n",
      "Topic #35: president moon korean summit visit park leader meeting south say hold south korea meet talk country trip issue office dae jae wa cheong day bilateral seoul\n",
      "\n",
      "Topic #36: north korea nuclear missile north say korean military south korea provocation defense threat security test minister seoul ballistic ministry peninsula launch pyongyang washington joint ally south foreign\n",
      "\n",
      "Topic #37: china u.s south korea say chinese beijing defense missile seoul security system north korea thaad military nuclear issue deployment south korean threat relation strategic washington korea ally\n",
      "\n",
      "Topic #38: ministry health say product government dust fine safety chemical environment humidifier korea air cause company disinfectant toxic victim level smoking oxy include damage measure insurance\n",
      "\n",
      "Topic #39: korean north south say government north korea inter seoul unification ministry complex park industrial pyongyang south korea official joint border group kaesong leaflet plan humanitarian exchange firm\n",
      "\n",
      "Topic #40: north kim korean jong say north korea un leader pyongyang report official defector nam regime late malaysia state source country il south korea koreans brother attack yonhap\n",
      "\n",
      "Topic #41: science technology research institute innovation ict future say scientist sample planning researcher biological anthrax daejeon forum lab develop south ministry yonhap scientific oecd startup base\n",
      "\n",
      "Topic #42: energy say nuclear power government reactor plan country construction plant year ministry new project climate emission percent change gas fuel south korea build yonhap electricity commission\n",
      "\n",
      "Topic #43: percent year number say show child increase rate age datum report high average country low korea accord people woman survey south korea total korean koreans rise\n",
      "\n",
      "Topic #44: party opposition lawmaker saenuri rep rule say assembly parliamentary national bill kim park leader new main committee lee member minister floor chung president meeting political\n",
      "\n",
      "Topic #45: u.s say obama alliance korean south korea attack president park washington seoul lippert visit house ambassador american barack security korea state mark official foreign envoy include\n",
      "\n",
      "Topic #46: patient medical health disease hospital say mers case virus outbreak people ministry report government doctor confirm treatment country center south korea respiratory day spread infection number\n",
      "\n",
      "Topic #47: donation yoo donate mr blood report chen ebc eun byung donor volunteer reserve rose red village cross hahoe folk hold type regard church yamaguchi s\n",
      "\n",
      "Topic #48: japan japanese woman victim tokyo issue say abe korean war seoul world deal sexual government comfort wartime slavery south korea force apology minister ii sex agreement\n",
      "\n",
      "Topic #49: government worker labor job say work company wage reform pay system year public business plan korea union tax ministry employment employee sector won market create\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "print_top_words(lda, vectorizer, n_top_words=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27d4d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "LDA 50 topics\n",
    "Topic 0: Tragedy\n",
    "Topic 1: Soldier\n",
    "Topic 2: nuclear / missile\n",
    "Topic 3: ?\n",
    "Topic 4: ?\n",
    "Topic 5: war \n",
    "Topic 6: Travel?\n",
    "Topic 7: Sex violence\n",
    "Topic 8: ?\n",
    "Topic 9: ?\n",
    "Topic 10: ?\n",
    "Topic 11: ?\n",
    "Topic 12: Olympics game\n",
    "Topic 13: ?\n",
    "Topic 14: Army\n",
    "Topic 15: Dokdo\n",
    "Topic 16: \n",
    "Topic 17: Moon and Park\n",
    "Topic 33: missile nuclear\n",
    "Topic 34: airplane\n",
    "Topic 35: President\n",
    "Topic 36: misile nuclear\n",
    "Topic 41: science technology\n",
    "Topic 46: disease\n",
    "Topic 47: blood donate\n",
    "Topic 48: Japanese and sex victim\n",
    "Topic 49: Worker, job market\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "57640a07",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2689/4013735802.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnk_doc_topic_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_vectorized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnk_doc_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnk_doc_topic_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/nk_doc_topic_dist.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/sklearn/decomposition/_lda.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    709\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_n_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"LatentDirichletAllocation.transform\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m         )\n\u001b[0;32m--> 711\u001b[0;31m         \u001b[0mdoc_topic_distr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unnormalized_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0mdoc_topic_distr\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mdoc_topic_distr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc_topic_distr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/sklearn/decomposition/_lda.py\u001b[0m in \u001b[0;36m_unnormalized_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0mDocument\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0mdistribution\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \"\"\"\n\u001b[0;32m--> 687\u001b[0;31m         \u001b[0mdoc_topic_distr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_e_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcal_sstats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc_topic_distr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/sklearn/decomposition/_lda.py\u001b[0m in \u001b[0;36m_e_step\u001b[0;34m(self, X, cal_sstats, random_init, parallel)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparallel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         results = parallel(\n\u001b[0m\u001b[1;32m    445\u001b[0m             delayed(_update_doc_distribution)(\n\u001b[1;32m    446\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_slice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/site-packages/sklearn/decomposition/_lda.py\u001b[0m in \u001b[0;36m_update_doc_distribution\u001b[0;34m(X, exp_topic_word_distr, doc_topic_prior, max_doc_update_iter, mean_change_tol, cal_sstats, random_state)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mnorm_phi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_doc_topic_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_topic_word_d\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mEPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mdoc_topic_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_doc_topic_d\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnts\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_phi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_topic_word_d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0;31m# Note: adds doc_topic_prior to doc_topic_d, in-place.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0m_dirichlet_expectation_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_topic_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_topic_prior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_doc_topic_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# nk_doc_topic_dist = pd.DataFrame(lda.transform(data_vectorized)).iloc[nk_doc_index]\n",
    "# nk_doc_topic_dist.to_csv('../data/nk_doc_topic_dist.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "36763d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nk_doc_topic_dist = pd.read_csv('../data/nk_doc_topic_dist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ff5bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_nearest_docs(doc_dist, k=5, get_dist=False):\n",
    "    '''\n",
    "    doc_dist: topic distribution (sums to 1) of one article\n",
    "    \n",
    "    Returns the index of the k nearest articles (as by Jensen–Shannon divergence in topic space). \n",
    "    '''\n",
    "    \n",
    "    temp = nk_doc_topic_dist\n",
    "         \n",
    "    distances = temp.apply(lambda x: jensenshannon(x, doc_dist), axis=1)\n",
    "    k_nearest = distances[distances != 0].nsmallest(n=k).index\n",
    "    \n",
    "    if get_dist:\n",
    "        k_distances = distances[distances != 0].nsmallest(n=k)\n",
    "        return k_nearest, k_distances\n",
    "    else:\n",
    "        return k_nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b85bbaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_related_docs(topic_index, num_docs=5):\n",
    "    sorted_doc = nk_doc_topic_dist.sort_values(by=['{}'.format(topic_index)], ascending=False)\n",
    "    return nk_df.iloc[sorted_doc[:num_docs].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e66813c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['China vows to ensure full implementation of U.N. sanctions against N.K,',\n",
       " 'China orders North Korean firms to close down',\n",
       " \"N. Korea's nuclear test site shows 'high-level activity': 38 North\",\n",
       " 'EU tightens sanctions on N. Korea in line with UN resolution',\n",
       " \"U.N. ban on mineral exports to hurt North Korea's revenue: Seoul\"]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nuclear, missile\n",
    "most_related_docs(2)['title'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ee6b9044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NK’s latest missile a ‘new type’: S. Korean military',\n",
       " 'Two NK missiles on mobile launchers could either be Rodong or part of ICBMs: US expert',\n",
       " \"Seoul confirms N. Korea's push to develop solid-fuel rockets\",\n",
       " 'N. Korea may be trying to develop new medium-range solid-fuel missile',\n",
       " \"N. Korea's new missile could be deployable by 2018: expert\"]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nuclear, missile\n",
    "most_related_docs(33)['title'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "302deb76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Seoul: N. Korea's provocation will only deepen its isolation\",\n",
       " \"NK vows to take 'toughest' military actions as US sends aircraft carrier\",\n",
       " \"U.S. assures 'ironclad' commitment to defending Korea\",\n",
       " 'Seoul, Washington defense chiefs agree to strengthen deployment of US strategic assets',\n",
       " 'S. Korea condemns NK‘s missile provocation, steps up diplomatic drive']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nuclear, missile\n",
    "most_related_docs(36)['title'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1694009c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S. Korea repeals anti-cheating law',\n",
       " 'Expats decry military for gay sex conviction',\n",
       " 'Korean military court convicts soldier over gay sex',\n",
       " '[Herald Interview] The price of faith for conscientious objectors',\n",
       " '[Election 2017] Gender biased language backfires on campaign trail']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sex violence\n",
    "most_related_docs(7)['title'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c0be0f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"PyeongChang 'ready to welcome the world' at next Winter Olympics: IOC\",\n",
       " 'N. Korean IOC member keeps mum on Olympic co-hosting with S. Korea',\n",
       " 'Moon says sports can create peace, invites N. Korea to PyeongChang Olympics',\n",
       " \"'NK submitted document for PyeongChang Paralympics participation'\",\n",
       " 'North Korean IOC member says joint Korean team at PyeongChang 2018 may be difficult']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Olympics game\n",
    "most_related_docs(12)['title'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2c380a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Civic groups demand U.S. apology for anthrax delivery',\n",
       " 'USFK vows transparency in bio defense training in S. Korea',\n",
       " \"U.S. calls for enhancing defense against 'very real' biological weapons threats from N. Korea\",\n",
       " '‘USFK conducted 16 covert anthrax tests since 2009’',\n",
       " 'S. Korea, U.S. agree to on-site probe of Osan base next week over anthrax shipment']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Science and Technology\n",
    "most_related_docs(41)['title'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5945eecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N. Korea claims to have developed panacea for MERS',\n",
       " 'North Korea ends preventive steps against MERS virus',\n",
       " 'NK soldier suffering from pneumonia and blood poisoning: report',\n",
       " 'Number of malaria patients has dropped steadily in recent years: data',\n",
       " \"Gov't to allow blood drives in malaria-prone areas to cope with low reserves\"]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Disease\n",
    "most_related_docs(46)['title'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2629eb95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12784, 2115, 12168, 1938, 15354]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic2_5docs.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbad1e37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
